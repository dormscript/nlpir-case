
error:出錯的SQL：select * from jianshu.articleurl  where id >= 277266 and id < 277366 order by id ASC	2006: MySQL server has gone away
error:出錯的SQL：replace into blog.zbp_post (log_ID,log_Tag,log_Intro,log_Title,log_Content,log_PostTime) VALUES
('325514','{975703}{12671}{975757}{975836}{946}{1623}{9229}{241452}{190}{9448}{51223}{135585}{4302}{10924}{1413}{1123434}{40}{778}{858}{868}{1149}{6747}{265734}{247}{975714}{1628}{975713}{11173}{831}{10131}{13523}{2915}{1098}{1031}{3781}{27371}{11942}{7058}{975735}{3992}{4889}{157567}{1123435}{239}{1123436}{1038}{196}{975768}','iOSer‘s 跨界之路 又到一年双十一，做一下从阿里回家这段日子的思想汇报吧~ Part One 在阿里这段日子的收获 呜谢这段日子阿里小伙伴们的帮助与陪伴（人太多，就不一一感谢啦）。 一、技能 Develop 很多东西都是相通的，许多知识都可以互相反哺。 回忆几个印象深刻的侧影吧（排名不分先后）。 跟着土土哥反编译源码探究一个诡异问题的内在原因。 和老谭一起讨论一个问题','iOSer‘s 跨界之路','<blockquote>
<p>又到一年双十一，做一下从阿里回家这段日子的思想汇报吧~</p>
</blockquote>
<h2>Part One 在阿里这段日子的收获</h2>
<blockquote>
<p>呜谢这段日子阿里小伙伴们的帮助与陪伴（人太多，就不一一感谢啦）。</p>
</blockquote>
<h3>一、技能</h3>
<h4>Develop</h4>
<blockquote>
<p>很多东西都是相通的，许多知识都可以互相反哺。<br>
回忆几个印象深刻的侧影吧（排名不分先后）。</p>
</blockquote>
<ul>
<li>跟着土土哥反编译源码探究一个诡异问题的内在原因。</li>
<li>和老谭一起讨论一个问题的最佳实现策略。（代码生成器。。超越手淘的金钟罩）</li>
<li>毒姐号称要超越YY的缓存库。</li>
<li>晓明哥惊世骇俗的服务中间层SP。</li>
<li>平哥的数据驱动型UI组件库。（以及对胸部的了解。。。）</li>
<li>宪华的只要两行代码~（强调对于代码的封装与精简极致）</li>
<li>茶哥、远哥对于组件化的规划与推进。</li>
<li>贤哥RTL的完美方案。</li>
<li>东伟网络库与混合容器（包含weex）的整合优化。</li>
<li>仁哥：“多思考总结，做每件事情最后能让你得到些什么”（偶尔看到仁哥代码的喜悦）。</li>
</ul>
<h4>PM</h4>
<blockquote>
<p>免责声明，粗略一写，纯个人体会</p>
</blockquote>
<div class=\"image-package\">
<img src=\"//upload-images.jianshu.io/upload_images/2197993-fc0ad1ff81bffc3d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" data-original-src=\"http://upload-images.jianshu.io/upload_images/2197993-fc0ad1ff81bffc3d.png?imageMogr2/auto-orient/strip\" alt=\"阿里项目流程\"><br><div class=\"image-caption\">阿里项目流程</div>
</div>
<h3>二、思想</h3>
<blockquote>
<p>其实感觉来阿里的这段日子，思想上的转变可能更重要一些。</p>
</blockquote>
<h4>产品Owner意识</h4>
<blockquote>
<p>体会颇深。</p>
</blockquote>
<p>程序猿们通过对于产品的思考，形成一个可行的初步规划（先跟自己的主管交流一下）。然后去跟产品交流这个规划的可行性，从不同角度达成共识后，技术发起的产品需求便会跟随迭代推进下去。</p>
<p>程序猿对于产品演进的方向会有自己的思考，结合技术视野，往往能与产品碰撞出不少可行的方案。对于程序猿产出的产品目标是可以写进自己KPI中的，那么也就会以需求Owner的角色负责将方案完善，整合资源，推进产品目标的达成。</p>
<p>对于业务产品的深度理解、思考与实践，作为一个产品的“Owner”去打磨她，她也会用最好的数据表现来回报Owner们--人人都是产品经理。</p>
<h4>善于总结</h4>
<blockquote>
<p>说到总结，也是因为之前总结了一篇博文，才被宪华推荐的。</p>
</blockquote>
<h5>知识</h5>
<p>小伙伴们都很善于总结，有的画导图，有的写ppt，团队云雀上的分享也干货满满（于是回来后，赶紧自己也用gitbook自建了一个团队的知识库）。</p>
<blockquote>
<p>带来的好处很多，比如知识结构化、系统化、知识的传承等。</p>
</blockquote>
<p>关注每次努力后自己的成长。有些功能可能上线不久就废弃了，不用伤心，因为宝贵的知识、经验与感悟已经留在我们的身上。</p>
<h5>数据</h5>
<p>对自己负责模块的数据表现应该做到了如指掌。数据表现背后的意义是什么；如何埋点才能完善数据路径，进而准确推算用户行动链。有了完善的数据行为反馈，对于引导未来的产品方向具有重要意义。</p>
<blockquote>
<p>回来的这段日子，自己的职业方向跟数据更亲密了，接下来总结下这段时间搞得一些东东（想到哪写到哪，不苛求逻辑性。。。）。</p>
</blockquote>
<h2>Part Two 为什么回来</h2>
<blockquote>
<p>感谢文哥、剑哥、亮哥等韩都老朋友的收留。</p>
</blockquote>
<ul>
<li>
<p>横向发展的思考：</p>
<ul>
<li>对于移动端大环境的思考。</li>
<li>横向扩展：
<ul>
<li>对于AI方向的思考尝试。</li>
<li>大前端跟潮。</li>
<li>后端知识弥补。</li>
<li>项目管理尝试。</li>
<li>。。。</li>
</ul>
</li>
</ul>
</li>
<li>
<p>家庭生活因素的综合考量：</p>
<ul>
<li>家人关怀。</li>
<li>房价。。。</li>
<li>上学。。。</li>
<li>。。。</li>
</ul>
</li>
</ul>
<h2>Part Three 折腾了什么</h2>
<blockquote>
<p>一晃回来快3个月了。</p>
</blockquote>
<h3>一、对自己的几点要求</h3>
<ul>
<li>既然是跨界，开始往往会比较痛苦，坚持不退缩。</li>
<li>不为自己设限。</li>
<li>多总结多沉淀。</li>
<li>保持产品思维。</li>
</ul>
<h3>二、技术栈</h3>
<blockquote>
<p>全是一波新东西，受益匪浅。</p>
</blockquote>
<h4>1. 机器学习</h4>
<blockquote>
<p>感谢文哥的悉心教导</p>
</blockquote>
<h5>搞出来的一些东东</h5>
<ul>
<li>spark mllib流派：<a href=\"http://alithink.com/2017/11/09/%E9%94%80%E5%94%AE%E9%A2%84%E6%B5%8B%E6%A1%88%E4%BE%8B%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/\" target=\"_blank\">销售预测案例源码分析</a>
</li>
<li>sklearn流派：<a href=\"http://alithink.com/2017/11/09/%E9%94%80%E5%94%AE%E9%A2%84%E6%B5%8B%E7%88%86%E6%97%BA%EF%BC%88scikit-learn%E7%89%88%E6%9C%AC%EF%BC%89/\" target=\"_blank\">销售预测爆旺（scikit-learn版本）</a>
</li>
</ul>
<h5>几点体会</h5>
<ul>
<li>记得路上听得到音频时也讲过，做AI或者ML方向，坚实的理论基础以及对于业务的深刻理解是非常重要的。而这块也是目前我最最最欠缺的（当然其它方面也欠缺）。将来要猛补这方面知识。
<ul>
<li>推荐几本入门的好书：
<ul>
<li>spark方向： 《spark机器学习》（PACKT）</li>
<li>sklearn方向（推荐这个方向，解决方案更丰富一些）：《白话大数据与机器学习》
<ul>
<li>这本书估计高手们都不屑一顾吧，不过感觉很适合我。从最基础的理论开始讲起，对于已经把高数，概率论已经忘干净的人士来说，太有帮助了。。（部分笔记分享一下）</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<div class=\"image-package\">
<img src=\"//upload-images.jianshu.io/upload_images/2197993-e61a1d4316ce106f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" data-original-src=\"http://upload-images.jianshu.io/upload_images/2197993-e61a1d4316ce106f.png?imageMogr2/auto-orient/strip\" alt=\"白话大数据与机器学习\"><br><div class=\"image-caption\">白话大数据与机器学习</div>
</div>
<ul>
<li>良好的数据基础是必要条件。良好的周期性数据积累，对于各数据变动节点相关数据的完备与丰富，会为ML分析打下坚实的基础。</li>
<li>数据探索是个体力活。不断对比相关度，探索特征相关性。发现一条规律弥足珍贵。</li>
<li>传统的专家模式，在维度较少的情况下，还能看的过来；维度数量上来了，机器的优势就上来了。</li>
<li>各种算法都是浮云，先做LR再说。。（高手绕行）</li>
<li>结果的可解释性。。。</li>
</ul>
<h4>2. 前后端</h4>
<blockquote>
<p>我其实是vue党。。</p>
</blockquote>
<h5>智子（Flask + Bootstrap）</h5>
<div class=\"image-package\">
<img src=\"//upload-images.jianshu.io/upload_images/2197993-f0f6453357bee7ed.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" data-original-src=\"http://upload-images.jianshu.io/upload_images/2197993-f0f6453357bee7ed.jpg?imageMogr2/auto-orient/strip\" alt=\"智子效果\"><br><div class=\"image-caption\">智子效果</div>
</div>
<blockquote>
<p>使用Flask的原因: 算法模型使用sklearn搞得，同是python好基友，于是就直接上手了。</p>
</blockquote>
<ul>
<li>Flask真的很轻，搭建起来异常轻松愉快，推荐一本书《Flask Web开发：基于Python的Web应用开发实战》。
<ul>
<li>单文件也能搞定，但工程拆分之后逻辑就更清楚了（虽然麻烦不少）。</li>
<li>建议用python virtual环境来搞，可以类比为npm package, cocoapods podfile。</li>
<li>千万不要virtual clear，太恐怖了，程序一下就没有了，真的啥都不剩了。。。</li>
</ul>
</li>
<li>小应用配合WTForms，前后端表单开发效率神器。</li>
<li>python写出来的代码看起来还是很德味的，对python的好感度大为提升。</li>
<li>bootstrap程序猿UI神器。（至少看起来不会那么丑了。。）</li>
</ul>
<h5>有趣的双十一（实时Dashboard angular + nebular + Elasticsearch）</h5>
<blockquote>
<p>vue党为什么用angular: 因为想用nebular的这套主题。。。</p>
</blockquote>
<div class=\"image-package\">
<img src=\"//upload-images.jianshu.io/upload_images/2197993-faa71250183ae206.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" data-original-src=\"http://upload-images.jianshu.io/upload_images/2197993-faa71250183ae206.png?imageMogr2/auto-orient/strip\" alt=\"interesting\"><br><div class=\"image-caption\">interesting</div>
</div>
<ul>
<li>直接看Angular官方文档吧，更新速度实在太快了。。
<ul>
<li>官方的英雄实例很赞，适合入门上手。</li>
<li>typescript用起来还是很爽的。</li>
<li>整体还是略重，cli很完备，如果没有cli配个工程估计比较痛苦了。</li>
<li>先查看自身逻辑再怀疑库逻辑。。（一个ngfor绑定的问题，定位半天发现是自己逻辑写错了。。）</li>
</ul>
</li>
<li>es压秒级查询速度：
<ul>
<li>结合Angular的数据绑定，实时更新效果不错。</li>
<li>es的查询逻辑可以封装一波，利于复用。</li>
<li>小白查询编写技巧：
<ul>
<li>书籍推荐：《Elasticsearch服务器开发》（PACKT）</li>
<li>先用Elasticsearch sql生成一个基本的模板（往往是无法直接拿来用的。。），再进行调整修改。</li>
<li>查询一定要用keyword...</li>
</ul>
</li>
<li>之前试过es的morelikethis 指定匹配字段，相似度神器哈</li>
</ul>
</li>
</ul>
<h4>3. UED</h4>
<blockquote>
<p>零星接了点这方面的货，还蛮有意思的哈</p>
</blockquote>
<h5>培训视频</h5>
<div class=\"image-package\">
<img src=\"//upload-images.jianshu.io/upload_images/2197993-f9f55ce72ae86a7f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" data-original-src=\"http://upload-images.jianshu.io/upload_images/2197993-f9f55ce72ae86a7f.png?imageMogr2/auto-orient/strip\" alt=\"培训\"><br><div class=\"image-caption\">培训</div>
</div>
<h5>双十一H5直播间设计</h5>
<div class=\"image-package\">
<img src=\"//upload-images.jianshu.io/upload_images/2197993-35b6e0fe2975d120.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" data-original-src=\"http://upload-images.jianshu.io/upload_images/2197993-35b6e0fe2975d120.png?imageMogr2/auto-orient/strip\" alt=\"直播间\"><br><div class=\"image-caption\">直播间</div>
</div>
<h5>双十一数据海报</h5>
<div class=\"image-package\">
<img src=\"//upload-images.jianshu.io/upload_images/2197993-6cba5406d5a0d4d8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" data-original-src=\"http://upload-images.jianshu.io/upload_images/2197993-6cba5406d5a0d4d8.png?imageMogr2/auto-orient/strip\" alt=\"report\"><br><div class=\"image-caption\">report</div>
</div>
<h2>总结</h2>
<p>持续学习，共同成长。</p>
<p>最后预祝今晚双十一：“大吉大利，晚上吃鸡~”</p>','1531183651'),
('325515','{133619}{1291}{175237}{1014240}{1123440}{19352}{2341}{1098}{682}{1277}{1152}{25041}{542}{133662}{37462}{23780}{189436}{366350}{367779}{324780}','电机 电机将电能转化成机械能，推动转轴转动。小电机的转轴直径可达2毫米。 电机在断电运行后会产生反电动势，如果采用三极管驱动电机，则应反向并联一个硅二极管（1N4001）来吸收这个反电动势。 两种电机驱动方案如下：','电机','<p>电机将电能转化成机械能，推动转轴转动。小电机的转轴直径可达2毫米。</p>
<p><br></p>
<div class=\"image-package\">
<img class=\"uploaded-img\" src=\"//upload-images.jianshu.io/upload_images/6515247-9a3c0c80b6de855e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\"><br><div class=\"image-caption\"></div>
</div>
<p>电机在断电运行后会产生反电动势，如果采用三极管驱动电机，则应反向并联一个硅二极管（1N4001）来吸收这个反电动势。</p>
<p><br></p>
<div class=\"image-package\">
<img class=\"uploaded-img\" src=\"//upload-images.jianshu.io/upload_images/6515247-2fc170ba40451f34.gif?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\"><br><div class=\"image-caption\"></div>
</div>
<p>两种电机驱动方案如下：</p>
<p><br></p>
<div class=\"image-package\">
<img class=\"uploaded-img\" src=\"//upload-images.jianshu.io/upload_images/6515247-400ce6b86ad701c8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\"><br><div class=\"image-caption\"></div>
</div>','1531183651'),
('325550','{1283}{2047}{415}{975707}{82}{621862}{427570}{39804}{975704}{930}{825}{975796}{211}{6747}{822}{518}{1945}{1237}{975880}{975768}{975714}{975721}{975795}{975728}{1642}{975946}{963}{1493}{13643}{1148}{2049}{3316}{362}{975901}{1098}{975968}{975940}{975749}{728}{1417}{714}{189}{1293}{898}{702}{1123848}{4126}{42376}','/public\', publicPath: \'/\', historyApiFallback: true, inline: true, proxy: { \'/search/*\': { target: \'https://image.baidu.com\', changeOrigin: true } } },  部署服务器 服务器申请的为centos阿里云服务器，将打包好的静态文件部署在nginx中，nginx服务器默认监听80端口，启动nginx时可能访问不到，这时需要在“安全组规则”中添加一条规则： nginx监听80端口','实现简单组件到部署服务器——react','<div class=\"show-content-free\">
            <p>本人工作栈为dva+antd，使用阿里开源出来的组件有一段时间之后，决定不依赖阿里的框架自己打个环境写一些组件出来，一来为了熟悉工作流程，二来也是为了更好的理解原理，从而更好的满足工作中的需求</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 690px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 43.95%;\"></div>
<div class=\"image-view\" data-width=\"1570\" data-height=\"690\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/5444167-4e2fdb495be91084.gif\" data-original-width=\"1570\" data-original-height=\"690\" data-original-format=\"image/gif\" data-original-filesize=\"3181850\"></div>
</div>
<div class=\"image-caption\">二次元萌图</div>
</div>
<blockquote>
<p>源码地址：<a href=\"https://link.jianshu.com?t=https%3A%2F%2Fgithub.com%2Fjdkwky%2Fmy-react-example\" target=\"_blank\" rel=\"nofollow\">https://github.com/jdkwky/my-react-example</a></p>
<p>预览地址：<a href=\"https://link.jianshu.com?t=http%3A%2F%2F47.94.218.152\" target=\"_blank\" rel=\"nofollow\">http://47.94.218.152</a>  (目前只兼容chrome和IEedge)</p>
</blockquote>
<h3>1. 技术点 react + webpack + antd</h3>
<h3>2. 学习流程</h3>
<ul>
<li>首先电脑上要有nodejs环境，本文并不涉及nodejs，主要是用nodejs环境中的npm安装需要的依赖包（nodejs安装即可，npm如果在下载包的时候很慢，可以将镜像换成淘宝的镜像）</li>
<li>npm i create-react-app -g 全局安装create-react-app脚手架</li>
<li>create-react-app创建自己的工程文件</li>
<li>写自己的webpack配置文件</li>
<li>跨域请求时，在webpack中配置代理，在服务器上采用nginx代理转发</li>
<li>打包</li>
<li>部署到服务器中</li>
</ul>
<h3>3. 详解</h3>
<p>安装nodejs环境和用create-react-app创建react脚手架过程省略，因为比较简单，安装都比较方便，不会出什么问题</p>
<ul>
<li>
<p>webpack配置文件</p>
<ol>
<li>对webpack简单的理解可以参考博文：<a href=\"https://link.jianshu.com?t=https%3A%2F%2Fblog.csdn.net%2Fwangkaiyuan110%2Farticle%2Fdetails%2F78653389\" target=\"_blank\" rel=\"nofollow\">webpack初印象</a>；</li>
<li>在本程序中的一个难点在于既想要引入antd的样式文件，又想要使用css modules,本程序中的解决方案就是针对不同目录中的css文件分别进行匹配，antd组件中的样式文件一定都在node_modules文件夹中，而本地自己写的文件一定都在src文件夹中，具体解决方案详见<a href=\"https://link.jianshu.com?t=https%3A%2F%2Fgithub.com%2Fjdkwky%2Fmy-react-example%2Fblob%2Fmaster%2Fwebpack.config.js\" target=\"_blank\" rel=\"nofollow\">webpack.config.js</a>；</li>
<li>在文件配置中遇到的坑详见 <a href=\"https://www.jianshu.com/p/9d812ece6cda\" target=\"_blank\">react-webpack-antd--环境篇</a>；</li>
<li>遇到跨域问题在本地的解决方案为在本地配置代理：</li>
</ol>
<pre><code>devServer: {
contentBase: \'./public\',
publicPath: \'/\',
historyApiFallback: true,
inline: true,
proxy: {
  \'/search/*\': {
    target: \'https://image.baidu.com\',
    changeOrigin: true
  }
}
},
&lt;!--当访问/search/路径下的所有url时，均走image.baidu.com这个域名--&gt;
</code></pre>
</li>
<li>
<p>部署服务器</p>
<p>服务器申请的为centos阿里云服务器，将打包好的静态文件部署在nginx中，nginx服务器默认监听80端口，启动nginx时可能访问不到，这时需要在“安全组规则”中添加一条规则：</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 191px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 8.82%;\"></div>
<div class=\"image-view\" data-width=\"2165\" data-height=\"191\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/5444167-9fadb851e28785f9.png\" data-original-width=\"2165\" data-original-height=\"191\" data-original-format=\"image/png\" data-original-filesize=\"20331\"></div>
</div>
<div class=\"image-caption\">nginx监听80端口</div>
</div>
</li>
</ul>

          </div>','1531183652'),
('325551','{975837}{9513}{787629}{5275}{408}{14844}{1149}{975788}{975714}{1050}{975761}{975776}{975790}{688}{1417}{976136}{542}{975707}{341}{2421}{2289}{1123856}{4101}{1123858}{4850}{1273}{1049}{761}{715}{1493}{975757}{4987}{975784}{7684}{2697}{258}{4096}{37142}{222708}{40007}{764}{222713}{63847}{1123861}{1263}{975728}{975738}{975865}','function sagaMiddleware({ getState, dispatch }) { const channel = stdChannel() channel.put = (options.emitter || identity)(channel.put) sagaMiddleware.run = runSaga.bind(null, { context, channel, dispatch, getState, sagaMonitor, logger, onError, effectMiddlewares, }) return next = action = { if (sagaMonitor   sagaMonitor.actionDispatched) { sagaMonitor.actionDispatched(action) } const result = next(action) // hit reducers channel.put(action) return result } } . Redux-saga 中文文档','redux-saga 初识','<div class=\"show-content-free\">
            <p><a href=\"https://link.jianshu.com?t=http%3A%2F%2Fwww.codedata.cn%2Fhacknews%2F152047495245427871\" target=\"_blank\" rel=\"nofollow\">原文链接</a><br>
如果感兴趣可以加我微信: xiaobei060537, 一起交流。<br>
redux-saga 是一个管理 Redux 应用异步操作的中间件，功能类似<code>redux-thunk + async/await</code>, 它通过创建 Sagas 将所有的异步操作逻辑存放在一个地方进行集中处理。</p>
<h3>redux-saga 的 effects</h3>
<p>redux-saga中的 Effects 是一个纯文本 JavaScript 对象，包含一些将被 saga middleware 执行的指令。这些指令所执行的操作包括如下三种：</p>
<ul>
<li>发起一个异步调用（如发一起一个 Ajax 请求）</li>
<li>发起其他的 action 从而更新 Store</li>
<li>调用其他的 Sagas</li>
</ul>
<p>Effects 中包含的指令有很多，具体可以异步<a href=\"https://link.jianshu.com?t=https%3A%2F%2Fredux-saga-in-chinese.js.org%2Fdocs%2Fapi%2Findex.html%23takeeverypattern-saga-args\" target=\"_blank\" rel=\"nofollow\">API 参考</a>进行查阅</p>
<h3>redux-saga 的特点</h3>
<ul>
<li>方便测试，例如:</li>
</ul>
<pre><code class=\"swift\">assert.deepEqual(iterator.next().value, call(Api.fetch, \'/products\'))
</code></pre>
<ul>
<li>action 可以保持其纯净性,异步操作集中在 saga 中进行处理</li>
<li>watch/worker（监听-&gt;执行） 的工作形式</li>
<li>被实现为 generator</li>
<li>对含有复杂异步逻辑的应用场景支持良好</li>
<li>更细粒度地实现异步逻辑，从而使流程更加清晰明了，遇到 bug 易于追踪和解决。</li>
<li>以同步的方式书写异步逻辑，更符合人的思维逻辑</li>
</ul>
<h3>从 redux-thunk 到 redux-saga</h3>
<p>假如现在有一个场景：用户在登录的时候需要验证用户的 username 和 password 是否符合要求。</p>
<h4>使用 redux-thunk 实现</h4>
<p>获取用户数据的逻辑（user.js):</p>
<pre><code class=\"swift\">// user.js

import request from \'axios\';

// define constants
// define initial state
// export default reducer

export const loadUserData = (uid) =&gt; async (dispatch) =&gt; {
    try {
        dispatch({ type: USERDATA_REQUEST });
        let { data } = await request.get(`/users/${uid}`);
        dispatch({ type: USERDATA_SUCCESS, data });
    } catch(error) {
        dispatch({ type: USERDATA_ERROR, error });
    }
}
</code></pre>
<p>验证登录的逻辑(login.js):</p>
<pre><code class=\"swift\">import request from \'axios\';
import { loadUserData } from \'./user\';

export const login = (user, pass) =&gt; async (dispatch) =&gt; {
    try {
        dispatch({ type: LOGIN_REQUEST });
        let { data } = await request.post(\'/login\', { user, pass });
        await dispatch(loadUserData(data.uid));
        dispatch({ type: LOGIN_SUCCESS, data });
    } catch(error) {
        dispatch({ type: LOGIN_ERROR, error });
    }
}
</code></pre>
<h4>redux-saga</h4>
<p>异步逻辑可以全部写进 saga.js 中：</p>
<pre><code class=\"swift\">export function* loginSaga() {
  while(true) {
    const { user, pass } = yield take(LOGIN_REQUEST) //等待 Store 上指定的 action LOGIN_REQUEST
    try {
      let { data } = yield call(loginRequest, { user, pass }); //阻塞，请求后台数据
      yield fork(loadUserData, data.uid); //非阻塞执行loadUserData
      yield put({ type: LOGIN_SUCCESS, data }); //发起一个action，类似于dispatch
    } catch(error) {
      yield put({ type: LOGIN_ERROR, error });
    }  
  }
}

export function* loadUserData(uid) {
  try {
    yield put({ type: USERDATA_REQUEST });
    let { data } = yield call(userRequest, `/users/${uid}`);
    yield put({ type: USERDATA_SUCCESS, data });
  } catch(error) {
    yield put({ type: USERDATA_ERROR, error });
  }
}
</code></pre>
<h3>难点解读</h3>
<p>对于 redux-saga， 还是有很多比较难以理解和晦涩的地方，下面笔者针对自己觉得比较容易混淆的概念进行整理：</p>
<h4>take 的使用</h4>
<p>take 和 takeEvery 都是监听某个 action, 但是两者的作用却不一致，takeEvery 是每次 action 触发的时候都响应，而 take 则是执行流执行到 take 语句时才响应。takeEvery 只是监听 action, 并执行相对应的处理函数，对何时执行 action 以及如何响应 action 并没有多大的控制权，被调用的任务无法控制何时被调用，并且它们也无法控制何时停止监听，它只能在每次 action 被匹配时一遍又一遍地被调用。但是 take 可以在 generator 函数中决定何时响应一个 action 以及 响应后的后续操作。<br>
例如在监听所有类型的 action 触发时进行 logger 操作，使用 takeEvery 实现如下：</p>
<pre><code class=\"swift\">import { takeEvery } from \'redux-saga\'

function* watchAndLog(getState) {
  yield* takeEvery(\'*\', function* logger(action) {
      //do some logger operation //在回调函数体内
  })
}
</code></pre>
<p>使用 take 实现如下：</p>
<pre><code class=\"swift\">import { take } from \'redux-saga/effects\'

function* watchAndLog(getState) {
  while(true) {
    const action = yield take(\'*\')
    //do some logger operation //与 take 并行 
  })
}
</code></pre>
<p>其中 <code>while(true)</code> 的意思是一旦到达流程最后一步（logger），通过等待一个新的任意的 action 来启动一个新的迭代（logger 流程）。</p>
<h4>阻塞和非阻塞</h4>
<p>call 操作是用来发起异步操作的，对于 generator 来说，call 是阻塞的操作，它在 Generator 调用结束之前不能执行或处理任何其他事情。，但是 fork 却是非阻塞操作，当 fork 调动任务时，该任务会在后台执行，此时的执行流可以继续往后面执行而不用等待结果返回。</p>
<p>例如如下的登录场景:</p>
<pre><code class=\"swift\">function* loginFlow() {
  while(true) {
    const {user, password} = yield take(\'LOGIN_REQUEST\')
    const token = yield call(authorize, user, password)
    if(token) {
      yield call(Api.storeItem({token}))
      yield take(\'LOGOUT\')
      yield call(Api.clearItem(\'token\'))
    }
  }
}
</code></pre>
<p>若在 call 在去请求 authorize 时，结果未返回，但是此时用户又触发了 LOGOUT 的 action,此时的 LOGOUT 将会被忽略而不被处理，因为 loginFlow 在 authorize 中被堵塞了，没有执行到 <code>take(\'LOGOUT\')</code>那里</p>
<h4>同时执行多个任务</h4>
<p>如若遇到某个场景需要同一时间执行多个任务，比如 请求 users 数据 和 products 数据, 应该使用如下的方式：</p>
<pre><code class=\"swift\">import { call } from \'redux-saga/effects\'
//同步执行
const [users, products] = yield [
  call(fetch, \'/users\'),
  call(fetch, \'/products\')
]

//而不是
//顺序执行
const users = yield call(fetch, \'/users\'),
      products = yield call(fetch, \'/products\')
</code></pre>
<p>当 yield 后面是一个数组时，那么数组里面的操作将按照 <code>Promise.all</code> 的执行规则来执行，genertor 会阻塞知道所有的 effects 被执行完成</p>
<h3>源码解读</h3>
<p>在每一个使用 redux-saga 的项目中，主文件中都会有如下一段将 sagas 中间件加入到 Store 的逻辑:</p>
<pre><code class=\"swift\">const sagaMiddleware = createSagaMiddleware({sagaMonitor})
const store = createStore(
  reducer,
  applyMiddleware(sagaMiddleware)
)
sagaMiddleware.run(rootSaga)
</code></pre>
<p>其中 createSagaMiddleware 是 redux-saga 核心源码文件 src/middleware.js 中导出的方法：</p>
<pre><code class=\"swift\">export default function sagaMiddlewareFactory({ context = {}, ...options } = {}) {
 ...
 
 function sagaMiddleware({ getState, dispatch }) {
    const channel = stdChannel()
    channel.put = (options.emitter || identity)(channel.put)

    sagaMiddleware.run = runSaga.bind(null, {
      context,
      channel,
      dispatch,
      getState,
      sagaMonitor,
      logger,
      onError,
      effectMiddlewares,
    })

    return next =&gt; action =&gt; {
      if (sagaMonitor &amp;&amp; sagaMonitor.actionDispatched) {
        sagaMonitor.actionDispatched(action)
      }
      const result = next(action) // hit reducers
      channel.put(action)
      return result
    }
  }
 ...
 
 }
</code></pre>
<p>这段逻辑主要是执行了 <code>sagaMiddleware()</code>，该函数里面将 runSaga 赋值给 sagaMiddleware.run 并执行，最后返回 middleware。 接着看 runSaga() 的逻辑：</p>
<pre><code class=\"swift\">export function runSaga(options, saga, ...args) {
...
  const task = proc(
    iterator,
    channel,
    wrapSagaDispatch(dispatch),
    getState,
    context,
    { sagaMonitor, logger, onError, middleware },
    effectId,
    saga.name,
  )

  if (sagaMonitor) {
    sagaMonitor.effectResolved(effectId, task)
  }

  return task
}
</code></pre>
<p>这个函数里定义了返回了一个 task 对象，该 task 是由 proc 产生的，移步 proc.js：</p>
<pre><code class=\"swift\">export default function proc(
  iterator,
  stdChannel,
  dispatch = noop,
  getState = noop,
  parentContext = {},
  options = {},
  parentEffectId = 0,
  name = \'anonymous\',
  cont,
) {
  ...
  const task = newTask(parentEffectId, name, iterator, cont)
  const mainTask = { name, cancel: cancelMain, isRunning: true }
  const taskQueue = forkQueue(name, mainTask, end)
  
  ...
  
  next()
  
  return task

  function next(arg, isErr){
  ...
      if (!result.done) {
        digestEffect(result.value, parentEffectId, \'\', next)
      } 
  ...
  }
}
</code></pre>
<p>其中 digestEffect 就执行了 <code>effectTriggerd()</code> 和 <code>runEffect()</code>,也就是执行 effect，其中 runEffect() 中定义了不同 effect 执行相对应的函数，每一个 effect 函数都在 proc.js 实现了。</p>
<p>除了一些核心方法之外，redux-saga 还提供了一系列的 helper 文件，这些文件的作用是返回一个类 iterator 的对象，便于后续的遍历和执行, 在此不具体分析。</p>
<h3>参考文档</h3>
<ul>
<li><a href=\"https://link.jianshu.com?t=https%3A%2F%2Fgithub.com%2FsuperRaytin%2Fredux-saga-in-chinese\" target=\"_blank\" rel=\"nofollow\">Redux-saga 中文文档</a></li>
<li><a href=\"https://link.jianshu.com?t=https%3A%2F%2Fgithub.com%2FPines-Cheng%2Fblog%2Fissues%2F9\" target=\"_blank\" rel=\"nofollow\">从redux-thunk到redux-saga实践</a></li>
</ul>

          </div>','1531183653'),
('325552','{3047}{10129}{975714}{559}{1123862}{62}{158}{1123863}{1882}{975717}{1123864}{975761}{1928}{442}{1528}{1123865}{476308}{4839}{4921}{5179}{1123868}{63402}{975728}{36}{1530}{975757}{387681}{242860}{190}{1884}{35698}{366163}{1123872}{8869}{975835}{975779}{975786}{394}{975721}{2370}{975878}{975715}{14859}{976136}{975842}{975969}{1747}{1123876}{975915}{1557}','h5）很高（h代表英雄）。但是，对这个联合概率进行建模并没有非常简单。或者，我们可以尝试使用最大化条件概率P（h0 | h1，h2，.. h5）来建模。由于游戏的预测只是P（结果| h0，h1，.. h5）与word2vec中连续词袋（CBOW）模型中的P（中心词语｜上下文词语）完全相同。不同于单词的是，（h1，h2，..','使用机器学习预测电子竞技游戏《守望先锋》的胜负','<div class=\"show-content-free\">
            <p><i>摘要：</i>机器学习可以预测游戏的输赢？来看看Bowen Yang博士是如何构建这一模型的。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 410px; max-height: 170px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 41.46%;\"></div>
<div class=\"image-view\" data-width=\"410\" data-height=\"170\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2509688-b3cfe8fd54ac92c0.jpeg\" data-original-width=\"410\" data-original-height=\"170\" data-original-format=\"image/jpeg\" data-original-filesize=\"47833\"></div>
</div>
<div class=\"image-caption\">

《守望先锋》中的英雄

</div>
</div>
<p>来自加州大学河滨分校的物理学博士学位的Bowen Yang正在致力于构建一个模型——对游戏中的人物特征进行有意义的学习，来预测电子竞技游戏中的获胜团队。这个方法广泛适用于任何具有结构化数据的业务。</p>
<p>现在，电子竞技游戏是一个有着巨大潜力且不断上升的市场。去年，在英雄联盟的世界冠军赛中，仅仅一场半决赛就吸引了1.06亿观众，甚至超过了2018年的超级碗（美国职业橄榄球大联盟年度冠军赛）。为玩家提供个性化游戏分析的公司<a href=\"https://visor.gg/\" target=\"_blank\" rel=\"nofollow\">Visor</a>，就希望能够有一个可以实时预测团队胜率的模型。</p>
<p><b>预测比赛</b></p>
<p>预测模型有很多种用途。比如，它可以向玩家提供有效反馈，帮助他们提高技能；对于玩家，它可以成为一个很好的参与工具，来吸引那些不熟悉游戏规则的潜在玩家；另外，如果一个模型在预测方面能够超越人类，那么它在电子竞技下注方面就会有着前所未有的潜力。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 410px; max-height: 273px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 66.59%;\"></div>
<div class=\"image-view\" data-width=\"410\" data-height=\"273\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2509688-68347ef8fd06b900.jpeg\" data-original-width=\"410\" data-original-height=\"273\" data-original-format=\"image/jpeg\" data-original-filesize=\"95381\"></div>
</div>
<div class=\"image-caption\">

DOTA2国际邀请赛现场

</div>
</div>
<p><b>《守望先锋》简介</b></p>
<p>我们今天建模的对象是《守望先锋》——一款基于团队的多人在线射击游戏。每个队伍有六名玩家，每位玩家从英雄列表（26名英雄）中选择一个英雄（游戏角色，如超级马里奥），与另一队进行战斗，每场游戏都有特定的游戏地图（游戏开始之前就已设定）。</p>
<p>游戏中有很多因素会影响游戏的预测结果，其中大部分是分类特征。举个例子，英雄的选择对于游戏的前期有着很大的作用。因此，我们面临的挑战是：如何处理这些分类特征。如果我只使用一种热编码，那么特征空间可以很轻松地增长到数百个维度。不幸的是，收集足够多的游戏数据来满足这个高纬度特征空间，这几乎是不可能的。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 410px; max-height: 354px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 86.33999999999999%;\"></div>
<div class=\"image-view\" data-width=\"410\" data-height=\"354\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2509688-3d904fae0c15f170.jpeg\" data-original-width=\"410\" data-original-height=\"354\" data-original-format=\"image/jpeg\" data-original-filesize=\"27943\"></div>
</div>
<div class=\"image-caption\">

纵轴和横轴分别为预测准确度与游戏进度，使用热编码和特征选择的逻辑回归对预测进行建模。在游戏接近尾声时，预测较为准确；但在游戏开始时，预测几乎是一个随机值（具有0.5的准确性）。

</div>
</div>
<p>本文将重点介绍如何使用<b>嵌入</b>对这些游戏角色进行建模，以及如何提升预测的准确度。</p>
<p>有关更多细节和实现，请参阅我的<a href=\"https://github.com/ybw9000/hero2vec\" target=\"_blank\" rel=\"nofollow\">Github链接</a>。</p>
<p><b>多个英雄可以组成一个队伍（</b><b>“</b><b>复仇者</b><b>”</b><b>）</b></p>
<p>从《魔兽世界》等角色扮演游戏到Dota 2、LoL和Overwatch等战斗类游戏，团队是现代多人在线视频游戏的核心概念，而英雄则是队伍的基础。</p>
<p>《守望先锋》中的英雄可以分为三类：进攻（DPS）、防御（坦克）和辅助，每个英雄都有自己的强项和弱点。一个团队应该保持英雄成员的平衡（所以没有特定的阵容）、配合（团队配合非常重要），根据当前的作战地图和英雄的技能水平形成团队策略。这和篮球比较比较相似，需要后卫、中锋和前锋合作。因此，团队的组合需要有一定的模式，甚至某个英雄可以在一个团队中共同出现。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 410px; max-height: 231px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 56.34%;\"></div>
<div class=\"image-view\" data-width=\"410\" data-height=\"231\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2509688-dd821e860b0cfd13.jpeg\" data-original-width=\"410\" data-original-height=\"231\" data-original-format=\"image/jpeg\" data-original-filesize=\"72195\"></div>
</div>
<div class=\"image-caption\">

典型的均衡团队需要有2名防御（坦克）、2名进攻（DPS）和2名辅助。

</div>
</div>
<p><b>多个单词可以组成一个句子</b></p>
<p>我们可以从英雄和单词的类比中得出某些结论。一个单词本身有自己的含义，如果形成一个句子或一篇文章，那么，它的意义更大。同样地，英雄本身也有自己的“含义”和特征，比如一些英雄攻击力强、一些英雄则擅长防守，如果二者组成一个团队，那么，他们的角色会变得更加复杂。</p>
<p>以前，单词是用一个热编码建模的，这种编码很大程度上受到<a href=\"https://en.wikipedia.org/wiki/Curse_of_dimensionality?spm=a2c4e.11153940.blogcont603861.6.79245777xMVRN7\" target=\"_blank\" rel=\"nofollow\">高纬灾难</a>的影响，因为词汇量太大，以至于特征空间的维度可能很容易就超过数十万。一个热编码简单地假设单词之间彼此独立，即它们的表示（representations）是相互正交的，它并不捕获单词在句子中的含义。另一方面，单词也可以表示为<a href=\"http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf?spm=a2c4e.11153940.blogcont603861.7.79245777xMVRN7&amp;file=bengio03a.pdf\" target=\"_blank\" rel=\"nofollow\">分布式表示</a>。这样，单词的语义可以通过更低维的矢量（嵌入）来捕获。</p>
<p>当用语词的分布式表示的算法是著名的<a href=\"https://arxiv.org/pdf/1301.3781.pdf?spm=a2c4e.11153940.blogcont603861.8.79245777xMVRN7&amp;file=1301.3781.pdf\" target=\"_blank\" rel=\"nofollow\">word2vec模型</a>。</p>
<p><b>超越</b><b>word2vec</b></p>
<p>为了利用嵌入的优势，我们应该考虑以下几个事项：</p>
<p><b>1.</b><b>相似性</b>：相似性代表了输入之间的“重叠”。例如，“国王”和“女王”代表统治者。输入的重叠越多，它们的嵌入就越密（更小的维度）。换句话说，必须有不同输入到相同输出的映射。如果输入是相互正交的，那么嵌入就没有任何意义了。</p>
<p><b>2.</b><b>训练任务</b>：嵌入是从训练任务中（预）学习的。训练任务应该与我们自己的任务相关，因此嵌入的信息是可转移的。例如，word2vec在Google新闻上进行训练，然后用于机器翻译。它们是相关的，因为它们的词语具有相同的语义含义。</p>
<p><b>3.</b><b>大量的数据：</b>为了找到输入数据之间的相似性或关系，我们需要大量数据来探索高维度空间。因为有大量的可用于无监督学习的数据，分布式表示可以减少维度背后的“黑魔法”。例如，word2vec模型在数十亿字上进行训练。在一定程度上，嵌入仅仅是独热编码输入和下行任务之间的附加线性层的权重。为了训练包括嵌入层的整个管道，我们仍然需要大量数据来填充高维度输入空间。<b>Continuous bag ofheroes</b><b>模型</b></p>
<p>考虑完以上几个问题，我们现在开始设计Hero2vec模型。</p>
<p><b>1.</b><b>相似性</b>：如前所述，《守望先锋》中的英雄属于某些类别。这种相似性表明它们可以通过分布式表示来描述，而不是一个热正交编码。</p>
<p><b>2.</b><b>训练任务</b>：通过对中心词和上下文词的共现进行建模，word2vec试图来捕捉单词的一般语义含义。同样，高协作性的英雄很可能会在一个团队中同时出现，即联合概率P（h0，h1，... h5）很高（h代表英雄）。但是，对这个联合概率进行建模并没有非常简单。或者，我们可以尝试使用最大化条件概率P（h0 | h1，h2，... h5）来建模。由于游戏的预测只是P（结果| h0，h1，... h5，其他因素），因此这两个任务是高度相关。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 410px; max-height: 231px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 56.34%;\"></div>
<div class=\"image-view\" data-width=\"410\" data-height=\"231\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2509688-2824f8d414795b73.jpeg\" data-original-width=\"410\" data-original-height=\"231\" data-original-format=\"image/jpeg\" data-original-filesize=\"72195\"></div>
</div>
<div class=\"image-caption\">

给定一个团队中的五个英雄，我们就可以预测出生存到最后的的英雄。例如，如果一支球队已经有2名后卫，2名中锋和1名前锋，那么最后一名球员很有可能成为球队的前锋。

</div>
</div>
<p><b>3.</b><b>数据</b>：<a href=\"http://visor.gg/?spm=a2c4e.11153940.blogcont603861.9.79245777xMVRN7\" target=\"_blank\" rel=\"nofollow\">Visor</a>提供了超过30,000多种团队组合用于预训练嵌入。与数十亿的单词相比，30,000个组合可能看起来很小，同样，输入维度也比词汇表中的单词（例如260,000+）要小的多（26英雄）。考虑到训练数据的需求随维度呈指数增长，实际上，30,000个组合足够进行训练。</p>
<p><b>4.</b><b>模型</b>：概率P（h0 | h1，h2，... h5）与word2vec中连续词袋（CBOW）模型中的P（中心词语｜上下文词语）完全相同。不同于单词的是，（h1，h2，... h5）之间相互置换，并不会影响概率，因此（h1，h2，... h5）的嵌入总和实际上就是输入总和。在这里，除了P（h0 | h1，h2，... h5）外，我们还可以对P（h1 | h0，h2，... h5）等进行建模，使数据集可以有效的扩展6次。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 410px; max-height: 231px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 56.34%;\"></div>
<div class=\"image-view\" data-width=\"410\" data-height=\"231\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2509688-40f511fec4738f1f.jpeg\" data-original-width=\"410\" data-original-height=\"231\" data-original-format=\"image/jpeg\" data-original-filesize=\"25499\"></div>
</div>
<div class=\"image-caption\">

Hero2vec的模型架构，包括嵌入层、全连接神经网络和softmax层。由于softmax层只有26个目标，所以不需要负采样。

</div>
</div>
<p><b>英雄的可视化处理</b></p>
<p>可以将英雄的嵌入（10个维度）投影到二维平面上（使用PCA），实现可视化，如下图所示。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 410px; max-height: 302px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 73.66%;\"></div>
<div class=\"image-view\" data-width=\"410\" data-height=\"302\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2509688-d3c01afc7c1878f9.jpeg\" data-original-width=\"410\" data-original-height=\"302\" data-original-format=\"image/jpeg\" data-original-filesize=\"23125\"></div>
</div>
<div class=\"image-caption\">

英雄的嵌入（投影到二维平面上）

</div>
</div>
<p>显然，嵌入成功地捕捉了英雄背后的游戏设计。英雄根据自己的角色或类别进行聚类。更有意思的是，嵌入还可以捕捉英雄超越其类别内其它英雄的更微妙的特征。例如，尽管Roadhog英雄是防御（坦克），玩家仍然把它看作进攻（DPS）；虽然Symmetra是辅助，但它并不能治愈队友，所以她更接近进攻（DPS）和防御（坦克）等。玩家并不像游戏设计师所认为的那样，将它们视为两类。对于熟悉《守望先锋》的玩家来说，进攻型DPS和防御性DPS之间的界限非常模糊，也就是说，玩家并没有根据游戏设计的本意，将它们归为两类。</p>
<p>因此，与硬编码类别的英雄（或产品）相比，在捕捉英雄的特征或属性时，嵌入可以更加流畅和准确的对其进行捕捉，即玩家和游戏设计者都能从嵌入中提取更多有用的信息。玩家可以用这个模型来更好地理解或欣赏该游戏，而游戏设计师也可以利用该模型对游戏设计进行验证和改进。</p>
<p><b>Map2vec</b></p>
<p>我们已经讨论过了如何在游戏中模拟英雄。在介绍英雄嵌入是如何帮助我们预测游戏胜负之前，我想简单地谈谈如何处理另一个分类特征——地图。</p>
<p>《守望先锋》的每场游戏都是在特定的游戏地图上进行的，而团队的组合取决于地图的布局，即P（团队|地图）。通过贝叶斯规则重写，P（团队|地图）P（地图|团队）P（团队）。因此，我们可以用P（地图|团队）来嵌入地图，如下所示。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 410px; max-height: 231px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 56.34%;\"></div>
<div class=\"image-view\" data-width=\"410\" data-height=\"231\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2509688-7e2bb03eba94e857.jpeg\" data-original-width=\"410\" data-original-height=\"231\" data-original-format=\"image/jpeg\" data-original-filesize=\"20722\"></div>
</div>
<div class=\"image-caption\">

map2vec的模型结构。包括英雄的嵌入层、全连接神经网络和softmax层。softmax层的权重是地图的嵌入。

</div>
</div>
<p>与上面的Hero2vec模型不同的是：映射的嵌入是从最后一个线性层绘制的，word2vec模型的输入嵌入和输出嵌入都可以用来代表单词。</p>
<p>同样的，地图的嵌入也可以进行可视化。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 410px; max-height: 302px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 73.66%;\"></div>
<div class=\"image-view\" data-width=\"410\" data-height=\"302\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2509688-e44c1f373661795c.jpeg\" data-original-width=\"410\" data-original-height=\"302\" data-original-format=\"image/jpeg\" data-original-filesize=\"23125\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p><b>地图的可视化</b></p>
<p>通过嵌入，我们可以很好地理解地图背后的游戏设计。对于那些熟悉《守望先锋》的玩家来说，能够看到单个地图的进攻区域和防御区域之间的差异，这比查看地图之间的差异要更有意思。</p>
<p>相同的体系结构可以对任何共同出现的事务进行建模。例如，输入为一堆电影，目标为喜欢这些电影的特定客户。训练这个管道，就可以为我们提供电影和客户的嵌入。</p>
<p><b>使用英雄嵌入来预测游戏的胜负</b></p>
<p>使用英雄嵌入，可以提高游戏预测的准确度，如下图所示：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 410px; max-height: 231px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 56.34%;\"></div>
<div class=\"image-view\" data-width=\"410\" data-height=\"231\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2509688-a984cd3001ba60e6.jpeg\" data-original-width=\"410\" data-original-height=\"231\" data-original-format=\"image/jpeg\" data-original-filesize=\"20722\"></div>
</div>
<div class=\"image-caption\">

纵轴和横轴分别为预测准确度与游戏进度。用Hero2vec嵌入，该逻辑回归模型可以提高游戏前期的预测准确度。

</div>
</div>
<p>如上图所示，二者都使用逻辑回归，当输入为英雄嵌入时，预测的准确度要比输入为一个热编码时高。更值得一提的是，英雄嵌入的确可以提升游戏前期或中期的预测准确度。</p>
<p>团队中英雄的组合能够为模型提供很多信息，这其中的一个原因就是，在游戏开始时，数字特征几乎不会有任何变化，因此，在游戏前期，数字特征基本上没有什么用处。随着游戏进入中期，数字特征种会积累更多的信息，这样一来，团队中英雄的组合形式就不再那么重要了。当游戏打到后期时，两个预测结果重叠，因为数值特征中的值足够多，足以来预测游戏结果。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 410px; max-height: 317px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 77.32%;\"></div>
<div class=\"image-view\" data-width=\"410\" data-height=\"317\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2509688-cc4ba0c55d94b253.jpeg\" data-original-width=\"410\" data-original-height=\"317\" data-original-format=\"image/jpeg\" data-original-filesize=\"57124\"></div>
</div>
<div class=\"image-caption\">

游戏结果与两个重要的数字特征。在游戏前期时（左下角），特征差异不大，结果几乎是重叠的。随着游戏继续进展（朝右上角），方差逐渐变大，预测结果也可以很容易的分开。

</div>
</div>
<p><b>总结</b></p>
<p>本文讨论了如何用低维分布表示来表示高维分类特征，并遵循<a href=\"http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf?spm=a2c4e.11153940.blogcont603861.10.79245777xMVRN7&amp;file=bengio03a.pdf\" target=\"_blank\" rel=\"nofollow\">NLP</a>和<a href=\"https://arxiv.org/pdf/1301.3781.pdf?spm=a2c4e.11153940.blogcont603861.11.79245777xMVRN7&amp;file=1301.3781.pdf\" target=\"_blank\" rel=\"nofollow\">word2vec算法</a>的逻辑。</p>
<p>通过对《守望先锋》中的英雄进行预训练，我构建了一个可以预测游戏胜负的可靠模型。并且，该模型在游戏前期的预测准确率更高，更为详细的模型和代码请查看我的<a href=\"https://github.com/ybw9000/hero2vec?spm=a2c4e.11153940.blogcont603861.12.79245777xMVRN7\" target=\"_blank\" rel=\"nofollow\">Git库</a>。</p>
<p><b>文章原标题《</b>Predicting e-sports winners with Machine Learning<b>》</b></p>
<p><b><a href=\"http://click.aliyun.com/m/1000003827/\" target=\"_blank\" rel=\"nofollow\">原文链接</a></b></p>
<p>本文为云栖社区原创内容，未经允许不得转载。</p>
          </div>','1531183655'),
('325553','{17}{975757}{1123884}{36}{192}{975761}{247}{976193}{975962}{2926}{191412}{1123886}{40712}{2208}{9414}{975935}{503}{3833}{1072}{975789}{4811}{975892}{1200}{775}{975786}{8052}{733}{2430}{975829}{356}{2217}{454}{6812}{1231}{975714}{11039}{378}{975721}{232}{975891}{2219}{1841}{975733}{976114}{975879}{712}{1642}{4467}{1747}{975715}','深度学习：将新闻报道按照不同话题性质进行分类 深度学习的广泛运用之一就是对文本按照其内容进行分类。例如对新闻报道根据其性质进行划分是常见的应用领域。在本节，我们要把路透社自1986年以来的新闻数据按照46个不同话题进行划分。网络经过训练后，它能够分析一篇新闻稿，然后按照其报道内容，将其归入到设定好的46个话题之一。深度学习在这方面的应用属于典','深度学习：将新闻报道按照不同话题性质进行分类','<div class=\"show-content-free\">
            <p>深度学习的广泛运用之一就是对文本按照其内容进行分类。例如对新闻报道根据其性质进行划分是常见的应用领域。在本节，我们要把路透社自1986年以来的新闻数据按照46个不同话题进行划分。网络经过训练后，它能够分析一篇新闻稿，然后按照其报道内容，将其归入到设定好的46个话题之一。深度学习在这方面的应用属于典型的“单标签，多类别划分”的文本分类应用。</p>
<p>我们这里采用的数据集来自于路透社1986年以来的报道，数据中每一篇新闻稿附带一个话题标签，以用于网络训练，每一个话题至少含有10篇文章，某些报道它内容很明显属于给定话题，有些报道会模棱两可，不好确定它到底属于哪一种类的话题，我们先把数据加载到机器里，代码如下：</p>
<pre><code>from keras.datasets import reuters
(train_data, train_label), (test_data, test_labels) = reuters.load_data(num_words=10000)
</code></pre>
<p>keras框架直接附带了相关数据集，通过执行上面代码就可以将数据下载下来。上面代码运行后结果如下：</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 608px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 26.340000000000003%;\"></div>
<div class=\"image-view\" data-width=\"2308\" data-height=\"608\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2849961-7229f329f2f648ad\" data-original-width=\"2308\" data-original-height=\"608\" data-original-format=\"image/png\" data-original-filesize=\"91357\"></div>
</div>
<div class=\"image-caption\">这里写图片描述</div>
</div>
<br>
<p>从上面运行结果看，它总共有8982条训练数据和2246条测试数据。跟我们上节数据类型一样，数据里面对应的是每个单词的频率编号，我们可以通过上一节类似的代码，将编号对应的单词从字典中抽取出来结合成一篇文章，代码如下：</p>

<pre><code>word_index = reuters.get_word_index()
reverse_word_index = dict([value, key] for (key, value) in word_index.items())
decoded_newswire = \' \'.join([reverse_word_index.get(i-3, \'?\') for i in train_data[0]])
print(decoded_newswire)
</code></pre>
<p>上面代码运行后结果如下：</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 540px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 26.32%;\"></div>
<div class=\"image-view\" data-width=\"2052\" data-height=\"540\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2849961-2d81ac67e54f9379\" data-original-width=\"2052\" data-original-height=\"540\" data-original-format=\"image/png\" data-original-filesize=\"159226\"></div>
</div>
<div class=\"image-caption\">这里写图片描述</div>
</div>
<p>如同上一节，我们必须要把训练数据转换成数据向量才能提供给网络进行训练，因此我们像上一节一样，对每条新闻创建一个长度为一万的向量，先把元素都初始为0，然后如果某个对应频率的词在文本中出现，那么我们就在向量中相应下标设置为1，代码如下：</p>
<pre><code>import numpy as np
def vectorize_sequences(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.
    return results

x_train = vectorize_sequences(train_data)
x_test = vectorize_sequences(test_data)

print(x_train[0])
</code></pre>
<p>上面代码运行后，我们就把训练数据变成含有1或0的向量了：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 602px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 43.309999999999995%;\"></div>
<div class=\"image-view\" data-width=\"1390\" data-height=\"602\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2849961-7259e1a1825c43f2\" data-original-width=\"1390\" data-original-height=\"602\" data-original-format=\"image/png\" data-original-filesize=\"99543\"></div>
</div>
<div class=\"image-caption\">这里写图片描述</div>
</div>
<p>其实我们可以直接调用keras框架提供的接口一次性方便简单的完成：</p>
<pre><code>from keras.utils.np_utils import to_categorical

one_hot_train_labels = to_categorical(train_label)
one_hot_test_labels = to_categorical(test_labels)
</code></pre>
<p>接下来我们可以着手构建分析网络，网络的结构与上节很像，因为要解决的问题性质差不多，都是对文本进行分析。然而有一个重大不同在于，上一节我们只让网络将文本划分成两种类别，而这次我们需要将文本划分为46个类别！上一节我们构造网络时，中间层网络我们设置了16个神经元，由于现在我们需要在最外层输出46个结果，因此中间层如果只设置16个神经元那就不够用，由于输出的信息太多，如果中间层神经元数量不足，那么他就会成为信息过滤的瓶颈，因此这次我们搭建网络时，中间层网络节点扩大为6个，代码如下：</p>
<pre><code>from keras import models
from keras import layers

model = models.Sequential()
model.add(layers.Dense(64, activation=\'relu\', input_shape=(10000,)))
model.add(layers.Dense(64, activation=\'relu\'))
#当结果是输出多个分类的概率时，用softmax激活函数,它将为46个分类提供不同的可能性概率值
model.add(layers.Dense(46, activation=\'softmax\'))

#对于输出多个分类结果，最好的损失函数是categorical_crossentropy
model.compile(optimizer=\'rmsprop\', loss=\'categorical_crossentropy\', metrics=[\'accuracy\'])
</code></pre>
<p>像上一节一样，在网络训练时我们要设置校验数据集，因为网络并不是训练得次数越多越好，有了校验数据集，我们就知道网络在训练几次的情况下能够达到最优状态，准备校验数据集的代码如下：</p>
<pre><code>x_val = x_train[:1000]
partial_x_train = x_train[1000:]

y_val = one_hot_train_labels[:1000]
partial_y_train = one_hot_train_labels[1000:]
</code></pre>
<p>有了数据，就相当于有米入锅，我们可以把数据输入网络进行训练：</p>
<pre><code>history = model.fit(partial_x_train, partial_y_train, epochs=20, batch_size=512, 
                   validation_data=(x_val, y_val))
</code></pre>
<p>代码进行了20个周期的循环训练，由于数据量比上一节小，因此速度快很多，与上一节一样，网络的训练并不是越多越好，它会有一个拐点，训练次数超出后，效果会越来越差，我们把训练数据图形化，以便观察拐点从哪里开始：</p>
<pre><code>import matplotlib.pyplot as plt
loss = history.history[\'loss\']
val_loss = history.history[\'val_loss\']

epochs = range(1, len(loss) + 1)

plt.plot(epochs, loss, \'bo\', label=\'Training loss\')
plt.plot(epochs, val_loss, \'b\', label=\'Validation loss\')
plt.xlabel(\'Epochs\')
plt.ylabel(\'Loss\')
plt.legend()

plt.show()
</code></pre>
<p>上面代码运行后结果如下：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 664px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 66.4%;\"></div>
<div class=\"image-view\" data-width=\"1000\" data-height=\"664\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2849961-620fd59b0b4babe8\" data-original-width=\"1000\" data-original-height=\"664\" data-original-format=\"image/png\" data-original-filesize=\"82368\"></div>
</div>
<div class=\"image-caption\">这里写图片描述</div>
</div>
<p>通过上图观察我们看到，以蓝点表示的是网络对训练数据的判断准确率，该准确率一直在不断下降，但是蓝线表示的是网络对校验数据判断的准确率，仔细观察发现，它一开始是迅速下降的，过了某个点，达到最低点后就开始上升，这个点大概是在epochs=9那里，所以我们把前面对网络训练的循环次数减少到9：</p>
<pre><code>from keras import models
from keras import layers

model = models.Sequential()
model.add(layers.Dense(64, activation=\'relu\', input_shape=(10000,)))
model.add(layers.Dense(64, activation=\'relu\'))
#当结果是输出多个分类的概率时，用softmax激活函数,它将为46个分类提供不同的可能性概率值
model.add(layers.Dense(46, activation=\'softmax\'))

#对于输出多个分类结果，最好的损失函数是categorical_crossentropy
model.compile(optimizer=\'rmsprop\', loss=\'categorical_crossentropy\', metrics=[\'accuracy\'])

history = model.fit(partial_x_train, partial_y_train, epochs=9, batch_size=512, 
                   validation_data=(x_val, y_val))
</code></pre>
<p>完成训练后，我们把结果输出看看：</p>
<pre><code>results = model.evaluate(x_test, one_hot_test_labels)
print(results)
</code></pre>
<p>上面两句代码运行结果为：</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 246px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 19.07%;\"></div>
<div class=\"image-view\" data-width=\"1290\" data-height=\"246\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2849961-8e6759412bda6d88\" data-original-width=\"1290\" data-original-height=\"246\" data-original-format=\"image/png\" data-original-filesize=\"40889\"></div>
</div>
<div class=\"image-caption\">这里写图片描述</div>
</div>
<br>
<p>右边0.78表示，我们网络对新闻进行话题分类的准确率达到78%，差一点到80%。我们从测试数据集中拿出一条数据，让网络进行分类，得到结果再与其对应的正确结果比较看看是否一致：</p>

<pre><code>predictions = model.predict(x_test)
print(predictions[0])
print(np.sum(predictions[0]))
print(np.argmax(predictions[0]))
print(one_hot_test_labels[0])
</code></pre>
<p>我们让网络对每一条测试数据一一进行判断，并把它对第一条数据的判断结果显示出来，最后我们打印出第一条测试数据对应的分类，最后看看网络给出去的结果与正确结果是否一致，上面代码运行后结果如下：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 423px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 60.5%;\"></div>
<div class=\"image-view\" data-width=\"1666\" data-height=\"1008\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2849961-fafc9c7178e4f1b5\" data-original-width=\"1666\" data-original-height=\"1008\" data-original-format=\"image/png\" data-original-filesize=\"194914\"></div>
</div>
<div class=\"image-caption\">这里写图片描述</div>
</div>
<p>从上面运行结果看到，网络对第一条数据给出了属于46个分类的概率，其中下标为3的概率值最大，也就是第一条数据属于分类4的概率最大，最后打印出来的测试数据对应的正确结果来看，它也是下标为3的元素值为1，也就是说数据对应的正确分类是4，由此我们网络得到的结果是正确的。</p>
<p>前面提到过，由于网络最终输出结果包含46个元素，因此中间节点的神经元数目不能小于46，因为小于46，那么有关46个元素的信息就会遭到挤压，于是在层层运算后会导致信息丢失，最后致使最终结果的准确率下降，我们试试看是不是这样：</p>
<pre><code>from keras import models
from keras import layers

model = models.Sequential()
model.add(layers.Dense(64, activation=\'relu\', input_shape=(10000,)))
model.add(layers.Dense(4, activation=\'relu\'))
#当结果是输出多个分类的概率时，用softmax激活函数,它将为46个分类提供不同的可能性概率值
model.add(layers.Dense(46, activation=\'softmax\'))

#对于输出多个分类结果，最好的损失函数是categorical_crossentropy
model.compile(optimizer=\'rmsprop\', loss=\'categorical_crossentropy\', metrics=[\'accuracy\'])

history = model.fit(partial_x_train, partial_y_train, epochs=9, batch_size=512, 
                   validation_data=(x_val, y_val))
results = model.evaluate(x_test, one_hot_test_labels)
print(results)
</code></pre>
<p>上面代码运行后，输出的results结果如下：<br>
[1.4625472680649796, 0.6705253784505788]</p>
<p>从上面结果看到，我们代码几乎没变，致使把第二层中间层神经元数量改成4，最终结果的准确率就下降10个点，所以中间层神经元的减少导致信息压缩后，最后计算的准确度缺失。反过来你也可以试试用128个神经元的中间层看看准确率有没有提升。</p>
<p>到这里不知道你发现没有，神经网络在实际项目中的运用有点类似于乐高积木，你根据实际需要，通过选定参数，用几行代码配置好基本的网络结构，把训练数据改造成合适的数字向量，然后就可以输入到网络中进行训练，训练过程中记得用校验数据监测最优训练次数，防止过度拟合。</p>
<p>在网络的设计过程中，其背后的数学原理我们几乎无需了解，只需要凭借经验，根据项目的性质，设定网络的各项参数，最关键的其实在根据项目数据性质对网络进行调优，例如网络设置几层好，每层几个神经元，用什么样的激活函数和损失函数等等，这些操作与技术无关，取决以个人经验，属于“艺术”的范畴。</p>
<p><a href=\"http://study.163.com/provider-search?keyword=Coding%E8%BF%AA%E6%96%AF%E5%B0%BC\" target=\"_blank\" rel=\"nofollow\">更详细的讲解和代码调试演示过程，请点击链接</a></p>
<p>更多技术信息，包括操作系统，编译器，面试算法，机器学习，人工智能，请关照我的公众号：</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 258px; max-height: 258px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 100.0%;\"></div>
<div class=\"image-view\" data-width=\"258\" data-height=\"258\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2849961-d584a529f20a04da\" data-original-width=\"258\" data-original-height=\"258\" data-original-format=\"\" data-original-filesize=\"27260\"></div>
</div>
<div class=\"image-caption\">这里写图片描述</div>
</div>

          </div>','1531183657'),
('325554','{1123893}{131}{975703}{975915}{975919}{975714}{260}{255}{190}{26413}{39}{1123894}{191187}{1123895}{1123896}{17}{975741}{975835}{975788}{1676}{975727}{1354}{36}{761}{554}{2592}{234}{1642}{975744}{3307}{975878}{2435}{2341}{975779}{1352}{975910}{975738}{327}{2323}{4206}{2148}{3672}{8297}{1123897}{3180}{58499}{975904}{196}{30152}{453}','机器学习论文笔记—如何利用高效的搜索算法来搜索网络的拓扑结构 简 介 分层表示高效的架构搜索(HIERARCHICAL REPRESENTATIONS FOR EFFICIENT ARCHITECTURE SEARCH)这篇文章讲的是如何利用高效的搜索算法来搜索网络的拓扑结构。用一个简单但功能强大的演化算法。 这个方法可以发现具有卓越性能的新架构。它这篇文章很大程度上借鉴了GECNN的一些东西，或者说，我之前写了GECNN的论文笔记','机器学习论文笔记—如何利用高效的搜索算法来搜索网络的拓扑结构','<div class=\"show-content-free\">
            <h2>简   介</h2>
<p>分层表示高效的架构搜索(HIERARCHICAL REPRESENTATIONS FOR EFFICIENT ARCHITECTURE SEARCH)这篇文章讲的是如何利用高效的搜索算法来搜索网络的拓扑结构。用一个简单但功能强大的演化算法。 这个方法可以发现具有卓越性能的新架构。它这篇文章很大程度上借鉴了GECNN的一些东西，或者说，我之前写了GECNN的论文笔记，里面也是讲演化算法的：<a href=\"https://zhuanlan.zhihu.com/p/36758195\" target=\"_blank\" rel=\"nofollow\">https://zhuanlan.zhihu.com/p/36758195</a> 。</p>
<p>github：<a href=\"https://github.com/markdtw/awesome-architecture-search\" target=\"_blank\" rel=\"nofollow\">https://github.com/markdtw/awesome-architecture-search</a>（还没开源代码）</p>
<p>arxiv：<a href=\"https://arxiv.org/abs/1711.00436\" target=\"_blank\" rel=\"nofollow\">https://arxiv.org/abs/1711.00436</a></p>
<p><strong>章节目录</strong></p>
<ul>
<li><p>优点摘要</p></li>
<li><p>抛砖</p></li>
<li><p>引玉</p></li>
<li><p>定义部分</p></li>
<li><p>元操作</p></li>
<li><p>进化</p></li>
<li><p>初始化</p></li>
<li><p>算法</p></li>
<li><p>训练结果</p></li>
</ul>
<h1>优点摘要</h1>
<ol>
<li><p>通过分层图学习的方法，大大减少了冗余的搜索空间</p></li>
<li><p>引入分层表示来描述神经网络体系结构。</p></li>
<li><p>通过堆叠简单的基元（比如conv，maxpooling），实现了复杂的结构，使用简单的随机搜索，也可以获得用于图像分类的竞争架构，这体现了 搜索空间构建的重要性。</p></li>
<li><p>可以通过跳跃链接实现resnet，densenet等深层次的网络（为resnet引用都快破万了，你现在用不上残差块的思想你好意思发论文吗……）。</p></li>
</ol>
<h1>抛砖</h1>
<p>首先一个问题，我们为什么要想办法设计出一个自动生成的网络架构，因为我最近发现一个问题啊，你一个神经网络特别那种实验室做出来的，效果估计也就是退cifar-10啊，mnist这些效果好一点点，但是一碰到真实环境，效果就菜的一批（可能会），可能这种机器生成的神经网络效果不一定有专业人员做的好，但是他在针对其他真实条件下的数据集，他的效果可能会反超那些定下来的网络结构，因为这个东西他上自适应调整的。</p>
<h1>引玉</h1>
<p>这篇文章，他提出来用邻接矩阵去表示一个有向无环图，然后用点表示feature map,用边表示一种操作（其实就只有conv, max_pooling, average-pooling, identity, 开心不，意外不）。</p>
<h1>定义部分</h1>
<p>我们先定义一个邻接矩阵，然后leval-1的矩阵的值的意思就是集合option{}。</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 154px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 14.26%;\"></div>
<div class=\"image-view\" data-width=\"1080\" data-height=\"154\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-377ff19a1d923ffc\" data-original-width=\"1080\" data-original-height=\"154\" data-original-format=\"image/jpeg\" data-original-filesize=\"21810\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>The architecture is obtained by assembling operations o according to the adjacency matrix G:<br>
结构 = assemble(G, o)</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 148px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 19.68%;\"></div>
<div class=\"image-view\" data-width=\"752\" data-height=\"148\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-8fe68b609dbde274\" data-original-width=\"752\" data-original-height=\"148\" data-original-format=\"image/jpeg\" data-original-filesize=\"11615\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 130px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 12.879999999999999%;\"></div>
<div class=\"image-view\" data-width=\"1009\" data-height=\"130\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-6e6859b33f55e58d\" data-original-width=\"1009\" data-original-height=\"130\" data-original-format=\"image/jpeg\" data-original-filesize=\"25791\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>mrge是合并</p>
<p>o1(1)在这里是1 *1的卷积，o2(1)是3 <em>3的卷积，o3(1)是3</em>3的最大池化（不是2 *2因为他要保证feature map一样大)，他们统称为元操作（相当于tf.nn里面一个函数啊）。</p>
<p>我大概画一下leval-2<br>
G1(2)=[0 3 2;<br>
0 0 1；<br>
0 0 1]<br>
</p>
<p></p>
然后我们把这个生成的子图当成一个新的子图 ，是不是很6<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 85px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 8.43%;\"></div>
<div class=\"image-view\" data-width=\"1008\" data-height=\"85\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-55c7146d977d9305\" data-original-width=\"1008\" data-original-height=\"85\" data-original-format=\"image/jpeg\" data-original-filesize=\"7204\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p></p>
<p></p>
所以我们就这样定义了一个level-2的子图了，接下来我们如法炮制的搞出了3个leval-2的子图，just like this：<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 240px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 24.099999999999998%;\"></div>
<div class=\"image-view\" data-width=\"996\" data-height=\"240\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-82cd50aee21a25a1\" data-original-width=\"996\" data-original-height=\"240\" data-original-format=\"image/jpeg\" data-original-filesize=\"21986\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>你发现了吗，每一层的piont的个数是一样的，是固定的，<strong>这个就是他这个算法不太好的地方一</strong>。</p>
<p>然后我们就得到了一个level-3的网络结构（简直就是insecption加resnet的样子）到此为止，我们就算是搞出来一种可以表示这个网络的方法了。</p>
<h1>元操作</h1>
<p>作者在实验中发现啊，3*3的conv只要搞多几次，就可有搞出很大的感受野什么的，所以他就搞的元操作其实很少（6）：</p>
<pre><code> 1 × 1 convolution of C channels(调整特征图的维度)
 3 × 3 depthwise convolution（不解释了吧）
 3 × 3 separable convolution of C channels 
 3 × 3 max-pooling（最大池化）
 3 × 3 average-pooling （平均池化）
 identity
</code></pre>
<p>对于每一个feature map，他都用了RELU激活函数和批次正则化（-.-)。channels固定为常数C（可以通过1*1卷积）。<br>
如果option(i,j)==0的话就说明i,j之间没有边。<br>
concat就是之间把feature map加到一起。</p>
<h1>进化</h1>
<p>首先，我们要实现3个基本操作：增加add，修改alter，删除remove。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 115px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 14.92%;\"></div>
<div class=\"image-view\" data-width=\"771\" data-height=\"115\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-43f5b3e4104994fa\" data-original-width=\"771\" data-original-height=\"115\" data-original-format=\"image/jpeg\" data-original-filesize=\"14621\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<h1>初始化</h1>
<ol>
<li><p>创建很小的元操作的DNA（类似GECNN上面有链接），为每个DNA创建一个映射，相当于是下一层的元操作</p></li>
<li><p>通过大量随机突变产生变异样本（类似蒙特卡洛方法随机）</p></li>
</ol>
<h1>算法</h1>
<p>异步锦标赛进化（相当于生物学里面的达尔文进化论）</p>
<p>part 1：</p>
<p><strong></strong></p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 207px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 26.71%;\"></div>
<div class=\"image-view\" data-width=\"775\" data-height=\"207\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-7474ef115d0f420f\" data-original-width=\"775\" data-original-height=\"207\" data-original-format=\"image/jpeg\" data-original-filesize=\"25456\"></div>
</div>
<strong><div class=\"image-caption\">image</div></strong>
</div>
<p></p>
<p>part 2:</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 214px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 28.01%;\"></div>
<div class=\"image-view\" data-width=\"764\" data-height=\"214\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-bbcaab475a937751\" data-original-width=\"764\" data-original-height=\"214\" data-original-format=\"image/jpeg\" data-original-filesize=\"26965\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>什么是锦标赛算法？</p>
<p>假设种群规模为n，该法的步骤为：</p>
<ol>
<li><p>随机产生n个个体作为第一代（其实这步准确的说不是属于选择操作的，但每个算子并没有绝对的界限，这个是在选择操作之前的必做之事）  。</p></li>
<li><p>从这n个个体中随机（注意是随机）选择k（k&lt; n）个个体，k的取值小，效率就高（节省运行时间），但不宜太小，一般取为n/2（取整）。</p></li>
<li><p>从这k个个体中选择最大的一个个体（涉及到排序的方法），作为下一代n个个体中的一个个体 。</p></li>
<li><p>重复2-4步，至得到新的n个个体。</p></li>
<li><p>进行这新的n个个体之间的交叉操作。</p></li>
</ol>
<h1>训练结果</h1>
<ol>
<li><p>对于不同大小的数据集，我们要输入不同大小的level层数和节点k数，总共最多会产生k^l个feature map，所以初始化的时候应该是有非常多的0元素才对（猜测）。</p></li>
<li><p>随机梯度下降，学习率的调整细节等……不想说了</p></li>
</ol>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 302px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 37.24%;\"></div>
<div class=\"image-view\" data-width=\"811\" data-height=\"302\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-22f067f57b4de260\" data-original-width=\"811\" data-original-height=\"302\" data-original-format=\"image/jpeg\" data-original-filesize=\"35957\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>我个人认为的改进点：可以看得出这个架构其实不是线性的，网络是可以比较复杂的，肯定是有resnet在里面的，应该是越深层越适应大的训练集，不过从我的实际工作来看，最好是再加一个网络结构预判器，因为针对点多且深的网络结构来说，没有做够的resnet到了深层肯定会梯度爆炸或者消失，那种明显不合格的网络是可以被检查出来的，可以极大的减少冗余计算，因为越好的网络训练应该是越快的，反而是大部分都是垃圾结构牺牲了大部分的时间（二八定律），具体是实现方法我自己留着发论文了。</p>
<p>具体的训练细节太复杂了，DEEPMIND也没开源代码，我就不好瞎说了。</p>
<p>在CIFAR-10上的训练效果</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 416px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 56.14%;\"></div>
<div class=\"image-view\" data-width=\"741\" data-height=\"416\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-392cd611a94d373a\" data-original-width=\"741\" data-original-height=\"416\" data-original-format=\"image/jpeg\" data-original-filesize=\"57803\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>我想吐槽的是：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 134px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 18.16%;\"></div>
<div class=\"image-view\" data-width=\"738\" data-height=\"134\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-52153d12d3cc1915\" data-original-width=\"738\" data-original-height=\"134\" data-original-format=\"image/jpeg\" data-original-filesize=\"35262\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>真的好有钱啊，200个p100，我算了一下在阿里云上面竞价的话要10<em>200</em>24*1.5=7200RMB，</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 643px; max-height: 332px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 51.629999999999995%;\"></div>
<div class=\"image-view\" data-width=\"643\" data-height=\"332\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-e2e332d8f59e4c55\" data-original-width=\"643\" data-original-height=\"332\" data-original-format=\"image/jpeg\" data-original-filesize=\"47209\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>说实话这个效果来说还是非常值这个价的，在CIFAR-10上这个被p过几百万次的数据集上还能和那些老p客难分伯仲，要是换的真实数据集的话效果应该会跟好一些吧。</p>
<h1>总结</h1>
<p>目前网络结构生成的两种方法强化学习和演化学习都在发paper，从难度来说我其实更喜欢演化学习，因为治疗都是现成的，但是长远的看我觉得强化学习会赢，这个元学习也一直是我觉得很有意思的一个方向，可能是迈向强人工智能的一个阶梯吧，百尺竿头更进一步。</p>
<hr>
<blockquote>
<p>个人微信：TonyJeemy520      加个人微信拉你进机器学习、深度学习交流群，请备注 : 来自简书<br>
QQ交流群：651616387   请备注 : 来自简书<br>
微信公众号：机器学习算法工程师   ----二维码见下图</p>
</blockquote>
<hr>
<p><strong>扫码关注微信号：机器学习算法工程师，更多干货分享， 或加个人微信，拉你进机器学习、深度学习交流群</strong><br>
</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 430px; max-height: 430px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 100.0%;\"></div>
<div class=\"image-view\" data-width=\"430\" data-height=\"430\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-0bdf004ac80c0197\" data-original-width=\"430\" data-original-height=\"430\" data-original-format=\"image/jpeg\" data-original-filesize=\"32906\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p></p>

          </div>','1531183658'),
('325555','{1123899}{4411}{1123900}{1100}{1123901}{63}{975757}{975755}{456029}{408}{975776}{884}{62}{1076}{1123902}{1005811}{6550}{1767}{1123903}{976042}{1354}{975714}{975787}{975761}{1187}{564}{3826}{1121}{7158}{39265}{1722}{1123904}{975741}{975835}{1129}{36}{1096}{975785}{1431}{975721}{975779}{530}{1277}{975788}{542}{975717}{251}{569}{975724}{975820}','Convolve+Nois：每个图片，先用卷积核k进行卷积操作，再加上噪声z（文中用的pθ，怕与上面的测度模型混淆，这里改写成z）。 Block-Patch：在图片中随机选择一个kk的块，将其像素值设置为0.','AmbientGAN:Generative models from lossy measuremen','<div class=\"show-content-free\">
            <p>环境GAN：从有损测度中生成模型</p>
<p>摘要：<br>
生成模型提供了一种对于复杂分布中结构进行建模的方式，并且已经被证明可用于很多实际感兴趣的任务中。但是，现在训练生成模型的技术需要访问完全可观测（fully-observed）的样本。在很多场景中，获取完全可观测的样本是昂贵的甚至不可能的，但是获取部分的有噪声的观测样本是比较实惠的。我们考虑在只给予感兴趣的分布的样本的有损失观测情况下，学习一个隐式的生成模型的任务。我们证明了即使在测度模型的某个类的每个样本信息丢失的情况下，真正隐藏的分布也可以被恢复。基于此，我们提出了一种我们称之为环境GAN（AmbientGAN）的新的训练生成对抗网络的方法。在三个基准数据集和对于很多测度模型而言，我们证实了方法实质性的定性和定量的改进。用我们方法训练的生成模型可以获得比基准高2-4倍的初始分数（inception score）。<br>
全文：<a href=\"https://openreview.net/forum?id=Hy7fDog0b\" target=\"_blank\" rel=\"nofollow\">https://openreview.net/forum?id=Hy7fDog0b</a></p>
<p>本文提出的模型很简单，就是生成器需要去拟合的数据分布并不可直接得到，但是知道经过一些处理后得到的原数据的有损数据，并且知道这个处理的过程，于是采用有损数据，并将处理过程整合到GAN的架构中，获得的就是这篇文章提出的AmbientGAN的架构，其具体架构如图所示：</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 494px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 40.23%;\"></div>
<div class=\"image-view\" data-width=\"1228\" data-height=\"494\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6080435-c461e8010ea188fd.png\" data-original-width=\"1228\" data-original-height=\"494\" data-original-format=\"image/png\" data-original-filesize=\"45826\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>其中Y是原始数据有损处理以后的结果数据，而黄色的fθ函数就是有损处理的映射函数，因而这个AmbientGAN与GAN之间的区别就在于分辨器D需要分辨输入的有损数据来自于真实的有损数据Yr，还是生成的数据Xg经过f函数映射后得到的Yg。因此，对应的AmbientGAN的loss修改为如下形式：</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 73px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 7.6499999999999995%;\"></div>
<div class=\"image-view\" data-width=\"954\" data-height=\"73\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6080435-9bdb34e72b4bda6c.png\" data-original-width=\"954\" data-original-height=\"73\" data-original-format=\"image/png\" data-original-filesize=\"8482\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>其中，q(x)表示的是质量函数（按照理解，就是loss函数，在原始的GAN中，这个函数就是q(x)=log(x)）。而fθ函数，就是模拟的测度函数，就是图中黄色部分。在论文中，这个函数要求对所有的输入，该函数可导。<br>
就测度模型 (measurement models)而言，文中提出了几种不一样的模型，来测试AmbientGAN模型：<br>
Block-Pixels：图片的每个像素都有独立的具有p的概率被设置为0.<br>
Convolve+Nois：每个图片，先用卷积核k进行卷积操作，再加上噪声z（文中用的pθ，怕与上面的测度模型混淆，这里改写成z）。<br>
Block-Patch：在图片中随机选择一个k<em>k的块，将其像素值设置为0.<br>
Extract-patch：在图片中随机选择一个k</em>k的块，截取出来作为图片（类似于截图）<br>
Pad-Rotate-Project：用0填充图片的四周，然后以图片中心为轴，旋转θ角度，然后每个通道沿着水平方向，叠加竖直方向的像素值，最后得到一个向量（维度为N<em>1</em>3，其中N是每个通道横向的像素个数的最大值）。<br>
Pad-Rotate-Project-θ：操作与Pad-Rotate-Project一致，但是θ是已知的。<br>
Gaussian-Projection：高斯投影，θ~N(0,In)，fθ(x)=(θ,&lt;θ,x&gt;)（这个处理不是很懂，大致的操作就是使用高斯向量做投影？）<br>
文章的理论基础认为AmbientGAN的分辨器D最优化的形式如下：<br>
</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 230px; max-height: 81px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 35.22%;\"></div>
<div class=\"image-view\" data-width=\"230\" data-height=\"81\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6080435-df8ae101d1ecbeea.png\" data-original-width=\"230\" data-original-height=\"81\" data-original-format=\"image/png\" data-original-filesize=\"3266\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p></p>
<p>其中r上标表示真实数据，g上标表示生成数据，y下标表示经过测度后的数据。如果测度函数对于输入px唯一确定一个py，那么D最优的情况虽然是pry=pgy，但是此时唯一对应着有，pgx=prx。因而本文接下来的一部分理论证明的过程，都是证明上述提及的测度的模型唯一确定了一个测度后的样本。<br>
由于AmbientGAN架构本质上就是在生成样本送D之前，加入了一个测度模型，因而其很容易整合到具体的GAN模型中去，文中在实验过程中，尝试将AmbientGAN整合到了DCGAN，WGANGP，ACWGANGP等架构中来测试其性能。<br>
对于本文提出的架构，测试的Baseline的设计分为几种方式，第一种是完全忽略在图片上进行的测度处理，直接用测度以后的图片，训练GAN模型，被文中称为忽略型baseline。另一种更好一些的baseline就是如果f测度函数是可逆的，那么从测度样本y求出其完全观察样本x即可，但这样并不符合文中假设（假设是只能接触到测度后的样本），基于此，文中提出可以获得近似的逆样本，被称作逆测度的样本（unmeasure），然后利用这样的近似样本来学习生成模型。这样的近似，对于上述不同的测度模型不同：<br>
Block-Pixels：模糊图片，对于0像素，使用其周围的像素来填充，文中采用的是total variation inpainting的方法。<br>
Convolve+Noise：Wiener deconvolution的方法处理图片作为近似。<br>
Block-Patch：Navier Stokes based inpainting的方法来填充0像素。<br>
其他的测度模型，没有其他的方法进行近似估计逆值，因而在试验中，作者只提供了AmbientGAN的结果。<br>
Block-Pixels实验结果：</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 393px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 31.819999999999997%;\"></div>
<div class=\"image-view\" data-width=\"1235\" data-height=\"393\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6080435-2dda6b39b546299b.png\" data-original-width=\"1235\" data-original-height=\"393\" data-original-format=\"image/png\" data-original-filesize=\"614991\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 486px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 39.190000000000005%;\"></div>
<div class=\"image-view\" data-width=\"1240\" data-height=\"486\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6080435-c3e32f7be038bf97.png\" data-original-width=\"1240\" data-original-height=\"486\" data-original-format=\"image/png\" data-original-filesize=\"926002\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>Convolve+Noise实验结果：</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 576px; max-height: 522px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 90.63%;\"></div>
<div class=\"image-view\" data-width=\"576\" data-height=\"522\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6080435-6f4487eaeccb3171.png\" data-original-width=\"576\" data-original-height=\"522\" data-original-format=\"image/png\" data-original-filesize=\"585203\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>Block-patch和Keep-patch实验结果：</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 520px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 41.8%;\"></div>
<div class=\"image-view\" data-width=\"1244\" data-height=\"520\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6080435-55b10b310637c4f0.png\" data-original-width=\"1244\" data-original-height=\"520\" data-original-format=\"image/png\" data-original-filesize=\"1255419\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 325px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 40.52%;\"></div>
<div class=\"image-view\" data-width=\"802\" data-height=\"325\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6080435-8abab462468574f6.png\" data-original-width=\"802\" data-original-height=\"325\" data-original-format=\"image/png\" data-original-filesize=\"347261\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
<p>Pad-Rotate-Project和Pad-Rotate-Project-θ实验结果</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 629px; max-height: 505px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 80.28999999999999%;\"></div>
<div class=\"image-view\" data-width=\"629\" data-height=\"505\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6080435-2ad496fa3e6cf457.png\" data-original-width=\"629\" data-original-height=\"505\" data-original-format=\"image/png\" data-original-filesize=\"154730\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 415px; max-height: 376px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 90.60000000000001%;\"></div>
<div class=\"image-view\" data-width=\"415\" data-height=\"376\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6080435-8f1d654ab9408f36.png\" data-original-width=\"415\" data-original-height=\"376\" data-original-format=\"image/png\" data-original-filesize=\"276961\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>可以从上述的结果中看到，除了一个降维测度模型中，AmbientGAN没有产生较好的实验结果，其他的实验结果都是比baseline在观感上要好的。<br>
接下来文章采用Inception Score来量化分析了AmbientGAN产生的图片质量，这部分重点关注的是在Block-Pixels和Convolve+Noise测度模型下的表现。</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 386px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 43.47%;\"></div>
<div class=\"image-view\" data-width=\"888\" data-height=\"386\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6080435-cf5662868c7938aa.png\" data-original-width=\"888\" data-original-height=\"386\" data-original-format=\"image/png\" data-original-filesize=\"50622\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 423px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 46.129999999999995%;\"></div>
<div class=\"image-view\" data-width=\"917\" data-height=\"423\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6080435-497c8c11b10da5a8.png\" data-original-width=\"917\" data-original-height=\"423\" data-original-format=\"image/png\" data-original-filesize=\"135390\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>本文的附录部分提供了更多的实验结果的图片，这里就不一一复制粘贴了，附录中值得关注的一个实验是作者额外的实验。文中提出的AmbientGAN的前提条件是，获得了一些有损的或者叫不完整的样本，然后用来生成完整的样本，但是由完整样本得到不完整样本的处理是已知，这样的情况，对于f的要求太过具体，同时在实际中也很难遇到，但是可能存在知道大致的处理过程，但是有些具体参数不知道。作者用Block-Pixels做了个测试，在概率p不知道的情况，在f中穷举p的值，来获得不同的生成模型，最终Inception Score与估计的概率p’的图像如下，虽然这样的图像猜测中了实际的p值，但在实际问题中，这样的穷举能得到多大的效果，确实未知。</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 450px; max-height: 328px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 72.89%;\"></div>
<div class=\"image-view\" data-width=\"450\" data-height=\"328\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6080435-d332618a95ddb790.png\" data-original-width=\"450\" data-original-height=\"328\" data-original-format=\"image/png\" data-original-filesize=\"15046\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
<p>这篇文章的网络架构很简单，有损的样本，得到有损样本的处理过程，在这种情况下，将得到有损样本的处理过程整合到GAN中，以此得到AmbientGAN，来在只能得到有损样本和有损样本的处理过程的情况下，学习从不完整数据中，生成完整数据的一个生成模型。虽然整体的约束条件比较苛刻，但是有论文的审核者提出，这或许是为去噪模型提供了一种思路，这样的模型可能可以用来去噪或者还原图片。</p>


          </div>','1531183659'),
('325556','{341}{975757}{1123906}{975878}{975729}{975721}{731}{744}{975714}{975761}{247}{1352}{62044}{62}{184}{21792}{1847}{1129}{2290}{2661}{975768}{3340}{9063}{41076}{186396}{186684}{140410}{975776}{975904}{976042}{454}{157262}{165}{1018}{1273}{1627}{975951}{255}{975720}{36}{10559}{528}{17}{4081}{761}{975903}{975779}{775}{7715}{975879}','buy是电商，我类比着做了用户每个类目（裤子、衣服、鞋子..Buy团队在实践中的一点总结，不代表公司的任何言论，仅仅是我个人的观点。最后感谢项目推进过程中所有合作方和项目组同学的付出和努力，感谢各个团队各位老大们的支持！溜了溜了。','关于\'Deep Neural Networks for YouTube Recommendation','<div class=\"show-content-free\">
            <p>欢迎大家关注我的个人<a href=\"http://shataowei.com\" target=\"_blank\" rel=\"nofollow\">bolg</a>，更多代码内容欢迎follow我的个人<a href=\"https://github.com/sladesha\" target=\"_blank\" rel=\"nofollow\">Github</a>，如果有任何算法、代码疑问都欢迎通过<a href=\"mailto:stw386@sina.com\" target=\"_blank\" rel=\"nofollow\">stw386@sina.com</a>联系我。</p>
<p>论文 Deep Neural Networks for YouTube Recommendations 来自google的YouTube团队，发表在16年9月的RecSys会议。我想应该很多人都读过，之前参与了公司的推荐系统优化的项目，本来想从各大搜索引擎中寻找到现成的分析，但是出人意料的一无所获。Github上的代码实现也出奇的少以及不清晰，所以就借着这个机会和大家分享一下自己做的过程中的一些理论心得、工程坑、代码实现等等。</p>
<p><em>本文基于大家对Deep Neural Networks for YouTube Recommendations已经完成通读的基础上，不会做细致的论文解析，只会涉及到自己实现过程中的一些总结，如果没有论文了解，会非常不易理解。</em></p>
<h2>一、系统概览</h2>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 554px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 79.16%;\"></div>
<div class=\"image-view\" data-width=\"1660\" data-height=\"1314\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1129359-67a74922f9908400.png\" data-original-width=\"1660\" data-original-height=\"1314\" data-original-format=\"image/png\" data-original-filesize=\"233844\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
<p>上面这张图可以说是比较详细的涵盖了基础框架部分，整体的模型的优点我就不详述了，包括规模容纳的程度大啊、鲁棒性好啊、实时性优秀啊、延展性好啊等等，网上很多水字数的文章很多，我们主要总结几个愿论文上的亮点和实际去做的时候需要注意的地方：</p>

<ul>
<li>DNN网络可以怎么改</li>
<li>负采样的“避坑”</li>
<li>example age有没有必要构造</li>
<li>user feature的选择方向</li>
<li>attention 机制的引入</li>
<li>video vectors的深坑</li>
<li>实时化的选择</li>
</ul>
<p>整体上来说，G厂的这套算法基于的是两个部分：matching+ranking，这个的也给我们带来了更大的工作量，在做的时候，分成两个部分，我们在实际处理的时候，通过recall rate来判断matching部分的好坏，通过<a href=\"https://www.cnblogs.com/eyeszjwang/articles/2368087.html\" target=\"_blank\" rel=\"nofollow\">NDCG</a>来判断排序部分的好坏。总体如下：<br>
</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 543px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 75.42%;\"></div>
<div class=\"image-view\" data-width=\"720\" data-height=\"543\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1129359-8778b64753e4bedb.png\" data-original-width=\"720\" data-original-height=\"543\" data-original-format=\"image/png\" data-original-filesize=\"252398\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p></p>
<p>candidate generation就是我们matching的模块，目的是把百万级的商品、视频筛选出百级、千级的可排序的量级；再通过ranking模块，选出十位数的展示商品、视频作为最后的推送内容。之所以把推荐系统划分成Matching和Ranking两个阶段，主要是从性能方面考虑的。Matching阶段面临的是百万级，而Ranking阶段的算法则非常消耗资源，不可能对所有目标都算一遍，而且就算算了，其中大部分在Ranking阶段排名也很低，也是浪费计算资源。</p>
<h2>二、Matching &amp; Ranking Problems</h2>
<p>首先，我们都知道，G厂给出的这个解决方案用的就是基于DNN的超大规模多分类思想，即在时刻t，为用户U（上下文信息C）在视频库V中精准的预测出视频i的类别（每个具体的视频视为一个类别，i即为一个类别），用数学公式表达如下：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 78px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 10.83%;\"></div>
<div class=\"image-view\" data-width=\"720\" data-height=\"78\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1129359-baa1dabf1aac8a0d.png\" data-original-width=\"720\" data-original-height=\"78\" data-original-format=\"image/png\" data-original-filesize=\"19647\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>很显然上式为一个softmax多分类器的形式。向量u是&lt;user, context&gt;信息的高纬“embedding”，而向量v则是视频 j 的embedding向量，通过u与v的点积大小来判断用户与对应视频的匹配程度。所以DNN的目标就是在用户信息和上下文信息为输入条件下学习视频的embedding向量v，从而得到用户的向量u。</p>
<p>说完基本思想，让我们看看实际的效果对比：</p>
<h4>DNN网络可以怎么改</h4>
<ul>
<li>
<strong>softmax及revise</strong>的考虑<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 508px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 72.67%;\"></div>
<div class=\"image-view\" data-width=\"1200\" data-height=\"872\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1129359-e9ee939096cbdbb6.png\" data-original-width=\"1200\" data-original-height=\"872\" data-original-format=\"image/png\" data-original-filesize=\"465497\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
</li>
</ul>
<p>如我图中两处红色标记，论文中虽然给出了模型的整体流程，但是却没有指明，1处的video vectors需要单独的embedding还是沿用最下方的embedded video watches里面的已经embedding好的结果，我们称之为softmax问题；2处论文没有提及一个问题，就是在固定好历史watch的长度，比如过去20次浏览的video，可能存在部分用户所有的历史浏览video数量都不足20次，在average的时候，是应该除以固定长度（比如上述例子中的20）还是选择除以用户真实的浏览video数量，我们称之为revise问题。</p>
<p>根据我们的数据实测，效果对比如下：</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 632px; max-height: 186px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 29.43%;\"></div>
<div class=\"image-view\" data-width=\"632\" data-height=\"186\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1129359-cefd070a589c1fe9.png\" data-original-width=\"632\" data-original-height=\"186\" data-original-format=\"image/png\" data-original-filesize=\"38470\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p><code>nosoft:沿用最下方的embedded video watches里面的已经embedding好的结果</code><br>
<code>revise:除以用户真实的浏览video数量</code></p>
<p>我们尝试的去探求原因发现，nosoft比softmax好的原因在于user vector是由最下方的embedded video watches里面的已经embedding好的结果进行多次FC传递得来的，如果新增一个video embedded vector 的话，和FC传递得到的u vector的点积的意义就难以解释；revise比norevise好的原因是，实际在yoho!buy的购物场景下，用户的点击历史比较我们实际选取的window size要短不少，如果所有的用户都除以固定长度的话，大量的用户history click average的vector大小接近于0。</p>
<ul>
<li><strong>神经元死亡及网络的内部构造</strong></li>
</ul>
<p>这是一个异常恶心还有没什么好方法的问题，在刚开始做的时候，我们遇到了一个常见的问题，神经元批量死亡的问题。在增加了batch normalization、clip_by_global_norm和exponential_decay learning rate 后有所缓解。</p>
<p>网络结构的变化比较常规，对比场景的激活函数，参考了论文中推荐的深度、节点数，效果对比如下：</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 678px; max-height: 316px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 46.61%;\"></div>
<div class=\"image-view\" data-width=\"678\" data-height=\"316\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1129359-94045e707dc8a59e.png\" data-original-width=\"678\" data-original-height=\"316\" data-original-format=\"image/png\" data-original-filesize=\"63637\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>虽然我们看到增加网络的深度（3--&gt;4）一定程度上会提高模型的命中率，增加leakyrelu的一层网络也可以有些许的提升，但是总的来说，对模型没有啥大的影响，所以在之后的实际模型中，我们选择了原论文中relu+relu+relu，1024+512+256的框架。</p>
<ul>
<li><strong>负采样的“避坑”</strong></li>
</ul>
<p>我们都知道，算法写起来小半天就可以搞定，但是前期的数据处理要搞个小半个月都不一定能出来。作为爱省事的我，为了快速实现算法，没有重视负采样的部分，采取了列表页点击为label=1，未点击为label=0的方式，详情如下：</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 1262px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 180.43%;\"></div>
<div class=\"image-view\" data-width=\"1308\" data-height=\"2360\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1129359-131d0c4e7db5b946.png\" data-original-width=\"1308\" data-original-height=\"2360\" data-original-format=\"image/png\" data-original-filesize=\"1566748\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>看上去没什么问题，省略了从全量样本中抽样作为负样本的复杂过程，实际上，我把代码狂改了n边效果也一直维持在1.57%，可以说是没有任何提升，在此过程期间，我还是了拿用户的尾次点击（last_record）进行训练，拿了有较多行为的用户的尾次点击（change_last_record）进行训练，效果很感人：</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 270px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 26.21%;\"></div>
<div class=\"image-view\" data-width=\"1030\" data-height=\"270\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1129359-b510056356e01a5b.png\" data-original-width=\"1030\" data-original-height=\"270\" data-original-format=\"image/png\" data-original-filesize=\"75666\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>在我孤注一掷一致，选择按照原论文中说的，每次label=0的我不拿展现给用户但是用户没有点击的商品，而是随机从全量商品中抽取用户没有点击过的商品作为label=0的商品后，奇迹发生了：</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 206px; max-height: 348px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 168.93%;\"></div>
<div class=\"image-view\" data-width=\"206\" data-height=\"348\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1129359-db33acabf5258604.png\" data-original-width=\"206\" data-original-height=\"348\" data-original-format=\"image/png\" data-original-filesize=\"27442\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>事后我仔细分析了原因：<br>
a.在当次展现的情况下，虽然用户只点击了click商品，其他商品没有点击，但是很多用户在后续浏览的时候未click的商品也在其他非列表页的地方进行click，我实际上将用户感兴趣的商品误标记为了负样本<br>
b.后来我咨询看了论文，也发现了原论文中也提及到，展现商品极有可能为热门商品，虽然该商品该用户未点击，但是我们不能降低热门商品的权重（通过label=0的方式），实际上最后的数据也证明了这一点<br>
c.“偷窥未来的行为”，如下图，原论文中指出input构造时候不能拿还未发生的点击，只能拿label=1产生时之前的所有历史点击作为input；同理，在构造label=0的时候，只能拿在label=0的时候已经上架的商品，由于训练时间的拉长，不能偷窥label=1发生时还未上架的商品作为label=0的负样本</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 560px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 31.64%;\"></div>
<div class=\"image-view\" data-width=\"1770\" data-height=\"560\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1129359-7d29f530cc06b2bf.png\" data-original-width=\"1770\" data-original-height=\"560\" data-original-format=\"image/png\" data-original-filesize=\"134381\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<ul>
<li><strong>example age有没有必要构造</strong></li>
</ul>
<p>首先，先稍微解释一下我对example age的概念的理解。所有的训练数据其实都是历史数据，距离当前的时刻都过去了一段时间，站在当前来看，距离当前原因的数据，对当前的影响应该是越小的。就比如1年前我买了白色的铅笔，对我现在需要不需要再买一支黑色的钢笔的影响是微乎其微的。而example age其实就是给了每一条数据一个权重，引用一下原论文的描述<code>In (5b), the example age is expressed as tmax  tN where tmax is the maximum observed time in the training data</code>，我这边采取了(tmax  tN)/tmax的赋权方式：<br>
</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 376px; max-height: 350px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 93.08999999999999%;\"></div>
<div class=\"image-view\" data-width=\"376\" data-height=\"350\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1129359-4edce5aad3bacbfe.png\" data-original-width=\"376\" data-original-height=\"350\" data-original-format=\"image/png\" data-original-filesize=\"39489\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p></p>
<p>很悲催的是，直观的离线训练数据并没有给出很直观的效果提升，但是由于评估机制的问题（我们后面会说到），我会在实际上线 做abtest的时候重新验证我的这个example age的点，但是可以肯定的是，理论和逻辑上，给样本数据进行权重的更改，是一个可以深挖的点，对线上的鲁棒性的增强是正向的。</p>
<ul>
<li><strong>user feature的选择方向</strong></li>
</ul>
<p>很不幸的是，在这一块的提升，确实没有论文中说的那么好，对于整个网络的贡献，以我做的实际项目的结果来说，history click embedded item &gt; history click embedded brand &gt; history click embedded sort &gt; user info &gt; example age &gt; others。不过，因为时间、数据质量、数据的真实性的原因，可能作为原始input的数据构造的就没有那么好。这边主要和大家说两个点：</p>
<p>1.topic数据<br>
原论文中在第四节的RANKING中指出:<br>
<code>We observe that the most important signals are those that describe a user’s previous interaction with the item itself and other similar items, matching others’ experience in ranking ads</code><br>
论文中还举出了比如用户前一天的每个频道（topic）的浏览视频个数，最后一次浏览距今时间，其实说白了就是强调了过去的行为汇总对未来的预测的作用，认为过去的行为贯穿了整体的用户点击轨迹。<br>
除此之外，G厂大佬还认为一些用户排序性质的描述特征对后面的ranking部分的提高也是蛮重要的，这边还举出了用户视频评分的例子，更多的内容大家可以自己去看一下原论文的部分，应该都会有自己的体会。</p>
<p>回到我们的项目，因为yoho!buy是电商，我类比着做了用户每个类目（裤子、衣服、鞋子...）的历史浏览点击购买次数、最后一次点击距今时长等等的topic信息，提升不是很明显。但是在大家做G厂这边论文，准确率陷入困境的时候，可以尝试一下这边的思路。</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 444px; max-height: 236px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 53.15%;\"></div>
<div class=\"image-view\" data-width=\"444\" data-height=\"236\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1129359-76bb7e2c9bb585cc.png\" data-original-width=\"444\" data-original-height=\"236\" data-original-format=\"image/png\" data-original-filesize=\"31351\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>2.query infomation<br>
相比于论文中的user information的添加，在实际模型测试中，我们发现，query的information的部分有更多的\"遐想\"。</p>
<p>原论文中点名指出user language and video language 做为basic info的重要性，这边给出的提升也是相对于user info有明显的增长的：</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 584px; max-height: 252px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 43.15%;\"></div>
<div class=\"image-view\" data-width=\"584\" data-height=\"252\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1129359-38c642822479bace.png\" data-original-width=\"584\" data-original-height=\"252\" data-original-format=\"image/png\" data-original-filesize=\"35853\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>有提升也自然有该部分的缺点：<br>
1.语言模型的处理复杂，耗时久<br>
在该部分的处理中，我强行拖着隔壁组的nlp博士和我一起搞了一周，每天都加班的搞去做数据清理，句法分析，语句树解析。如果需要让一个常规做推荐的人去弄，会有各种各样的坑，而且耗时还久<br>
2.语言新增问题<br>
商品的标题这类的文本处理还好，毕竟每日更新的数据存在一个可控的范围，但是用户搜索内容的变化是巨大的，粗略估测一下，一周时间间隔后，原提纯文本数据和新提纯文本数据的交集覆盖率不到78%，这意味着要重复的做nlp工作</p>
<ul>
<li><strong>attention 机制的引入</strong></li>
</ul>
<p>attention 机制的引入是我老大的硬性需求，我这边也就做了下，如果不了解attention 机制的朋友，可以阅读以下这边文章：<a href=\"https://blog.csdn.net/qq_21190081/article/details/53083516\" target=\"_blank\" rel=\"nofollow\">Attention model</a>。</p>
<p>我通俗的解释一下，不准确但是方便理解，Attention model就是让你每一个input与你的output计算一个similarity，再通过这些similarities给出每个input的权重。但是，很明显，我们离线训练还好，既有input也有output，但是线上预测的时候，就没有output了，所以，我们采取了lastclick替代的方式：</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 576px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 34.0%;\"></div>
<div class=\"image-view\" data-width=\"1694\" data-height=\"576\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1129359-9d4c577e72fd248a.png\" data-original-width=\"1694\" data-original-height=\"576\" data-original-format=\"image/png\" data-original-filesize=\"60597\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 254px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 32.82%;\"></div>
<div class=\"image-view\" data-width=\"774\" data-height=\"254\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1129359-37029b65bb75fc82.png\" data-original-width=\"774\" data-original-height=\"254\" data-original-format=\"image/png\" data-original-filesize=\"36853\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>不得不说，老祖宗传下来的东西确实有独到之处，但是在提升了近1pp的rate代价之下，会有一个让人头疼的问题耗时。因为每一个input的weight需要和output进行一次相似度计算，而且后续还要对计算出的相似度进行处理，原本只需要6-7小时训练完的模型，在我加了3层Multihead Attention后被拖到了一天。数据量还只采样了一半，确实需要斟酌带来的提升与投入的成本之间的平衡问题。</p>
<ul>
<li><strong>video vectors的深坑</strong></li>
</ul>
<p>G厂一句话，我们测断腿。这句话不是瞎说的，大家应该还记得一开始我给出的那张图，在最上面有一行不是很明显的小字：video vectors。G厂的大佬们既没有说这些video vectors该怎么构造，也没有说video vectors需不需要变动，留下了一个乐趣点让大家体验。</p>
<p>刚开始我很傻的用了我们最开始的embedded item作为video vectors，与模型FC出来的user vectors进行点击，计算top items。我来来回回测了一个月，老命都快改没了，最后提升rate到4pp。然而RNN随便跑跑就能到达3pp，我说很不服气的，所以拉着同事一起脑洞了一下，我们之前做图片相似度匹配的时候，喜欢把图片的向量拆成颜色+款式+性别，所以我们就借用了一下，改成了embedded item + embedded brand + embedded sort作为video vectors，历史总是给我们惊喜，效果上一下子就能大到5.2pp左右，这个点的提升应该是得来的最意外的，建议大家在用的时候考虑一下。</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 246px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 26.229999999999997%;\"></div>
<div class=\"image-view\" data-width=\"938\" data-height=\"246\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1129359-b47b9a7120403eb6.png\" data-original-width=\"938\" data-original-height=\"246\" data-original-format=\"image/png\" data-original-filesize=\"43153\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<ul>
<li><strong>实时化的选择</strong></li>
</ul>
<p>实时部署上，我们用了tensor flow serving，没什么好说的，给一下关键代码，大家看下自己仿一下就行，一般自己做做demo不需要，企业级上线才需要，企业级上线的那些大佬可能也比我有更多想法，所以就不展开了。</p>
<pre><code>部署及用python作为Client进行调用的测试：
#1.编译服务
bazel build //tensorflow_serving/model_servers:tensorflow_model_server
#2.启动服务
bazel-bin/tensorflow_serving/model_servers/tensorflow_model_server --port=9005 --model_name=test --model_base_path=/Data/sladesha/tmp/test/ 
#3.编译文件 
bazel build //tensorflow_serving/test:test_client
#4.注销报错的包
注销：/Data/muc/serving/bazel-bin/tensorflow_serving/test/test_client.runfiles/org_tensorflow/tensorflow/contrib/image/__init__.pyc中的from tensorflow.contrib.image.python.ops.single_image_random_dot_stereograms import single_image_random_dot_stereograms
参考：https://github.com/tensorflow/serving/issues/421
#5.运行
bazel-bin/tensorflow_serving/test/test_client --server=localhost:9005
</code></pre>
<p>相关的问题，有大佬已经梳理好了，自取其他可选的一些参数设置：<a href=\"https://blog.csdn.net/langb2014/article/details/54317490\" target=\"_blank\" rel=\"nofollow\">tensorflow serving 参数设置</a>。</p>
<p>还有一些评估技巧，模型之间的对比技巧，这边就不细讲了，可借鉴的意义也不大。</p>
<h2>三、总结</h2>
<p>虽然早就读过这篇文章，但是实现之后，发现新收获仍然不少。我特别赞成清凇的一句话:\'对于普通的学术论文，重要的是提供一些新的点子，而对于类似google这种工业界发布的paper，特别是带有practical lessons的paper，很值得精读。\'<br>
G厂的这个推荐代码和attention model的代码之前是准备放GitHub的，想想还是算了。一是之前也放过很多此代码，也没什么反馈，二是这两个代码自己写也不是很难，可以作为练手项目。</p>
<h2>四、鸣谢</h2>
<p>以上我个人在Yoho!Buy团队在实践中的一点总结，不代表公司的任何言论，仅仅是我个人的观点。最后感谢项目推进过程中所有合作方和项目组同学的付出和努力，感谢各个团队各位老大们的支持！溜了溜了。</p>

          </div>','1531183662'),
('325557','{1123926}{1123927}{3954}{1123928}{36}{39}{1121}{1096}{1123929}{87861}{1123930}{1100}{975714}{2341}{1277}{975724}{598}{1076}{975740}{975890}{530}{408}{976114}{3343}{3352}{785}{6049}{62}{1132}{975786}{975951}{542}{247}{975757}{453}{975768}{975935}{975755}{1137}{564}{761}{1031}{946}{712}{975738}{975820}{975741}{3403}{978116}{189}','Spectral Normalization for Generative Adversarial 对抗生成网络的谱标准化 摘要： 生成对抗网络研究中的一个挑战就是它训练的不稳定性。在本篇文章中，我们提出了一种新的称为谱标准化的权重标准化技术来稳定分辨器的训练。我们的新的标准化技术计算量少，并且很容易并入现有的实现中。我们在CIFAR10，STL-10和ILSVRC2012数据集上测试了谱标准化的功效，然后我们在实验上证实了谱标','Spectral Normalization for Generative Adversarial ','<div class=\"show-content-free\">
            <p>对抗生成网络的谱标准化</p>
<p>摘要：<br>
生成对抗网络研究中的一个挑战就是它训练的不稳定性。在本篇文章中，我们提出了一种新的称为谱标准化的权重标准化技术来稳定分辨器的训练。我们的新的标准化技术计算量少，并且很容易并入现有的实现中。我们在CIFAR10，STL-10和ILSVRC2012数据集上测试了谱标准化的功效，然后我们在实验上证实了谱标准化的GANs（SN-GANs）能够产生相较之前的训练稳定技术更高质量或者质量相当的图片。用Chainer实现的代码，生成的图片以及预训练的模型都可以在<a href=\"https://github.com/pfnet-research/sngan_projection\" target=\"_blank\" rel=\"nofollow\">https://github.com/pfnet-research/sngan_projection</a>上获得。</p>
<p>GAN很久以来都有训练不稳定的问题，因此很多的研究都着手解决这个问题。多篇论文认为训练分辨器D相当于训练一个好的估计器来估计建模分布（也就是生成的数据分布）和目标分布之间的密度比率。而训练GANs一直存在的挑战是控制分辨器的表现。因为在目标分布和建模分布是分开的情况下，可以存在一个分辨器能完美的将生成的数据和真实的数据完全区分开（这个现象很常见，一开始来写GAN代码的，会有人经常忘记将输入的真实图片归一化到[-1,1]区间，而生成的数据都是在[-1,1]区间，这个时候去训练，很容易发现G的loss很快就达到0的最优化，接下去训练G，生成的图片质量很难提升，这就是因为这两个分布差异很大，D很容易就区分，然后达到了最优化）。</p>
<p>标准的GAN的分辨器D的最优的形式是下列式（3），它的导数是下列式（4）,它们都可能没有边界或者说无法计算，因而需要一定的机制来限定f(x)的导数，</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 119px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 9.610000000000001%;\"></div>
<div class=\"image-view\" data-width=\"1238\" data-height=\"119\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6080435-3b91c71f935af2b3.png\" data-original-width=\"1238\" data-original-height=\"119\" data-original-format=\"image/png\" data-original-filesize=\"12771\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 94px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 9.53%;\"></div>
<div class=\"image-view\" data-width=\"986\" data-height=\"94\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6080435-5be97e57785a116d.png\" data-original-width=\"986\" data-original-height=\"94\" data-original-format=\"image/png\" data-original-filesize=\"7915\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>在正式介绍文章中提出的谱标准化之前，必须得强调一个概念，就是矩阵的范数（matrix norm）和向量的范数（vector norm）是存在区别的，具体的两者分别的概念，可以先去维基百科上了解下：</p>
<p>矩阵范数：<a href=\"https://en.wikipedia.org/wiki/Matrix_norm\" target=\"_blank\" rel=\"nofollow\">https://en.wikipedia.org/wiki/Matrix_norm</a></p>
<p>范数：<a href=\"https://en.wikipedia.org/wiki/Norm_(mathematics)\" target=\"_blank\" rel=\"nofollow\">https://en.wikipedia.org/wiki/Norm_(mathematics)</a></p>
<p>接下来提出的谱标准化，其实用到的就是矩阵的2-范式，对于矩阵W的谱标准化如下式：</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 86px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 10.63%;\"></div>
<div class=\"image-view\" data-width=\"809\" data-height=\"86\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6080435-443fd49d3523e153.png\" data-original-width=\"809\" data-original-height=\"86\" data-original-format=\"image/png\" data-original-filesize=\"4241\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
<p>其中，σ(W)表示的是W的二范式，如果对于分辨器D的每层权重W都做如上所示的谱标准化，那么将分辨器D看做一个函数隐射f，即可将其Lipschitz范数约束在1以下，下面试文章中给出的简单证明过程，f为D的函数表示，第L层的映射为hL-&gt; W(L+1)hL。对于矩阵范数，有如下重要的两个性质：</p>

<p>1.一致性||AB||&lt;=||A|| ||B||</p>
<p>2.线性性：对于任意系数α有，||αA||=|α| ||A||</p>
<p>省略D的各个层的加上的biase，那么对于分辨器D的f函数利用一致性有下列不等式：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 173px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 14.08%;\"></div>
<div class=\"image-view\" data-width=\"1229\" data-height=\"173\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6080435-42f52753336a809e.png\" data-original-width=\"1229\" data-original-height=\"173\" data-original-format=\"image/png\" data-original-filesize=\"18117\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
<p>然后在上述式7中的不等式中，对于每个W代入式（8），对于σ(W)看做一个常数，利用性质2线性性，那么可以得出f的上界为1：</p>

<p>||f||Lip &lt;= 1</p>
<p>这样就达到了限制分辨器D的Lipschitz范数的效果。</p>
<p>在文章2.3小结的梯度分析中可以看出，相对常规的GAN，谱标准化后的GAN引入了新的正则项，该正则项防止W的列空间在训练中只关心一个特定的方向，与此同时其防止D中每层的转换对某一个方向敏感。</p>
<p>其实谱标准化很简单的表述就是，每层的权重W，在更新后，都除以W的最大的奇异值，但是奇异值的分解计算是很耗时的，因而文中采用的是一个被称作power iteration的方式，来获得近似的最大奇异值的解。</p>
<p>实验部分</p>
<p>作者设计了多个实验来验证谱标准化的有效性。首先进行的实验，是采用多组训练的设置参数，来观察在谱标准化对于模型鲁棒性的影响。采用的评价指标分别是Inception Score和Fréchet inception distance（FID），使用的数据集分别是CIFAR-10和STL-10,其对比的实验结果如下图所示</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 471px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 67.38%;\"></div>
<div class=\"image-view\" data-width=\"1223\" data-height=\"824\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6080435-1c742c8414c86ed9.png\" data-original-width=\"1223\" data-original-height=\"824\" data-original-format=\"image/png\" data-original-filesize=\"81517\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 493px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 40.150000000000006%;\"></div>
<div class=\"image-view\" data-width=\"1228\" data-height=\"493\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6080435-0213c8447ea71b21.png\" data-original-width=\"1228\" data-original-height=\"493\" data-original-format=\"image/png\" data-original-filesize=\"46444\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>除此之外，作者在CIFAR-10和STL-10上对比了多个正则化的算法的表现能力，同时也发现采用谱标准化的GAN，在迭代多次后，仍未达到收敛，其中Inception Score一直在上升。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 550px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 78.68%;\"></div>
<div class=\"image-view\" data-width=\"924\" data-height=\"727\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6080435-a991581859ce55dc.png\" data-original-width=\"924\" data-original-height=\"727\" data-original-format=\"image/png\" data-original-filesize=\"83636\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
<p>为了证实SN-GAN可以使得权重不只关心一个方向（前面梯度分析中提到的），于是其分析了，在不同的标准化情况下得到的最优化的GAN中，分辨器D中权重的分布情况，实验结果图中，所有的权重都被归一化到了[0,1]区间中，从图中可以清晰的看出，采用谱标准化优化产生的D，其中的权重的数值分布较为广泛，并且具有多样性。</p>

<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 587px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 62.92%;\"></div>
<div class=\"image-view\" data-width=\"933\" data-height=\"587\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6080435-6737a4c6ad998b09.png\" data-original-width=\"933\" data-original-height=\"587\" data-original-format=\"image/png\" data-original-filesize=\"88396\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>由于与正交标准化同样使用了谱范数，文章不仅从理论上分析了两种方法的不同，同时从实验上，测试了两者的区别，实验是在STL-10数据集上，增加最后一层特征图的维度，用Inception Score来度量两者的性能差别，结果如图：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 452px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 47.18%;\"></div>
<div class=\"image-view\" data-width=\"958\" data-height=\"452\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6080435-99602e639a90ece4.png\" data-original-width=\"958\" data-original-height=\"452\" data-original-format=\"image/png\" data-original-filesize=\"63355\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>谱标准化在GAN上的应用，最引人关注的最大的一个原因就是文中最后一个大的实验，用谱标准化的GAN在ImageNet上生成1000类的图片。这个实验，首先将ImageNet的图片缩放到了128*128的规模，采用的GAN的架构是conditional GAN结构，而GAN的loss采用的是被称作hinge loss的损失函数，如下：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 155px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 16.72%;\"></div>
<div class=\"image-view\" data-width=\"927\" data-height=\"155\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6080435-60a059e94c6e9f07.png\" data-original-width=\"927\" data-original-height=\"155\" data-original-format=\"image/png\" data-original-filesize=\"16917\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>在面对产生高维度的数据集的情况下，其他的标准化方法都无法产生有意义的图片，只有谱标准化方法的GAN和正交标准化的GAN产生了有意义的结果，并且SN-GAN生成的图片的Inception Score一直在上升：</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 478px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 51.62%;\"></div>
<div class=\"image-view\" data-width=\"926\" data-height=\"478\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6080435-9e88b0076cd02e1d.png\" data-original-width=\"926\" data-original-height=\"478\" data-original-format=\"image/png\" data-original-filesize=\"37878\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>总结：</p>
<p>这篇文章提出了一种简单有效的标准化方法来限制GAN中分辨器D的优化过程，从而达到整个模型能学习到更好的生成器G的结果。本文采用单个的G和D的模型，在ImageNet上产生了较高质量的图片，因而值得关注。与此同时，文中多处的实验表明，采用了谱标准化的GAN，依然存在收敛缓慢的问题，这个收敛缓慢是相较于自身而言的，因为作者多次提及迭代次数的增加，Inception Score一直处于提升的状况，所以近一步改进，是提高其收敛的速度。从另外一个侧面看，现在的GAN的问题，大家一致认为是在于D，而不是在于G，因而有多篇文章在解决D上发生的问题，同时通过不同的角度阐释这些问题发生的原因。</p>
<p>最后，除了作者提供的实现代码之外，一份tensorflow的简洁清楚的实现如下：<br>
</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 456px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 51.01%;\"></div>
<div class=\"image-view\" data-width=\"894\" data-height=\"456\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6080435-52327dc06213ada6.png\" data-original-width=\"894\" data-original-height=\"456\" data-original-format=\"image/png\" data-original-filesize=\"29200\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
<a href=\"https://github.com/taki0112/Spectral_Normalization-Tensorflow\" target=\"_blank\" rel=\"nofollow\">https://github.com/taki0112/Spectral_Normalization-Tensorflow</a><p></p>

          </div>','1531183663'),
('325558','{17}{1123937}{1077}{6390}{36}{1751}{1123938}{975761}{1302}{825369}{23472}{4861}{1094}{975714}{975728}{442}{1123942}{1084019}{1123943}{1084029}{1901}{5131}{1740}{569}{975755}{976193}{1580}{351}{296}{1123945}{51913}{4871}{975892}{975717}{975911}{761}{5559}{1887}{2397}{976136}{2336}{6483}{1560}{454}{1123948}{975935}{189}{303}{2109}{1190}','深度学习——目标检测（2） 前言：RCNN虽然能进行目标检测，但检测的精确度，尤其是速度方面太慢了，没秒才0.03帧。在RCNN基础上做了改进就有了FAST RCNN和FASTER RCNN Fast-RCNN Fast-RCNN主要贡献在于对RCNN进行加速。 在以下方面得到改进： 1 - 借鉴SSP思路，提出简化版的ROI池化层（注意，没用金字塔），同时加入了候选框映射功能，使得网络能够反向传播，解决了SPP的整体网络训','深度学习——目标检测（2）','<div class=\"show-content-free\">
            <p>前言：RCNN虽然能进行目标检测，但检测的精确度，尤其是速度方面太慢了，没秒才0.03帧。在RCNN基础上做了改进就有了FAST RCNN和FASTER RCNN</p>
<h1>Fast-RCNN</h1>
<p>Fast-RCNN主要贡献在于对RCNN进行加速。<br>
在以下方面得到改进：<br>
1 - 借鉴SSP思路，提出简化版的ROI池化层（注意，没用金字塔），同时加入了候选框映射功能，使得网络能够反向传播，解决了SPP的整体网络训练问题；</p>
<h4>什么是ssp网络？</h4>
<p>字面意思就是你可能听说过的“池化金字塔”。论文名字：Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</p>
<ul>
<li>
<p>先说目的：解决输入输出矩阵（feature map）大小不一样的问题：其他优点：由于把一个feature map从不同的角度进行特征提取，再聚合的特点，显示了算法的robust的特性。第三：同时也在object recongtion增加了精度。其实，你也可以这样想，最牛掰的地方是因为在卷积层的后面对每一张图片都进行了多方面的特征提取，他就可以提高任务的精度<br>
网络结构如下图：</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 576px; max-height: 407px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 70.66%;\"></div>
<div class=\"image-view\" data-width=\"576\" data-height=\"407\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1652713-8a755e30eb08f66e.png\" data-original-width=\"576\" data-original-height=\"407\" data-original-format=\"image/png\" data-original-filesize=\"58548\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
<p>从下往上看，这是一个传统的网络架构模型，5层卷积层，这里的卷积层叫做convolution和pooling层的联合体，统一叫做卷积层，后面跟随全连接层。我们这里需要处理的就是在网络的全连接层前面加一层金字塔pooling层解决输入图片大小不一的情况。我们可以看到这里的spatital pyramid pooling layer就是把前一卷积层的feature maps的每一个图片上进行了3个卷积操作。最右边的就是原图像，中间的是把图像分成大小是4的特征图，最右边的就是把图像分成大小是16的特征图。那么每一个feature map就会变成16+4+1=21个feature maps。这不就解决了特征图大小不一的状况了吗？</p>
</li>
</ul>
<p>2 - 多任务Loss层<br>
A）SoftmaxLoss代替了SVM，证明了softmax比SVM更好的效果；<br>
B）SmoothL1Loss取代Bouding box回归。<br>
将分类和边框回归进行合并（又一个开创性的思路），通过多任务Loss层进一步整合深度网络，统一了训练过程，从而提高了算法准确度。</p>
<ul>
<li>
<p>FAST RCNN框架图</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 295px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 40.08%;\"></div>
<div class=\"image-view\" data-width=\"736\" data-height=\"295\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1652713-30c9b4ad174b066f.png\" data-original-width=\"736\" data-original-height=\"295\" data-original-format=\"image/png\" data-original-filesize=\"263014\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
</li>
<li>与R-CNN框架图对比，可以发现主要有两处不同：<br>
一是最后一个卷积层后加了一个ROI pooling layer，ROI pooling layer实际上是SPP-NET的一个精简版<br>
二是损失函数使用了多任务损失函数(multi-task loss)，将边框回归直接加入到CNN网络中训练。R-CNN训练过程分为了三个阶段，而Fast R-CNN直接使用softmax替代SVM分类，同时利用多任务损失函数边框回归也加入到了网络中，这样整个的训练过程是端到端的(除去region proposal提取阶段)。Fast R-CNN在网络微调的过程中，将部分卷积层也进行了微调，取得了更好的检测效果。<br>
小结：Fast R-CNN融合了R-CNN和SPP-NET的精髓，并且引入多任务损失函数，使整个网络的训练和测试变得十分方便。<br>
缺点：region proposal的提取使用selective search，目标检测时间大多消耗在这上面（提region proposal 2~3s，而提特征分类只需0.32s），无法满足实时应用，而且并没有实现真正意义上的端到端训练测试（region proposal使用selective search先提取处来）。</li>
</ul>
<h1>FASTER RCNN</h1>
<p>这里完全实现了端对端的操作</p>
<ul>
<li>什么是端对端？<br>
就是从输入到输出的全过程都在深度学习网络中进行，这是一种美妙的趋势。<br>
在Fast R-CNN网络基础上引入RPN网络进行先验框的选取。其他一样，只是region proposal现在是用RPN网络提取的（代替原来的selective search）</li>
</ul>
<h4>RPN网络</h4>
<p>RPN的核心思想是使用卷积神经网络直接产生region proposal，使用的方法本质上就是滑动窗口。RPN的设计比较巧妙，RPN只需在最后的卷积层上滑动一遍，因为anchor机制和边框回归可以得到多尺度多长宽比的region proposal。<br>
RPN网络的特点在于通过滑动窗口的方式实现候选框的提取，每个滑动窗口位置生成9个候选窗口（不同尺度、不同宽高），提取对应9个候选窗口（anchor）的特征，用于目标分类和边框回归，与FastRCNN类似。目标分类只需要区分候选框内特征为前景或者背景。<br>
边框回归确定更精确的目标位置，基本网络结构如下图所示：</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 492px; max-height: 264px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 53.66%;\"></div>
<div class=\"image-view\" data-width=\"492\" data-height=\"264\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1652713-ce929c12f75359ee.png\" data-original-width=\"492\" data-original-height=\"264\" data-original-format=\"image/png\" data-original-filesize=\"45433\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
<p>RPN网络也是全卷积网络（FCN，fully-convolutional network），可以针对生成检测建议框的任务端到端地训练，能够同时预测出object的边界和分数。只是在CNN上额外增加了2个卷积层（全卷积层cls和reg）。</p>

<p>①将每个特征图的位置编码成一个特征向量（256dfor ZF and 512d for VGG）。</p>
<p>②对每一个位置输出一个objectness score和regressedbounds for k个region proposal，即在每个卷积映射位置输出这个位置上多种尺度（3种）和长宽比（3种）的k个（3*3=9）区域建议的物体得分和回归边界。</p>
<p>RPN网络的输入可以是任意大小（但还是有最小分辨率要求的，例如VGG是228*228）的图片。如果用VGG16进行特征提取，那么RPN网络的组成形式可以表示为VGG16+RPN。</p>
<ul>
<li>
<p>FASTER RCNN结构</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 516px; max-height: 513px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 99.42%;\"></div>
<div class=\"image-view\" data-width=\"516\" data-height=\"513\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1652713-33d0c14665105485.png\" data-original-width=\"516\" data-original-height=\"513\" data-original-format=\"image/png\" data-original-filesize=\"70191\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
</li>
<li>FASTER RCNN过程<br>
训练过程中，涉及到的候选框选取，选取依据：<br>
1）丢弃跨越边界的anchor；<br>
2）与样本重叠区域大于0.7的anchor标记为前景，重叠区域小于0.3的标定为背景；<br>
对于每一个位置，通过两个全连接层（目标分类+边框回归）对每个候选框（anchor）进行判断，并且结合概率值进行舍弃（仅保留约300个anchor），没有显式地提取任何候选窗口，完全使用网络自身完成判断和修正。<br>
从模型训练的角度来看，通过使用共享特征交替训练的方式，达到接近实时的性能，交替训练方式描述为：<br>
1）根据现有网络初始化权值w，训练RPN；<br>
2）用RPN提取训练集上的候选区域，用候选区域训练FastRCNN，更新权值w；<br>
3）重复1、2，直到收敛。</li>
</ul>
<h1>RCNN网络的演进</h1>
<p></p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 174px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 22.770000000000003%;\"></div>
<div class=\"image-view\" data-width=\"764\" data-height=\"174\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1652713-7c3593c4793e842b.png\" data-original-width=\"764\" data-original-height=\"174\" data-original-format=\"image/png\" data-original-filesize=\"26887\"></div>
</div>
<div class=\"image-caption\">image.png</div>
</div>
<br>
更多关于FASTER RCNN的理解，可以去看博客<a href=\"https://blog.csdn.net/lk123400/article/details/54343550/\" target=\"_blank\" rel=\"nofollow\">https://blog.csdn.net/lk123400/article/details/54343550/</a><p></p>

          </div>','1531183664'),
('325559','{1302}{1084019}{1737}{975761}{72}{442}{1094}{8647}{975717}{975728}{28873}{24}{587}{975703}{1580}{30}{1123952}{163372}{769}{36}{3289}{976042}{1740}{296}{975835}{975744}{976193}{62}{975714}{1785}{2391}{351}{4861}{424}{975768}{3150}{975740}{975857}{616}{884}{3537}{1901}{979369}{1123953}{6368}{975741}{975911}{2372}{975721}{2337}','就是对每一类而言，若一个proposal与一个分值比它大的proposal相交，且IoU（intersection over union，即相交面积比这两个proposal的并集面积之比）小于于一定阈值的情况下，则抛弃该proposal 正负样本比例应该在1;','深度学习——目标检测（1）','<div class=\"show-content-free\">
            <p>前言：深度学习在图像的应用中目标检测是最基本也是最常用的，下面介绍几种常见的目标检测算法或者模型</p>
<h1>什么是目标检测？</h1>
<p>目标检测主要是明确从图中看到了什么物体？他们在什么位置。<br>
从目标检测的概念可以得到：<br>
也就是传统的目标检测方法一般分为三个阶段：首先在给定的图像上选择一些候选的区域，然后对这些区域提取特征，最后使用训练的分类器进行分类。</p>
<ol>
<li>区域选择<br>
这一步是为了对目标进行定位。传统方法是采用穷举策略。由于目标可能在图片上的任意位置，而且大小不定，因此使用滑动窗口的策略对整幅图像进行遍历，而且需要设置不同的长宽。这种策略虽然可以检测到所有可能出现的位置，但是时间复杂度太高，产生的冗余窗口太多，严重影响后续特征的提取和分类速度的性能。</li>
<li>特征提取<br>
提取特征的好坏会直接影响到分类的准确性，但又由于目标的形态多样性，提取一个鲁棒的特征并不是一个简单的事。这个阶段常用的特征有SIFT（尺度不变特征变换 ，Scale-invariant feature transform）和HOG（ 方向梯度直方图特征，Histogram of Oriented Gradient）等。</li>
<li>分类器<br>
主要有SVM，Adaboost等综上所述，传统目标检测存在两个主要问题：一个是基于滑动窗口的区域选择策略没有针对性，时间复杂度高，窗口冗余；而是手工设计的特征对于多样性没有很好的鲁棒性。</li>
</ol>
<h1>RCNN</h1>
<p>rcnn是目标检测早期的模型算法。R是指region proposal（候选区域）。也就是先通过人工预先找到目标可能出现的位置。然后进行cnn对图像的目标进行识别。</p>
<h1>RCNN的检测流程：</h1>
<p>RCNN主要分为3个大部分，第一部分产生候选区域，第二部分对每个候选区域使用CNN提取长度固定的特征；第三个部分使用一系列的SVM进行分类。<br>
</p>
<p></p>
下面就是RCNN的整体检测流程：<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 643px; max-height: 224px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 34.839999999999996%;\"></div>
<div class=\"image-view\" data-width=\"643\" data-height=\"224\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1652713-7c614b75131f7c18.png\" data-original-width=\"643\" data-original-height=\"224\" data-original-format=\"image/png\" data-original-filesize=\"119478\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
转化为文字表述为：<br>
<p>（1）首先输入一张自然图像;<br>
（2）使用Selective Search提取大约2000个候选区域（proposal）;<br>
（3）对每个候选区域的图像进行拉伸形变，使之成为固定大小的正方形图像，并将该图像输入到CNN中提取特征;<br>
使用AlexNet对得到的候选区域的图像进行特征提取，最终生成的是一个4096维的特征向量。注意AlexNet输入的是227x227大小的图像，因此在输入到AlexNet之前，作者把候选区域的图像首先进行了一小部分的边缘扩展（16像素），然后进行了拉伸操作，使得输入的候选区域图像满足AlexNet的输入要求（即227x227）。</p>

<ul>
<li>注意这里进行的是拉伸操作而不是padding。原因是padding的效果不是很好。</li>
</ul>
<p>（4）使用线性的SVM对提取的特征进行分类。</p>
<ul>
<li>非最大值抑制NMS<br>
在上述过程中（首先使用selective search提取测试图像的2000个proposals，然后将所有proposal图像拉伸到合适的大小并用CNN进行特征提取，得到固定长度的特征向量。最终对于每个类别，使用为该类别训练的SVM分类器对得到的所有特征向量（对应每个proposal图像）进行打分（代表的是这个proposal是该类的概率），应用到了NMS。<br>
什么是NMS?<br>
就是对每一类而言，若一个proposal与一个分值比它大的proposal相交，且IoU（intersection over union，即相交面积比这两个proposal的并集面积之比）小于于一定阈值的情况下，则抛弃该proposal</li>
<li>正负样本比例应该在1;3左右</li>
</ul>
<h1>Bounding-box回归</h1>
<p>这是为了提高定位效果，原作者提出的一个回归模型算法。<br>
Bounding-box Regression训练的过程中，输入数据为N个训练对</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 193px; max-height: 37px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 19.17%;\"></div>
<div class=\"image-view\" data-width=\"193\" data-height=\"37\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1652713-fda69110ee7c787b.png\" data-original-width=\"193\" data-original-height=\"37\" data-original-format=\"image/png\" data-original-filesize=\"4079\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
<p>其中</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 182px; max-height: 29px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 15.93%;\"></div>
<div class=\"image-view\" data-width=\"182\" data-height=\"29\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1652713-5f1c46e6f6db97ba.png\" data-original-width=\"182\" data-original-height=\"29\" data-original-format=\"image/png\" data-original-filesize=\"2242\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
为proposal的位置，前两个坐标表示proposal的中心坐标，后面两个坐标分别表示proposal的width和height，而<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 210px; max-height: 37px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 17.62%;\"></div>
<div class=\"image-view\" data-width=\"210\" data-height=\"37\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1652713-e07ff29fbe9cd7e3.png\" data-original-width=\"210\" data-original-height=\"37\" data-original-format=\"image/png\" data-original-filesize=\"5601\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
表示groundtruth的位置，regression的目标就是学会一种映射将P转换为G。<br>
<p>其中将t看做label，他表示p和g之间的关系，当模型参数训练出来之后，就可以通过p来预测g的位置了。</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 552px; max-height: 149px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 26.99%;\"></div>
<div class=\"image-view\" data-width=\"552\" data-height=\"149\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1652713-2c7ce2521aadba6b.png\" data-original-width=\"552\" data-original-height=\"149\" data-original-format=\"image/png\" data-original-filesize=\"20651\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<ul>
<li>
<p>对于pair(P,G)的选择是选择离P较近的G进行配对，这里表示较近的方法是需要P和一个G的最大的IoU要大于0.6,否则则抛弃该P。<br>
可以写出目标函数如下：</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 129px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 17.25%;\"></div>
<div class=\"image-view\" data-width=\"748\" data-height=\"129\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1652713-0e81fa5b45f36a64.png\" data-original-width=\"748\" data-original-height=\"129\" data-original-format=\"image/png\" data-original-filesize=\"21130\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
</li>
<li>
<p>总的来说设计Bounding-box回归是为了使得提高定位效果，就是在原来定位的基础上进行微调使得，定位更加准确。如下图：</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 503px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 56.330000000000005%;\"></div>
<div class=\"image-view\" data-width=\"893\" data-height=\"503\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1652713-f0ad538510279c61.png\" data-original-width=\"893\" data-original-height=\"503\" data-original-format=\"image/png\" data-original-filesize=\"98865\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
</li>
<li>注意：只有当Proposal和Ground Truth比较接近时（线性问题），我们才能将其作为训练样本训练我们的线性回归模型，否则会导致训练的回归模型不work（当Proposal跟GT离得较远，就是复杂的非线性问题了，此时用线性回归建模显然不合理）。所以才叫“微调”</li>
<li>输入：输入就是这四个数值吗？其实真正的输入是这个窗口对应的CNN特征，也就是R-CNN中的Pool5feature（特征向量）这就是深度学习的特征吧，就是无限模拟接近label</li>
</ul>

          </div>','1531183665'),
('325560','{36}{62}{4219}{1123967}{975757}{17}{1123968}{2119}{1082}{1137}{997865}{975935}{4982}{733}{183621}{2149}{1123969}{201}{975755}{9949}{442}{1094}{785}{976193}{976063}{1190}{63}{975708}{975761}{975728}{166671}{582}{975915}{975741}{976008}{975886}{2960}{975721}{2029}{1123970}{184}{3537}{975798}{975714}{975776}{72}{113508}{975910}{463}{975785}','format(i) for i in range(1000)] for fname in fnames: src = os.path.join(original_dataset_dir, fname) dat = os.path.join(train_cats_dir, fname) shutil.copyfile(src, dat) fnames = [\'cat.{}.format(i) for i in range(1500, 2000)] for fname in fnames: src = os.path.join(original_dataset_dir, fname) dat = os.path.join(test_cats_dir, fname) shutil.copyfile(src, dat) fnames = [\'dog.{}.','基于Keras实现Kaggle2013--Dogs vs. Cats12500张猫狗图像的精准分类','<div class=\"show-content-free\">
            <h4>【下载数据集】</h4>
<ul>
<li>下载链接--<a href=\"https://pan.baidu.com/s/1_j-jndypkKkZVSldlVKjmA\" target=\"_blank\" rel=\"nofollow\">百度网盘</a><br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 471px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 51.99%;\"></div>
<div class=\"image-view\" data-width=\"906\" data-height=\"471\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2145769-71deb73d2f818230.png\" data-original-width=\"906\" data-original-height=\"471\" data-original-format=\"image/png\" data-original-filesize=\"587524\"></div>
</div>
<div class=\"image-caption\">关于猫的部分数据集示例</div>
</div>
</li>
</ul>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 480px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 58.18%;\"></div>
<div class=\"image-view\" data-width=\"825\" data-height=\"480\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2145769-ecee53c7fa177238.png\" data-original-width=\"825\" data-original-height=\"480\" data-original-format=\"image/png\" data-original-filesize=\"516577\"></div>
</div>
<div class=\"image-caption\">关于狗的部分数据集示例</div>
</div>
<h4>【整理数据集】</h4>
<ul>
<li>
<p>将训练数据集分割成训练集、验证集、测试集，目录结构如图所示：</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 245px; max-height: 333px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 135.92%;\"></div>
<div class=\"image-view\" data-width=\"245\" data-height=\"333\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2145769-86964833bc33a46a.png\" data-original-width=\"245\" data-original-height=\"333\" data-original-format=\"image/png\" data-original-filesize=\"4333\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
</li>
</ul>
<ul>
<li>在Pycharm中新建项目，创建split_dataset.py</li>
</ul>
<pre><code>import os, shutil

# 数据集解压之后的目录
original_dataset_dir = \'D:\\kaggle\\dogsvscats\\\\train\'
# 存放小数据集的目录
base_dir = \'D:\\kaggle\\dogsvscats\\\\cats_and_dogs_small\'
os.mkdir(base_dir)

# 建立训练集、验证集、测试集目录
train_dir = os.path.join(base_dir, \'train\')
os.mkdir(train_dir)
validation_dir = os.path.join(base_dir, \'validation\')
os.mkdir(validation_dir)
test_dir = os.path.join(base_dir, \'test\')
os.mkdir(test_dir)

# 将猫狗照片按照训练、验证、测试分类
train_cats_dir = os.path.join(train_dir, \'cats\')
os.mkdir(train_cats_dir)

train_dogs_dir = os.path.join(train_dir, \'dogs\')
os.mkdir(train_dogs_dir)

validation_cats_dir = os.path.join(validation_dir, \'cats\')
os.mkdir(validation_cats_dir)

validation_dogs_dir = os.path.join(validation_dir, \'dogs\')
os.mkdir(validation_dogs_dir)

test_cats_dir = os.path.join(test_dir, \'cats\')
os.mkdir(test_cats_dir)

test_dogs_dir = os.path.join(test_dir, \'dogs\')
os.mkdir(test_dogs_dir)

# 切割数据集
fnames = [\'cat.{}.jpg\'.format(i) for i in range(1000)]
for fname in fnames:
    src = os.path.join(original_dataset_dir, fname)
    dat = os.path.join(train_cats_dir, fname)
    shutil.copyfile(src, dat)

fnames = [\'cat.{}.jpg\'.format(i) for i in range(1000, 1500)]
for fname in fnames:
    src = os.path.join(original_dataset_dir, fname)
    dat = os.path.join(validation_cats_dir, fname)
    shutil.copyfile(src, dat)

fnames = [\'cat.{}.jpg\'.format(i) for i in range(1500, 2000)]
for fname in fnames:
    src = os.path.join(original_dataset_dir, fname)
    dat = os.path.join(test_cats_dir, fname)
    shutil.copyfile(src, dat)

fnames = [\'dog.{}.jpg\'.format(i) for i in range(1000)]
for fname in fnames:
    src = os.path.join(original_dataset_dir, fname)
    dat = os.path.join(train_dogs_dir, fname)
    shutil.copyfile(src, dat)

fnames = [\'dog.{}.jpg\'.format(i) for i in range(1000, 1500)]
for fname in fnames:
    src = os.path.join(original_dataset_dir, fname)
    dat = os.path.join(validation_dogs_dir, fname)
    shutil.copyfile(src, dat)

fnames = [\'dog.{}.jpg\'.format(i) for i in range(1500, 2000)]
for fname in fnames:
    src = os.path.join(original_dataset_dir, fname)
    dat = os.path.join(test_dogs_dir, fname)
    shutil.copyfile(src, dat)
</code></pre>
<h4>【建立简单版CNN网络模型】</h4>
<pre><code>from keras import layers
from keras import models
model = models.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation=\'relu\', input_shape=(150, 150, 3)))
model.add(layers.MaxPool2D((2, 2)))

model.add(layers.Conv2D(64, (3, 3), activation=\'relu\'))
model.add(layers.MaxPool2D((2, 2)))

model.add(layers.Conv2D(128, (3, 3), activation=\'relu\'))
model.add(layers.MaxPool2D((2, 2)))

model.add(layers.Conv2D(128, (3, 3), activation=\'relu\'))
model.add(layers.MaxPool2D((2, 2)))

model.add(layers.Flatten())
model.add(layers.Dense(512, activation=\'relu\'))
model.add(layers.Dense(1, activation=\'sigmoid\'))

model.compile(loss=\'binary_crossentropy\', optimizer=optimizers.RMSprop(lr=1e-4), metrics=[\'acc\'])
</code></pre>
<h4>【对图像信息进行预处理】</h4>
<ul>
<li>读取图片文件；</li>
<li>将jpg解码成RGB像素点；</li>
<li>将这些像素点转换成浮点型张量；</li>
<li>将[0, 255]区间的像素值减小到[0, 1]区间中，CNN更喜欢处理小的输入值。</li>
</ul>
<pre><code>train_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
    directory=train_dir,
    target_size=(150, 150),
    batch_size=20,
    class_mode=\'binary\')

validation_generator = test_datagen.flow_from_directory(
    directory=validation_dir,
    target_size=(150, 150),
    batch_size=20,
    class_mode=\'binary\')
</code></pre>
<ul>
<li>用fit_generator向模型中填充数据</li>
</ul>
<pre><code>history = model.fit_generator(
    train_generator,
    steps_per_epoch=100,
    epochs=30,
    validation_data=validation_generator,
    validation_steps=50)
</code></pre>
<ul>
<li>保存模型</li>
</ul>
<pre><code>model.save(\'cats_and_dogs_small_1.h5\')
</code></pre>
<ul>
<li>显示训练中loss和acc的曲线</li>
</ul>
<pre><code>acc = history.history[\'acc\']
val_acc = history.history[\'val_acc\']
loss = history.history[\'loss\']
val_loss = history.history[\'val_loss\']

epochs = range(1, len(acc) + 1)
plt.plot(epochs, acc, \'bo\', label=\'Training acc\')
plt.plot(epochs, val_acc, \'b\', label=\'Validation acc\')
plt.title(\'Training and validation accuracy\')
plt.legend()

plt.figure()

plt.plot(epochs, loss, \'bo\', label=\'Training loss\')
plt.plot(epochs, val_loss, \'b\', label=\'Validation loss\')
plt.title(\'Training and validation loss\')
plt.legend()

plt.show()
</code></pre>
<h4>【简单版CNN模型完整代码】</h4>
<pre><code>from keras import layers
from keras import models
import matplotlib.pyplot as plt
from keras import optimizers
from keras.preprocessing.image import ImageDataGenerator

train_dir = r\'D:\\kaggle\\\\dogsvscats\\\\cats_and_dogs_small\\\\train\'
validation_dir = r\'D:\\kaggle\\\\dogsvscats\\\\cats_and_dogs_small\\\\validation\'


model = models.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation=\'relu\', input_shape=(150, 150, 3)))
model.add(layers.MaxPool2D((2, 2)))

model.add(layers.Conv2D(64, (3, 3), activation=\'relu\'))
model.add(layers.MaxPool2D((2, 2)))

model.add(layers.Conv2D(128, (3, 3), activation=\'relu\'))
model.add(layers.MaxPool2D((2, 2)))

model.add(layers.Conv2D(128, (3, 3), activation=\'relu\'))
model.add(layers.MaxPool2D((2, 2)))

model.add(layers.Flatten())
model.add(layers.Dense(512, activation=\'relu\'))
model.add(layers.Dense(1, activation=\'sigmoid\'))

model.compile(loss=\'binary_crossentropy\', optimizer=optimizers.RMSprop(lr=1e-4), metrics=[\'acc\'])



# 调整像素值
train_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
    directory=train_dir,
    target_size=(150, 150),
    batch_size=20,
    class_mode=\'binary\')

validation_generator = test_datagen.flow_from_directory(
    directory=validation_dir,
    target_size=(150, 150),
    batch_size=20,
    class_mode=\'binary\')

history = model.fit_generator(
    train_generator,
    steps_per_epoch=100,
    epochs=30,
    validation_data=validation_generator,
    validation_steps=50)

model.save(\'cats_and_dogs_small_1.h5\')

acc = history.history[\'acc\']
val_acc = history.history[\'val_acc\']
loss = history.history[\'loss\']
val_loss = history.history[\'val_loss\']

epochs = range(1, len(acc) + 1)
plt.plot(epochs, acc, \'bo\', label=\'Training acc\')
plt.plot(epochs, val_acc, \'b\', label=\'Validation acc\')
plt.title(\'Training and validation accuracy\')
plt.legend()

plt.figure()

plt.plot(epochs, loss, \'bo\', label=\'Training loss\')
plt.plot(epochs, val_loss, \'b\', label=\'Validation loss\')
plt.title(\'Training and validation loss\')
plt.legend()

plt.show()
</code></pre>
<h4>【训练结果及分析】</h4>
<ul>
<li>
<p>训练结果</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 588px; max-height: 449px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 76.36%;\"></div>
<div class=\"image-view\" data-width=\"588\" data-height=\"449\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2145769-bc66ddb5e9c4d7cf.png\" data-original-width=\"588\" data-original-height=\"449\" data-original-format=\"image/png\" data-original-filesize=\"19140\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 572px; max-height: 458px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 80.07%;\"></div>
<div class=\"image-view\" data-width=\"572\" data-height=\"458\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2145769-0df829ca46bcfae2.png\" data-original-width=\"572\" data-original-height=\"458\" data-original-format=\"image/png\" data-original-filesize=\"19290\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
</li>
<li>分析<br>
训练曲线的最大特征就是过拟合。训练集上的准确率线性增加，接近100%，而验证集上的准确率是在70%~72%之间。同样的，训练集上的loss线性下降趋于0，而验证集上的loss在迭代5个epoch之后趋于上升。<br>
由于训练样本只选取了2000个，因此数据量不足是过拟合的最大的问题。缓解过拟合的方法有很多，诸如：dropout、L2-norm等等。在这里，我们使用<strong>增大数据（data augmentation）</strong>的方式来试一试解决过拟合，这种方法也是处理图片分类的通常做法。</li>
</ul>
<h4>【优化版本(1)---增大数据（data augmentation）】</h4>
<ul>
<li>
<h5>增大数据</h5>
</li>
</ul>
<p>过拟合是由于学习到样本量过小导致的，使得我们训练的模型对于新的数据没有很好的泛化能力。增大数据（data augmentation）是在已有的训练样本上增加数据的一种最好的方式。增大数据是通过随机的改变那些已经被模型“记住”的图片，通过缩放、裁剪、拉伸图片来使得模型不会两次见到同一张图片。<br>
在Keras中是通过ImageDataGenerator来实现，看一个例子：</p>
<pre><code>train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=40,
    width_shift_range=0.2,     # 宽度平移
    height_shift_range=0.2,   # 高度平移
    shear_range=0.2,            # 修剪
    zoom_range=0.2,            # 缩放
    horizontal_flip=True,
    fill_mode=\'nearest\')        # 添加新像素  
</code></pre>
<p>效果如下图所示，图一是原图像，图二是纵向平移，图三是纵向拉伸，图四 是添加了新的像素。</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 421px; max-height: 376px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 89.31%;\"></div>
<div class=\"image-view\" data-width=\"421\" data-height=\"376\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2145769-bae8673ab61bc77d.png\" data-original-width=\"421\" data-original-height=\"376\" data-original-format=\"image/png\" data-original-filesize=\"294364\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<ul>
<li>
<h5>隐患分析</h5>
</li>
</ul>
<p>虽然使用了增大数据，但是从输入数据上看还是有很大一部分是有联系的，是相似的，因为它们均来自同一张原始图片，并没有提供新的信息。为了长远的与过拟合做斗争，我们在模型中增加一层Dropout，并将batch_size调大为32，epoch调大至100，再来看看效果。</p>
<ul>
<li>
<h5>网络模型完整代码</h5>
</li>
</ul>
<pre><code>from keras import layers
from keras import models
import matplotlib.pyplot as plt
from keras import optimizers
from keras.preprocessing.image import ImageDataGenerator

train_dir = r\'D:\\kaggle\\\\dogsvscats\\\\cats_and_dogs_small\\\\train\'
validation_dir = r\'D:\\kaggle\\\\dogsvscats\\\\cats_and_dogs_small\\\\validation\'


model = models.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation=\'relu\', input_shape=(150, 150, 3)))
model.add(layers.MaxPool2D((2, 2)))

model.add(layers.Conv2D(64, (3, 3), activation=\'relu\'))
model.add(layers.MaxPool2D((2, 2)))

model.add(layers.Conv2D(128, (3, 3), activation=\'relu\'))
model.add(layers.MaxPool2D((2, 2)))

model.add(layers.Conv2D(128, (3, 3), activation=\'relu\'))
model.add(layers.MaxPool2D((2, 2)))

model.add(layers.Flatten())
model.add(layers.Dropout(0.5))
model.add(layers.Dense(512, activation=\'relu\'))
model.add(layers.Dense(1, activation=\'sigmoid\'))

model.compile(loss=\'binary_crossentropy\', optimizer=optimizers.RMSprop(lr=1e-4), metrics=[\'acc\'])



# 调整像素值
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=40,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True)
test_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
    directory=train_dir,
    target_size=(150, 150),
    batch_size=32,
    class_mode=\'binary\')

validation_generator = test_datagen.flow_from_directory(
    directory=validation_dir,
    target_size=(150, 150),
    batch_size=32,
    class_mode=\'binary\')

history = model.fit_generator(
    train_generator,
    steps_per_epoch=100,
    epochs=100,
    validation_data=validation_generator,
    validation_steps=50)

model.save(\'cats_and_dogs_small_2.h5\')

acc = history.history[\'acc\']
val_acc = history.history[\'val_acc\']
loss = history.history[\'loss\']
val_loss = history.history[\'val_loss\']

epochs = range(1, len(acc) + 1)
plt.plot(epochs, acc, \'bo\', label=\'Training acc\')
plt.plot(epochs, val_acc, \'b\', label=\'Validation acc\')
plt.title(\'Training and validation accuracy\')
plt.legend()

plt.figure()

plt.plot(epochs, loss, \'bo\', label=\'Training loss\')
plt.plot(epochs, val_loss, \'b\', label=\'Validation loss\')
plt.title(\'Training and validation loss\')
plt.legend()

plt.show()
</code></pre>
<ul>
<li>
<h5>优化版本(1)---训练结果及分析</h5>
</li>
<li>
<p>训练结果</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 584px; max-height: 448px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 76.71%;\"></div>
<div class=\"image-view\" data-width=\"584\" data-height=\"448\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2145769-8ddf352e9732a27a.png\" data-original-width=\"584\" data-original-height=\"448\" data-original-format=\"image/png\" data-original-filesize=\"38102\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
</li>
</ul>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 588px; max-height: 457px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 77.72%;\"></div>
<div class=\"image-view\" data-width=\"588\" data-height=\"457\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2145769-11a08fb8989ee85a.png\" data-original-width=\"588\" data-original-height=\"457\" data-original-format=\"image/png\" data-original-filesize=\"30816\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<ul>
<li>分析</li>
</ul>
<p>我们可以看出在准确率上有很大的提升，训练集和验证集上的准确率是在85%~86%之间。同样的，训练集和验证集上的loss均趋于0.35以下，未出现大幅度的过拟合。那么我们思考一下，如何将准确率达到90%以上，有一种很好的优化方式就是使用预训练模型。</p>
<h4>【优化版本(2)---预训练网络之特征提取】</h4>
<ul>
<li>
<h5>预训练卷积网络</h5>
</li>
</ul>
<p>才疏学浅，我也是第一次用到预训练的模型来优化网络，由于我们的数据样本本身就少，那么想要追求高准确率，需要基于大数据集图片分类的预先训练好的网络，例如ImageNet。由于ImageNet的数据量无比之大（1.4 million 已经标注的图片，1000个不同的类别），那么我们用ImageNet预先训练好的网络来对我们的猫狗进行提取特征，可见这个预训练网络可以为大多数不同的计算机视觉分类的问题提供很大的帮助。ImageNet中包含了许多的动物类别，包含不同品种的猫和狗，因此我们期望很够很好的提升猫狗大战的准确率。<br>
我们选用VGG16的结构，得益于它在ImageNet中良好的表现，VGG16不仅简单而且好用，不需要引入其他的概念，还有一些其他的模型，它们都有一些很优雅的名字-VGG，ResNet， Inception-ResNet，Xception等等。</p>
<ul>
<li>
<h5>预训练网络使用方式</h5>
</li>
</ul>
<p>特征提取 and 微调</p>
<ul>
<li>
<h5>特征提取</h5>
</li>
</ul>
<p>特征提取是用一个之前的已经训练好的网络结构，利用这些已经训练好的参数来对于新的样本提取有趣的特征。之后，将这些提取出的特征送入一个新的分类器。我们将卷积层的模型称之为基线卷积，这一部分是已经有训练好的，算是被冻结了，不需要修改的，我们需要做的就是定义全连接层，定义新的分类器。</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 421px; max-height: 289px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 68.65%;\"></div>
<div class=\"image-view\" data-width=\"421\" data-height=\"289\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2145769-d1a665748c076f40.png\" data-original-width=\"421\" data-original-height=\"289\" data-original-format=\"image/png\" data-original-filesize=\"15896\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<ul>
<li>
<h5>在优化模型的第二版本中，采取的优化策略是：不增加数据量 + 预训练网络，那我们试一试。</h5>
</li>
<li>
<h5>采用VGG16作为基线卷积</h5>
</li>
</ul>
<pre><code>from keras.applications import VGG16
conv_base = VGG16(weights=\'imagenet\',
                  include_top=False,    # 是否包括全连接分类器，显然在ImageNet中有上千分类，在我们这里是不需要的
                  input_shape=(150, 150, 3))
</code></pre>
<ul>
<li>
<h5>从预训练的基线卷积层中提取特征</h5>
</li>
</ul>
<pre><code>import os
import numpy as np
from keras.preprocessing.image import ImageDataGenerator
base_dir = \'D:\\kaggle\\dogsvscats\\\\cats_and_dogs_small\'
train_dir = os.path.join(base_dir, \'train\')
validation_dir = os.path.join(base_dir, \'validation\')
test_dir = os.path.join(base_dir, \'test\')

datagen = ImageDataGenerator(rescale=1./255)
batch_size = 20


def extarct_features(directory, sample_count):
    features = np.zeros(shape=(sample_count, 4, 4, 512))
    labels = np.zeros(shape=(sample_count))
    generator = datagen.flow_from_directory(
        directory,
        target_size=(150, 150),
        batch_size=batch_size,
        class_mode=\'binary\')

    i = 0
    for inputs_batch, labels_batch in generator:
        features_batch = conv_base.predict(inputs_batch)
        features[i * batch_size : (i + 1) * batch_size] = features_batch
        labels[i * batch_size : (i + 1) * batch_size] = labels_batch
        i += 1
        if i * batch_size &gt;= sample_count:
            break

    return features, labels


train_features, train_labels = extarct_features(train_dir, 2000)
validation_features, validation_labels = extarct_features(validation_dir, 1000)
test_features, test_labels = extarct_features(test_dir, 1000)

train_features = np.reshape(train_features, (2000, 4 * 4 * 512))
validation_features = np.reshape(validation_features, (1000, 4 * 4 * 512))
test_features = np.reshape(test_features, (1000, 4 * 4 * 512))
</code></pre>
<ul>
<li>
<h5>优化版本(2)---网络模型</h5>
</li>
</ul>
<pre><code>from keras.applications import VGG16
import os
import numpy as np
from keras.preprocessing.image import ImageDataGenerator
from keras import models
from keras import layers
from keras import optimizers
import matplotlib.pyplot as plt


conv_base = VGG16(weights=\'imagenet\',
                  include_top=False,
                  input_shape=(150, 150, 3))

base_dir = \'D:\\kaggle\\dogsvscats\\\\cats_and_dogs_small\'
train_dir = os.path.join(base_dir, \'train\')
validation_dir = os.path.join(base_dir, \'validation\')
test_dir = os.path.join(base_dir, \'test\')

datagen = ImageDataGenerator(rescale=1./255)
batch_size = 20


def extarct_features(directory, sample_count):
    features = np.zeros(shape=(sample_count, 4, 4, 512))
    labels = np.zeros(shape=(sample_count))
    generator = datagen.flow_from_directory(
        directory,
        target_size=(150, 150),
        batch_size=batch_size,
        class_mode=\'binary\')

    i = 0
    for inputs_batch, labels_batch in generator:
        features_batch = conv_base.predict(inputs_batch)
        features[i * batch_size : (i + 1) * batch_size] = features_batch
        labels[i * batch_size : (i + 1) * batch_size] = labels_batch
        i += 1
        if i * batch_size &gt;= sample_count:
            break

    return features, labels


train_features, train_labels = extarct_features(train_dir, 2000)
validation_features, validation_labels = extarct_features(validation_dir, 1000)
test_features, test_labels = extarct_features(test_dir, 1000)

train_features = np.reshape(train_features, (2000, 4 * 4 * 512))
validation_features = np.reshape(validation_features, (1000, 4 * 4 * 512))
test_features = np.reshape(test_features, (1000, 4 * 4 * 512))

model = models.Sequential()
model.add(layers.Dense(256, activation=\'relu\', input_dim=4 * 4 * 512))
model.add(layers.Dropout(0.5))
model.add(layers.Dense(1, activation=\'sigmoid\'))

model.compile(optimizer=optimizers.RMSprop(lr=2e-5),
              loss=\'binary_crossentropy\',
              metrics=[\'acc\'])

history = model.fit(train_features, train_labels,
                    epochs=30,
                    batch_size=20,
                    validation_data=(validation_features, validation_labels))


acc = history.history[\'acc\']
val_acc = history.history[\'val_acc\']
loss = history.history[\'loss\']
val_loss = history.history[\'val_loss\']

epochs = range(1, len(acc) + 1)

plt.plot(epochs, acc, \'bo\', label=\'Training acc\')
plt.plot(epochs, val_acc, \'b\', label=\'Validation acc\')
plt.title(\'Training and validation accuracy\')
plt.legend()

plt.figure()

plt.plot(epochs, loss, \'bo\', label=\'Training loss\')
plt.plot(epochs, val_loss, \'b\', label=\'Validation loss\')
plt.title(\'Training and validation loss\')
plt.legend()

plt.show()
</code></pre>
<ul>
<li>
<h5>优化版本(2)---训练结果及分析</h5>
</li>
<li>
<p>训练结果</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 568px; max-height: 450px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 79.23%;\"></div>
<div class=\"image-view\" data-width=\"568\" data-height=\"450\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2145769-7637f3b71950fd40.png\" data-original-width=\"568\" data-original-height=\"450\" data-original-format=\"image/png\" data-original-filesize=\"17566\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 593px; max-height: 433px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 73.02%;\"></div>
<div class=\"image-view\" data-width=\"593\" data-height=\"433\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2145769-ce81d35224e76a94.png\" data-original-width=\"593\" data-original-height=\"433\" data-original-format=\"image/png\" data-original-filesize=\"19357\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
</li>
<li><p>分析<br>
Yes，准确率已经达到90%，要好于之前一贯在小的数据集上做训练。由曲线可以看出仍然出现了过拟合这个问题，在训练集上达到了95%以上啊，尽管我们用了dropout以及相对更大的学习率，还是有过拟合的问题，很可能是因为我们并没有用到增加数据量（data augmentation）这个有效的方法，那我们应该加上这个方法，看看效果怎样，是不是可以提升准确率呢？！</p></li>
</ul>
<h4>【优化版本(3)---预训练网络之特征提取】</h4>
<ul>
<li>
<h5>带有增大数据量的特征提取</h5>
</li>
</ul>
<p>我们所作的改进就是在优化版本（2）中添加了data augentation，去掉了dropout。</p>
<pre><code>train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=40,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode=\'nearest\')


test_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
    directory=train_dir,
    target_size=(150, 150),
    batch_size=32,
    class_mode=\'binary\')

validation_generator = test_datagen.flow_from_directory(
    directory=validation_dir,
    target_size=(150, 150),
    batch_size=32,
    class_mode=\'binary\')

model.compile(loss=\'binary_crossentropy\',
              optimizer=optimizers.RMSprop(lr=2e-5),
              metrics=[\'acc\'])

history = model.fit_generator(
    train_generator,
    steps_per_epoch=100,
    epochs=30,
    validation_data=validation_generator,
    validation_steps=50)
</code></pre>
<ul>
<li>
<h5>优化版本(3)---网络模型</h5>
</li>
</ul>
<pre><code>from keras.applications import VGG16
from keras.preprocessing.image import ImageDataGenerator
from keras import models
from keras import layers
from keras import optimizers
import matplotlib.pyplot as plt

train_dir = r\'D:\\kaggle\\\\dogsvscats\\\\cats_and_dogs_small\\\\train\'
validation_dir = r\'D:\\kaggle\\\\dogsvscats\\\\cats_and_dogs_small\\\\validation\'

conv_base = VGG16(weights=\'imagenet\',
                  include_top=False,
                  input_shape=(150, 150, 3))
conv_base.trainable = False


model = models.Sequential()
model.add(conv_base)
model.add(layers.Flatten())
model.add(layers.Dense(256, activation=\'relu\', input_dim=4 * 4 * 512))
model.add(layers.Dense(1, activation=\'sigmoid\'))

train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=40,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode=\'nearest\')


test_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
    directory=train_dir,
    target_size=(150, 150),
    batch_size=32,
    class_mode=\'binary\')

validation_generator = test_datagen.flow_from_directory(
    directory=validation_dir,
    target_size=(150, 150),
    batch_size=32,
    class_mode=\'binary\')

model.compile(loss=\'binary_crossentropy\',
              optimizer=optimizers.RMSprop(lr=2e-5),
              metrics=[\'acc\'])

history = model.fit_generator(
    train_generator,
    steps_per_epoch=100,
    epochs=30,
    validation_data=validation_generator,
    validation_steps=50)

model.save(\'cats_and_dogs_small_3.h5\')

acc = history.history[\'acc\']
val_acc = history.history[\'val_acc\']
loss = history.history[\'loss\']
val_loss = history.history[\'val_loss\']

epochs = range(1, len(acc) + 1)
plt.plot(epochs, acc, \'bo\', label=\'Training acc\')
plt.plot(epochs, val_acc, \'b\', label=\'Validation acc\')
plt.title(\'Training and validation accuracy\')
plt.legend()

plt.figure()

plt.plot(epochs, loss, \'bo\', label=\'Training loss\')
plt.plot(epochs, val_loss, \'b\', label=\'Validation loss\')
plt.title(\'Training and validation loss\')
plt.legend()

plt.show()

</code></pre>
<ul>
<li>
<h5>优化版本(3)---训练结果及分析</h5>
</li>
<li>
<p>训练结果</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 599px; max-height: 438px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 73.11999999999999%;\"></div>
<div class=\"image-view\" data-width=\"599\" data-height=\"438\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2145769-69854259f2e270b2.png\" data-original-width=\"599\" data-original-height=\"438\" data-original-format=\"image/png\" data-original-filesize=\"20045\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 611px; max-height: 452px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 73.98%;\"></div>
<div class=\"image-view\" data-width=\"611\" data-height=\"452\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2145769-92937f106e27fc44.png\" data-original-width=\"611\" data-original-height=\"452\" data-original-format=\"image/png\" data-original-filesize=\"21326\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
</li>
<li><p>分析<br>
由上图的曲线可以看出，增大数据量很好的并有效的解决了过拟合的问题，准确率稳定在90%以上，由于我的GPU是750 Ti，仅仅迭代30次，就跑了将近18个小时，需要一块高性能的GPU是很提升工作效率的。</p></li>
</ul>
<h4>【优化版本(4)---预训练网络之微调模型】</h4>
<ul>
<li>
<h5>微调（Fine-tuning）</h5>
</li>
</ul>
<p>这也是另一种重用预训练模型的一种方式，微调就是我们解冻之前固定的VGG16模型，进行细微的调整，使模型与我们的问题更相关。<br>
1）在一个已经训练好的基线网络上添加自定义网络；<br>
2）冻结基线网络；<br>
3）训练我们所添加的部分；<br>
4）解冻一些基线网络中的卷积层；<br>
5）将我们所添加的部分与解冻的卷积层相连接；</p>
<ul>
<li>
<h5>优化版本(4)---网络模型</h5>
</li>
</ul>
<p>我们将VGG16中的第5大卷积层解冻，和全连接层一起参与训练，更新参数。同时，加入测试集，并对测试集最后计算结果进行平滑处理。同样，我们增加了数据量来防止过拟合，我们训练试一试。</p>
<pre><code>from keras.applications import VGG16
from keras.preprocessing.image import ImageDataGenerator
from keras import models
from keras import layers
from keras import optimizers
import matplotlib.pyplot as plt

train_dir = r\'D:\\kaggle\\\\dogsvscats\\\\cats_and_dogs_small\\\\train\'
validation_dir = r\'D:\\kaggle\\\\dogsvscats\\\\cats_and_dogs_small\\\\validation\'
test_dir = r\'D:\\kaggle\\\\dogsvscats\\\\cats_and_dogs_small\\\\test\'

conv_base = VGG16(weights=\'imagenet\',
                  include_top=False,
                  input_shape=(150, 150, 3))
set_trainable = False

for layer in conv_base.layers:
    if layer.name == \'block5_conv1\':
        set_trainable = False
    if set_trainable:
        layer.trainable = True
    else:
        layer.trainable = False


model = models.Sequential()
model.add(conv_base)
model.add(layers.Flatten())
model.add(layers.Dense(256, activation=\'relu\', input_dim=4 * 4 * 512))
model.add(layers.Dense(1, activation=\'sigmoid\'))

train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=40,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode=\'nearest\')


test_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
    directory=train_dir,
    target_size=(150, 150),
    batch_size=32,
    class_mode=\'binary\')

validation_generator = test_datagen.flow_from_directory(
    directory=validation_dir,
    target_size=(150, 150),
    batch_size=32,
    class_mode=\'binary\')

model.compile(loss=\'binary_crossentropy\',
              optimizer=optimizers.RMSprop(lr=1e-5),
              metrics=[\'acc\'])

history = model.fit_generator(
    train_generator,
    steps_per_epoch=100,
    epochs=100,
    validation_data=validation_generator,
    validation_steps=50)

model.save(\'cats_and_dogs_small_4.h5\')


def smooth_curve(points, factor=0.8):
    smoothed_points = []
    for point in points:
        if smoothed_points:
            previous = smoothed_points[-1]
            smoothed_points.append(int(previous * factor + points * (1 - factor)))
        else:
            smoothed_points.append(point)


acc = history.history[\'acc\']
val_acc = history.history[\'val_acc\']
loss = history.history[\'loss\']
val_loss = history.history[\'val_loss\']

epochs = range(1, len(acc) + 1)
plt.plot(epochs, smooth_curve(acc), \'bo\', label=\'Smoothed training acc\')
plt.plot(epochs, smooth_curve(val_acc), \'b\', label=\'Smoothed validation acc\')
plt.title(\'Training and validation accuracy\')
plt.legend()

plt.figure()

plt.plot(epochs, smooth_curve(loss), \'bo\', label=\'Smoothed training loss\')
plt.plot(epochs, smooth_curve(val_loss), \'b\', label=\'Smoothed validation loss\')
plt.title(\'Training and validation loss\')
plt.legend()

plt.show()

test_generator = test_datagen.flow_from_directory(
    test_dir,
    target_size=(150, 150),
    batch_size=20,
    class_mode=\'binary\'
)
test_loss, test_acc = model.evaluate_generator(test_generator, steps=50)
print(\'test acc:\', test_acc)

</code></pre>
<ul>
<li>
<h5>优化版本(4)---训练结果与分析</h5>
</li>
<li>训练结果<br>
测试结果能够保持在90%~91%，但是始终没有训练到95%以上，由于训练后期机器出了问题，没法截图给大家了，希望体谅750 Ti~</li>
<li>分析<br>
此次训练时长将近50多个小时，在训练的50多个小时里，由于电脑处于超负荷状态，我只能为训练让路，我看完一本阿根廷作家的小说《沙之书》、一部电影《侏罗纪公园》、以及《DEEP LEARNING with Python》全部看完。<br>
个人觉得Fine-Tuning方法在本实验中起的作用并不大，并且如果没有更大的GPU资源来训练，在本地小机器上是无法训练的，可能更适用于其他场景吧。</li>
</ul>
<h4>【参考文献】</h4>
<ul>
<li>《DEEP LEARNING with Python》<br>
一本很好的有关深度学习的书籍，能让你构建一个正确的、系统的深度学习的思想和体系，值得推荐~</li>
<li>电子版链接--<a href=\"https://pan.baidu.com/s/1vvEoK5yqeG2SgfbVZNF7zg\" target=\"_blank\" rel=\"nofollow\">百度网盘</a>
</li>
</ul>
<h4>【代码】</h4>
<ul>
<li>GitHub地址：<br>
<a href=\"https://github.com/zhangpengpengpeng/kaggle-dogs-vs-cats\" target=\"_blank\" rel=\"nofollow\">https://github.com/zhangpengpengpeng/kaggle-dogs-vs-cats</a>
</li>
</ul>
<h4>【感谢】</h4>
<p>感谢每一个读到这里的朋友，如有疑问，请在下方留言与评论，或者发到我的邮箱，互相学习，互相分享，走过路过，点个赞呗</p>
<ul>
<li>邮箱：<a href=\"mailto:zhangpeng@webprague.com\" target=\"_blank\" rel=\"nofollow\">zhangpeng@webprague.com</a>
</li>
</ul>

          </div>','1531183669'),
('325561','{975757}{17}{126}{131}{54697}{1123983}{503}{36}{6109}{30656}{975786}{975835}{1682}{1751}{975935}{17893}{4572}{1528}{977696}{650}{2430}{59642}{6175}{3153}{247}{1123984}{42313}{42193}{240876}{247189}{1123985}{3833}{30162}{1123986}{2217}{975714}{975899}{408}{975811}{975728}{10185}{1968}{785}{4057}{378}{975761}{975915}{111}{1314}{975789}','充”，2表示”文本起始“， 3表示”未知“，因此当我们从train_data中读到的数值是1，2，3时，我们要忽略它，从4开始才对应单词，如果数值是4， 那么它表示频率出现最高的单词 \'\'\' decoded_review = \' \'.','深度学习项目实践，使用神经网络分析电影评论的正能量与负能量','<div class=\"show-content-free\">
            <p>在前面章节中，我们花费大量精力详细解析了神经网络的内在原理。神经网络由如下4个部分组成：<br>
1，神经层，每层由多个神经元组合而成。<br>
2，输入训练数据，已经数据对应的结果标签<br>
3，设计损失函数，也就是用数学公式来表示，神经网络的输出结果与正确结果之间的差距。<br>
4，优化，通过梯度下降法修改神经元的链路权重，然后使得网络的输出结果与正确结果之间的差距越来越小。</p>
<p>下图就能将网络的各个组件以及我们前面讨论过的内容综合起来：</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 696px; max-height: 510px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 73.28%;\"></div>
<div class=\"image-view\" data-width=\"696\" data-height=\"510\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2849961-879174f510aeb50c.png\" data-original-width=\"696\" data-original-height=\"510\" data-original-format=\"image/png\" data-original-filesize=\"109953\"></div>
</div>
<div class=\"image-caption\">屏幕快照 2018-06-19 上午10.58.04.png</div>
</div>
<p>神经网络的学习过程，其实就是把大量数据输入，让网络输出结果，计算结果与预期结果间的差距，通过梯度下降法修改神经元链路权重，以便降低输出结果与预期结果的差距。这个流程反复进行，数据量越大，该流程进行的次数越多，链路权重的修改就能越精确，于是网络输出的结果与预期结果就越准确。</p>
<p>本节我们将神经网络技术直接运用到具体的项目实践上，我们用神经网络来判断用户在网络上编写的影评中包含的是正能量还是负能量，如果对电影正能量的影评越多，电影的票房便会越好，负能量影评越多，电影票房可能就会暗淡。在项目中我们不再像以前一样从零创建一个网络，而是直接使用keras框架快速的搭建学习网络。</p>
<p>首先要做的是将影评数据下载到本地，我们使用的是国外著名电影评论网站IDMB的数据，数据有60000条影评，其中一半包含正能量，另一半包含负能量，这些数据将用于训练我们的网络，数据的下载代码如下：</p>
<pre><code>from keras.datasets import imdb
(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)
</code></pre>
<p>数据流不小，下载可能需要几十秒，运行上面代码后，结果如下：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 165px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 14.469999999999999%;\"></div>
<div class=\"image-view\" data-width=\"1140\" data-height=\"165\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2849961-206763eacf621223.png\" data-original-width=\"1140\" data-original-height=\"165\" data-original-format=\"image/png\" data-original-filesize=\"41027\"></div>
</div>
<div class=\"image-caption\">屏幕快照 2018-06-19 上午11.04.58.png</div>
</div>
<p>数据中的评论是用英语拟写的文本，我们需要对数据进行预处理，把文<br>
本变成数据结构后才能提交给网络进行分析。我们当前下载的数据条目中，包含的已经不是原来的英文，而是对应每个英语单词在所有文本中的出现频率，我们加载数据时，num_words=10000，表示数据只加载那些出现频率排在前一万位的单词。我们看看数据内容：</p>
<pre><code>print(train_data[0])
print(train_labels[0])
</code></pre>
<p>上面带运行后结果如下：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 291px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 28.470000000000002%;\"></div>
<div class=\"image-view\" data-width=\"1022\" data-height=\"291\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2849961-5a4548263db0a80c.png\" data-original-width=\"1022\" data-original-height=\"291\" data-original-format=\"image/png\" data-original-filesize=\"71294\"></div>
</div>
<div class=\"image-caption\">屏幕快照 2018-06-19 上午11.10.42.png</div>
</div>
<p>train_data的一个元素的值是1，它对应的是频率出现排在第一位的单词，假设频率出现最高的单词是\"is\"，那么train_data第1个元素对应的单词就是\"is\"，以此类推。train_lables用来存储对应影评是正能量还是负能量，1表示正能量，0表示负能量。</p>
<p>接下来我们尝试根据train_data中给定的单词频率，把单词还原回来。频率与单词的对应存储在imdb.get_word_index()返回的哈希表中，通过查询该表，我们就能将频率转换成对应的单词，代码如下：</p>
<pre><code>#频率与单词的对应关系存储在哈希表word_index中,它的key对应的是单词，value对应的是单词的频率
word_index = imdb.get_word_index()
#我们要把表中的对应关系反转一下，变成key是频率，value是单词
reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])
        
\'\'\'
在train_data所包含的数值中，数值1，2，3对应的不是单词，而用来表示特殊含义，1表示“填充”，2表示”文本起始“，
3表示”未知“，因此当我们从train_data中读到的数值是1，2，3时，我们要忽略它，从4开始才对应单词，如果数值是4，
那么它表示频率出现最高的单词
\'\'\'
decoded_review = \' \'.join([reverse_word_index.get(i-3, \'?\') for i in train_data[0]])
print(decoded_review)
</code></pre>
<p>上面这段代码运行后，我们把条目一的影评还原如下：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 501px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 49.17%;\"></div>
<div class=\"image-view\" data-width=\"1019\" data-height=\"501\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2849961-2a2163aeec648d63.png\" data-original-width=\"1019\" data-original-height=\"501\" data-original-format=\"image/png\" data-original-filesize=\"200804\"></div>
</div>
<div class=\"image-caption\">屏幕快照 2018-06-19 上午11.32.26.png</div>
</div>
<p>对英语不好的同学可能会吃力点，但没办法，这些标记好的训练数据只有英语才有，中文没有相应的训练数据，从这点看可以明白，为何西方人的科技会领先中国，他们有很多人愿意做一些为他人铺路的工作。</p>
<p>在把数据输入网络前，我们需要对数据进行某种格式化，数据的格式对网络的分析准确性具有很大的影响。在深度学习中，有一种常用的数据格式叫:one-hot-vector，它的形式是这样的，假设集合中总共有10个整数{0,1,2,3,4,5,6,7,8,9}，我们选取一个子集,它包含所有偶数，也就是{0,2,4,6,8}，那么对应的one-hot-vector就是一个含有10个元素的向量，如果某个元素出现在子集中，我们就把向量中对应的元素设置为1，没有出现则设置为0，于是对应子集的向量就是：[1, 0, 1, 0, 1, 0, 1, 0, 1, 0]。</p>
<p>由于文本中只包含10000个单词，于是我们设置一个长度为一万的向量，当某个频率的词出现在文章中时，我们就把向量相应位置的元素设置成1，代码如下：</p>
<pre><code>import numpy as np

def vectorize_sequences(sequences, dimension=10000):
    \'\'\'
    sequences 是包含所有评论的序列，一条评论对应到一个长度为10000的数组，因此我们要构建一个二维矩阵，
    矩阵的行对应于评论数量，列对应于长度为10000
    \'\'\'
    results = np.zeros((len(sequences),dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.0
    return results

x_train = vectorize_sequences(train_data)
x_test = vectorize_sequences(test_data)

print(x_train[0])

y_train = np.asarray(train_labels).astype(\'float32\')
y_test = np.asarray(test_labels).astype(\'float32\')
</code></pre>
<p>经过上面的格式化后，网络的输入数据就是简单的向量，训练数据对应的结果就是简单的0或1。接下来我们要构建一个三层网络，代码如下：</p>
<pre><code>from keras import models
from keras import layers

model = models.Sequential()
#构建第一层和第二层网络，第一层有10000个节点，第二层有16个节点
#Dense的意思是，第一层每个节点都与第二层的所有节点相连接
#relu 对应的函数是relu(x) = max(0, x)
model.add(layers.Dense(16, activation=\'relu\', input_shape=(10000,)))
#第三层有16个神经元，第二层每个节点与第三层每个节点都相互连接
model.add(layers.Dense(16, activation=\'relu\'))
#第四层只有一个节点，输出一个0-1之间的概率值
model.add(layers.Dense(1, activation=\'sigmoid\'))
</code></pre>
<p>上面代码构造了一个四层网络，第一层含有10000个神经元，用于接收长度为10000的文本向量，第二第三层都含有16个神经元，最后一层只含有一个神经元，它输出一个概率值，用于标记文本含有正能量的可能性。</p>
<p>接下来我们设置损失函数和设置链路参数的调教方式，代码如下：</p>
<pre><code>from keras import losses
from keras import metrics
from keras import optimizers

model.compile(optimizer=optimizers.RMSprop(lr=0.001), loss=\'binary_crossentropy\', metrics=[\'accuracy\'])
</code></pre>
<p>optimizer参数指定的是如何优化链路权重，事实上各种优化方法跟我们前面讲的梯度下降法差不多，只不过在存在一些微小的变化，特别是在更新链路权值时，会做一些改动，但算法主体还是梯度下降法。当我们的网络用来将数据区分成两种类型时，损失函数最好使用binary_crossentroy,它的表达式如下：</p>
<p>Hy′(y):=∑i(y′ilog(y[i])+(1y′[i])log(1y[i]))</p>
<p>其中y[i]对应的是训练数据提供的结果，y\'[i]是我们网络计算所得的结果。metrics用于记录网络的改进效率，我们暂时用不上。接着我们把训练数据分成两部分，一部分用于训练网络，一部分用于检验网络的改进情况：</p>
<pre><code>x_val = x_train[:10000]
partial_x_train = x_train[10000:]

y_val = y_train[: 10000]
partial_y_train = y_train[10000:]

history = model.fit(partial_x_train, partial_y_train, epochs=20, batch_size=512, 
                    validation_data = (x_val, y_val))

</code></pre>
<p>训练数据总共有60000条，我们把最前一万条作为校验数据，用来检测网络是否优化到合适的程度，然后我们把数据从第一万条开始作为训练网络来用，把数据分割好后，调用fit函数就可以开始训练过程，上面代码运行后结果如下：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 382px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 30.490000000000002%;\"></div>
<div class=\"image-view\" data-width=\"1253\" data-height=\"382\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2849961-0bff9a4ab4232a6c.png\" data-original-width=\"1253\" data-original-height=\"382\" data-original-format=\"image/png\" data-original-filesize=\"81590\"></div>
</div>
<div class=\"image-caption\">屏幕快照 2018-06-19 下午5.38.26.png</div>
</div>
<p>我把代码运行结果最后部分信息截取出来。我们看到loss的值越来越小，这意味着网络越来越能准确的识别训练数据的类型，但是校验数据的识别准确度却越来越低，也就是我们的模型只试用与训练数据，不适用与校验数据，这意味着我们的训练过程有问题。fit函数返回的history对象记录了训练过程中，网络的相关数据，通过分析这些数据，我们可以了解网络是如何改进自身的，它是一个哈希表，记录了四种内容：</p>
<pre><code>history_dict = history.history
print(history_dict.keys())
</code></pre>
<p>上面输出结果如下：<br>
dict_keys([\'val_loss\', \'val_acc\', \'loss\', \'acc\'])<br>
它记录了网络对训练数据识别的精确度和对校验数据识别的精确度。我们把数据画出来看看：</p>
<pre><code>import matplotlib.pyplot as plt

acc = history.history[\'acc\']
val_acc = history.history[\'val_acc\']
loss = history.history[\'loss\']
val_loss = history.history[\'val_loss\']

epochs = range(1, len(acc) + 1)
#绘制训练数据识别准确度曲线
plt.plot(epochs, loss, \'bo\', label=\'Trainning loss\')
#绘制校验数据识别的准确度曲线
plt.plot(epochs, val_loss, \'b\', label=\'Validation loss\')
plt.title(\'Trainning and validation loss\')
plt.xlabel(\'Epochs\')
plt.ylabel(\'Loss\')
plt.legend()
plt.show()
</code></pre>
<p>上面代码运行后结果如下：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 523px; max-height: 352px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 67.30000000000001%;\"></div>
<div class=\"image-view\" data-width=\"523\" data-height=\"352\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2849961-d33d33debb851ed7.png\" data-original-width=\"523\" data-original-height=\"352\" data-original-format=\"image/png\" data-original-filesize=\"37648\"></div>
</div>
<div class=\"image-caption\">屏幕快照 2018-06-19 下午5.48.15.png</div>
</div>
<p>我们看上面图示能发现一个问题，随着迭代次数的增加，网络对训练数据识别的准确度越来越高，也就是loss越来越低，然后校验数据的识别准确的却越来越低，这种现象叫“过度拟合”，这意味着训练的次数并不是越多越好，而是会“过犹不及”，有时候训练迭代次数多了反而导致效果下降。从上图我们看到，大概在第4个epoch的时候，校验数据的识别错误率开始上升，因此我们将前面的代码修改，把参数epochs修改成4才能达到最佳效果。</p>
<p>训练好网络后，我们就可以用它来识别新数据，我们把测试数据放入网络进行识别，代码如下：</p>
<pre><code>model = models.Sequential()
model.add(layers.Dense(16, activation=\'relu\', input_shape=(10000,)))
model.add(layers.Dense(16, activation=\'relu\'))
model.add(layers.Dense(1, activation=\'sigmoid\'))

model.compile(optimizer=\'rmsprop\',
              loss=\'binary_crossentropy\',
              metrics=[\'accuracy\'])

model.fit(x_train, y_train, epochs=4, batch_size=512)
results = model.evaluate(x_test, y_test)
</code></pre>
<p>上面代码运行后结果如下：</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 558px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 39.94%;\"></div>
<div class=\"image-view\" data-width=\"1397\" data-height=\"558\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2849961-5824e4193b314b0c\" data-original-width=\"1397\" data-original-height=\"558\" data-original-format=\"image/png\" data-original-filesize=\"103061\"></div>
</div>
<div class=\"image-caption\">这里写图片描述</div>
</div>
<p>results有两个值，第二个表示的是判断的准确度，从结果我们可以看到，网络经过训练后，对新的影评文本，其对其中正能量和负能量的判断准确率达到88%。对于专业的项目实施，经过巧妙的调优，各种参数的调整，多增加网络的层次等方法，准确率可以达到95%左右。接着我们看看网络对每一条测试文本得出它是正能量的几率：</p>
<pre><code>model.predict(x_test)
</code></pre>
<p>代码运行后结果如下：</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 430px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 29.74%;\"></div>
<div class=\"image-view\" data-width=\"1446\" data-height=\"430\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2849961-0d8b20bf66d45862\" data-original-width=\"1446\" data-original-height=\"430\" data-original-format=\"image/png\" data-original-filesize=\"58273\"></div>
</div>
<div class=\"image-caption\">这里写图片描述</div>
</div>
<p>网络对每一篇影评给出了其是正能量的可能性。</p>
<p>我们的网络还有不少可以尝试改进的地方，例如在中间多增加两层神经元，改变中间层神经元的数量，将16改成32，64等，尝试使用其他损失函数等。</p>
<p>从整个项目流程我们看到，一个神经网络在实践中的应用需要完成以下几个步骤：<br>
1，将原始数据进行加工，使其变成数据向量以便输入网络。<br>
2，根据问题的性质选用不同的损失函数和激活函数，如果网络的目标是将数据区分成两类，那么损失函数最好选择binary_crossentropy，输出层的神经元如果选用sigmoid激活函数，那么它会给出数据属于哪一种类型的概率。<br>
3，选取适当的优化函数，几乎所有的优化函数都以梯度下降法为主，只不过在更新链路权重时，有些许变化。<br>
4，网络的训练不是越多越好，它容易产生“过度拟合”的问题，导致训练的越多，最终效果就越差，所以在训练时要密切关注网络对检验数据的判断准确率。</p>
<p><a href=\"http://study.163.com/provider-search?keyword=Coding%E8%BF%AA%E6%96%AF%E5%B0%BC\" target=\"_blank\" rel=\"nofollow\">更详细的讲解和代码调试演示过程，请点击链接</a></p>
<p>更多技术信息，包括操作系统，编译器，面试算法，机器学习，人工智能，请关照我的公众号：</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 258px; max-height: 258px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 100.0%;\"></div>
<div class=\"image-view\" data-width=\"258\" data-height=\"258\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2849961-d584a529f20a04da\" data-original-width=\"258\" data-original-height=\"258\" data-original-format=\"\" data-original-filesize=\"27260\"></div>
</div>
<div class=\"image-caption\">这里写图片描述</div>
</div>

          </div>','1531183671'),
('325562','{1204}{975714}{1215}{975878}{1218}{975744}{1145}{408}{884}{4883}{975951}{975703}{1124009}{376}{84048}{7187}{3889}{975768}{440}{785}{400}{621}{975761}{1551}{976193}{1188}{380}{872}{6090}{975757}{975738}{4153}{503}{1124011}{31528}{19593}{1785}{4321}{187326}{5559}{1124012}{1124013}{189}{84}{727}{255}{1848}{7341}{387}{975830}','T * os.K[:, k] + os.b) Ek = fxk - float(os.labels[k]) return Ek def updateEk(os,k): Ek = calculateEi(os,k) os.eCache[k]=[1,Ek] 刚刚那个执行函数其实已经包括了kernel的，所以直接就可以看到效果了： 用的是Gaussion kernel，不知道怎么做拟合，就把支持向量点圈出来就好了。 最后附上所有代码GitHub： https://github.com/GreenArrow2017/MachineLearning','支持向量机（Support Vector Machine）','<div class=\"show-content-free\">
            <h1>支持向量机</h1>
<p>linear regression ， perceptron learning algorithm ， logistics regression都是分类器，我们可以使用这些分类器做线性和非线性的分类，比如下面的一个问题：</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 402px; max-height: 307px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 76.37%;\"></div>
<div class=\"image-view\" data-width=\"402\" data-height=\"307\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-f889f30cb53b684a.png\" data-original-width=\"402\" data-original-height=\"307\" data-original-format=\"image/png\" data-original-filesize=\"13926\"></div>
</div>
<div class=\"image-caption\">GV0SHYC3S{P{Q4QVB66UN6T.png</div>
</div>
<br>
<p>这里的每一条线都是可以把这个平面分开的，支持向量机要做的就是要在这些可以选择的直线中选择一条最好的直线来作为分类的直线。再给一个简单的解释，比如下面的三个图片，圆圈区域越大，说明这条直线对这些点放错的容忍度就越高：</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 306px; max-height: 147px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 48.04%;\"></div>
<div class=\"image-view\" data-width=\"306\" data-height=\"147\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-26ba65705a08c78b.png\" data-original-width=\"306\" data-original-height=\"147\" data-original-format=\"image/png\" data-original-filesize=\"10090\"></div>
</div>
<div class=\"image-caption\">0T0FN3C5MKTQE`DO{N}QX(C.png</div>
</div>
<h2>①超平面</h2>
<p>介绍SVM之前，先来看<strong>超平面</strong>的概念：<br>
其实超平面就是用于分割当前维度的一个空间。比如一维可以用一个点来进行分割，二维用一条线来进行分割，那么这些点和线就叫做“超”平面。加双引号是因为他们其实并不是正在的超平面，因为超平面是要求大于三维的。<br>
所以四维空间里面的超平面就是三维。比如在二维的空间里的超平面就是一条直线：<br>
</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 196px; max-height: 33px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 16.84%;\"></div>
<div class=\"image-view\" data-width=\"196\" data-height=\"33\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-4573a91b0e7687d5.png\" data-original-width=\"196\" data-original-height=\"33\" data-original-format=\"image/png\" data-original-filesize=\"2579\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p></p>
<p>三维里面的超平面：</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 265px; max-height: 35px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 13.209999999999999%;\"></div>
<div class=\"image-view\" data-width=\"265\" data-height=\"35\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-e81013706a78c0ff.png\" data-original-width=\"265\" data-original-height=\"35\" data-original-format=\"image/png\" data-original-filesize=\"3366\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>(其实这里的应该都不能叫超平面，因为超平面是三维以及三维以上的)<br>
我们把a ， b ， c看做是W0 , W1 , W2...，把x , y , z看做是x1 , x2 , x3，那么就有：</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 181px; max-height: 36px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 19.89%;\"></div>
<div class=\"image-view\" data-width=\"181\" data-height=\"36\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-c56b947b3f6a43f3.png\" data-original-width=\"181\" data-original-height=\"36\" data-original-format=\"image/png\" data-original-filesize=\"2550\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
<p>而W向量就是这个平面的法向量，我们要求的就是法向量，证明一下：<br>
以二维为例：</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 221px; max-height: 36px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 16.29%;\"></div>
<div class=\"image-view\" data-width=\"221\" data-height=\"36\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-b18b6cfaf1f222b9.png\" data-original-width=\"221\" data-original-height=\"36\" data-original-format=\"image/png\" data-original-filesize=\"3043\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 217px; max-height: 35px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 16.13%;\"></div>
<div class=\"image-view\" data-width=\"217\" data-height=\"35\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-abdcc67b5978793e.png\" data-original-width=\"217\" data-original-height=\"35\" data-original-format=\"image/png\" data-original-filesize=\"2818\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
相减得到：<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 341px; max-height: 41px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 12.02%;\"></div>
<div class=\"image-view\" data-width=\"341\" data-height=\"41\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-9f9be9d6351d0521.png\" data-original-width=\"341\" data-original-height=\"41\" data-original-format=\"image/png\" data-original-filesize=\"4742\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
<p>而[ (X_0 - X_1) , (Y_0 , Y_1)]这个点就在这个平面上，所以得到了：</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 140px; max-height: 40px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 28.57%;\"></div>
<div class=\"image-view\" data-width=\"140\" data-height=\"40\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-dd2f604f7eb8fd5b.png\" data-original-width=\"140\" data-original-height=\"40\" data-original-format=\"image/png\" data-original-filesize=\"1961\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
所以W就是平面的法向量，这上面的X代表的是这个平面而不是点。
<h2>②函数间隔的最大化</h2>
<p>刚刚说到支持向量机也不是找超平面了，而是找最好的超平面，也就是对于点的犯错的容忍度越大越好，其实就是函数间隔越大越好：</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 255px; max-height: 127px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 49.8%;\"></div>
<div class=\"image-view\" data-width=\"255\" data-height=\"127\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-4421f47ca4398cb0.png\" data-original-width=\"255\" data-original-height=\"127\" data-original-format=\"image/png\" data-original-filesize=\"4645\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
<p>右边的明显要好过左边的，因为左边的可犯错空间大啊。所以我们要寻找的就是最大最肥的超平面——hperplane。</p>

<p>函数的间隔最大，<strong>其实就是距离直线距离最短的点离直线的距离要最大。</strong>所以先要知道直线的距离怎么求：<br>
首先我们假设X0在平面上的投影是X1，则肯定有法向量W垂直于X0X1,：<br>
</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 389px; max-height: 57px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 14.649999999999999%;\"></div>
<div class=\"image-view\" data-width=\"389\" data-height=\"57\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-76f69840c94ba31a.png\" data-original-width=\"389\" data-original-height=\"57\" data-original-format=\"image/png\" data-original-filesize=\"4150\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 102px; max-height: 38px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 37.25%;\"></div>
<div class=\"image-view\" data-width=\"102\" data-height=\"38\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-d6a51b24df223aea.png\" data-original-width=\"102\" data-original-height=\"38\" data-original-format=\"image/png\" data-original-filesize=\"1560\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
又因为：<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 153px; max-height: 52px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 33.989999999999995%;\"></div>
<div class=\"image-view\" data-width=\"153\" data-height=\"52\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-b75d2ba0eb4cf083.png\" data-original-width=\"153\" data-original-height=\"52\" data-original-format=\"image/png\" data-original-filesize=\"2319\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 695px; max-height: 46px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 6.619999999999999%;\"></div>
<div class=\"image-view\" data-width=\"695\" data-height=\"46\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-45137c9172cc1455.png\" data-original-width=\"695\" data-original-height=\"46\" data-original-format=\"image/png\" data-original-filesize=\"8443\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 41px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 5.17%;\"></div>
<div class=\"image-view\" data-width=\"793\" data-height=\"41\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-f21108016eb1d5f7.png\" data-original-width=\"793\" data-original-height=\"41\" data-original-format=\"image/png\" data-original-filesize=\"8307\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
右因为X1在平面上的，前半部分就是-b了，可以写成：<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 155px; max-height: 38px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 24.52%;\"></div>
<div class=\"image-view\" data-width=\"155\" data-height=\"38\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-32a507371def6c60.png\" data-original-width=\"155\" data-original-height=\"38\" data-original-format=\"image/png\" data-original-filesize=\"2288\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
和上面那条式子相等就得到：<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 187px; max-height: 58px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 31.019999999999996%;\"></div>
<div class=\"image-view\" data-width=\"187\" data-height=\"58\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-1a8b454f205f3b45.png\" data-original-width=\"187\" data-original-height=\"58\" data-original-format=\"image/png\" data-original-filesize=\"7117\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
这就是我们要求的点到直线的距离了。而如果这个hperplane是正确的话，那么所有点的分类都是对的，那么我们就默认他是对的，于是有：<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 289px; max-height: 53px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 18.34%;\"></div>
<div class=\"image-view\" data-width=\"289\" data-height=\"53\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-501885a93c839bba.png\" data-original-width=\"289\" data-original-height=\"53\" data-original-format=\"image/png\" data-original-filesize=\"4967\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
这里可以相乘的条件是，我们默认label正确的是1错误的是-1，如果你的错误是0正确是1的话公式是不同的。乘上一个Y首先是可以去掉绝对值，使得函数变得可微，另外乘上之后函数值的绝对值也不会有变化，使得求解更加方便。<br>
所以，最后的我们的优化目标就是这样了：<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 503px; max-height: 114px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 22.66%;\"></div>
<div class=\"image-view\" data-width=\"503\" data-height=\"114\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-ac5ba85d9c643c02.png\" data-original-width=\"503\" data-original-height=\"114\" data-original-format=\"image/png\" data-original-filesize=\"36145\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
<strong>里面的minimize是指找到距离hperplane最小距离的点，最外面就是挑选一个最好的W,b使得这个距离最小的点距离hperplane是最大的。</strong><p></p>
<h2>③目标函数的化简</h2>
<p>对于上面的式子，注意看到里面的那个式子：<br>
</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 152px; max-height: 32px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 21.05%;\"></div>
<div class=\"image-view\" data-width=\"152\" data-height=\"32\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-8fa705f4e5f1e2cc.png\" data-original-width=\"152\" data-original-height=\"32\" data-original-format=\"image/png\" data-original-filesize=\"2705\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
举一个例子：<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 207px; max-height: 35px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 16.91%;\"></div>
<div class=\"image-view\" data-width=\"207\" data-height=\"35\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-a97b2db13af0a05d.png\" data-original-width=\"207\" data-original-height=\"35\" data-original-format=\"image/png\" data-original-filesize=\"2432\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
我们代入(4,3)这个点，得到19，似乎这个数字太大了，我们不想要他这么大，我们两边同时除去19，这个时候我们的超平面就变成了：<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 252px; max-height: 44px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 17.46%;\"></div>
<div class=\"image-view\" data-width=\"252\" data-height=\"44\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-4ab3576d0a1f0034.png\" data-original-width=\"252\" data-original-height=\"44\" data-original-format=\"image/png\" data-original-filesize=\"2820\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
数字是边了，但是这个超平面还是在这个位置，所以可以认为超平面是没有变化的，这就证明了我们上面那个式子：<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 152px; max-height: 32px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 21.05%;\"></div>
<div class=\"image-view\" data-width=\"152\" data-height=\"32\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-8fa705f4e5f1e2cc.png\" data-original-width=\"152\" data-original-height=\"32\" data-original-format=\"image/png\" data-original-filesize=\"2705\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
是可以通过对w,b进行放缩而把左边的结果放大的无限多倍的。既然这样，那这个东西留着有什么意义，直接放缩到1就可以了，于是我们把他放缩到1，也就是最小值是1。其实等于1都是差不多的，因为最小值之后都是1，于是就是有了：<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 144px; max-height: 35px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 24.310000000000002%;\"></div>
<div class=\"image-view\" data-width=\"144\" data-height=\"35\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-1e6cb793a9fe7706.png\" data-original-width=\"144\" data-original-height=\"35\" data-original-format=\"image/png\" data-original-filesize=\"4412\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
那么target fomula就可以变成这样：<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 386px; max-height: 55px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 14.249999999999998%;\"></div>
<div class=\"image-view\" data-width=\"386\" data-height=\"55\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-844f62903f269abe.png\" data-original-width=\"386\" data-original-height=\"55\" data-original-format=\"image/png\" data-original-filesize=\"10799\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
放缩之后并不是就完了，这个是要加入当做条件来使用的。对于这个：<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 45px; max-height: 48px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 106.67%;\"></div>
<div class=\"image-view\" data-width=\"45\" data-height=\"48\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-d1ec2c431a554408.png\" data-original-width=\"45\" data-original-height=\"48\" data-original-format=\"image/png\" data-original-filesize=\"770\"></div>
</div>
<div class=\"image-caption\"></div>
</div>事实上我们不太喜欢化简这样的，找最大化的不就是找最小化的W吗？找最小化的W不就是找最小化的W*W^T吗？不如再加个1/2?<br>
所以问题就变成了：<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 142px; max-height: 41px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 28.87%;\"></div>
<div class=\"image-view\" data-width=\"142\" data-height=\"41\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-69f7f17304dd6a0a.png\" data-original-width=\"142\" data-original-height=\"41\" data-original-format=\"image/png\" data-original-filesize=\"2286\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
<strong>为什么要加上1/2呢？其实是为了后面求导的时候方便化简的，但是这样对结果又没有什么影响。而W变成平方其实就是用上凸优化，因为平方之后就个凸函数了，这样变换同样对于最优结果没有任何影响。</strong><br>
所以最后要优化的结果：<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 332px; max-height: 78px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 23.49%;\"></div>
<div class=\"image-view\" data-width=\"332\" data-height=\"78\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-5eeda9dfeb9ae305.png\" data-original-width=\"332\" data-original-height=\"78\" data-original-format=\"image/png\" data-original-filesize=\"8007\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p></p>
<h2>④Dual problem and KKT condiction</h2>
<p>对于上述有条件的最优化问题，自然就要用上lagrange乘子法了。</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 405px; max-height: 58px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 14.32%;\"></div>
<div class=\"image-view\" data-width=\"405\" data-height=\"58\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-abb4381968e7fd99.png\" data-original-width=\"405\" data-original-height=\"58\" data-original-format=\"image/png\" data-original-filesize=\"13749\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
<p>右边的约束条件是要小于等于0的，α ≥ 0，只不过前面是符号所以转一下而已。<br>
到这里，其实只要把等式扔到Quadratic Programming里面直接计算就好了。下面的步骤其实都是围绕解决这个优化问题展开的。</p>

<hr>
<hr>
<p>先在这停顿一下，我们考虑一下：</p>
<blockquote>
<blockquote>
<h4>⑴SVM的机器学习可行性问题：</h4>
<p>首先先来观察一下这个式子，感觉似曾相识。<strong>他和L2 regularization很像，对于L2 regularization，首先是先要计算Ein的最小值，所谓的Ein就是该模型在当前的训练数据上犯错误的期望值。然后再正则化，所以L2是Minimizing Ein and Regularized L2 Paradigms；而支持向量机正好相反，他是先假设我这个平面是分类正确的，然后minimize W方</strong>：<br>
</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 404px; max-height: 79px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 19.55%;\"></div>
<div class=\"image-view\" data-width=\"404\" data-height=\"79\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-58e6febdee21513c.png\" data-original-width=\"404\" data-original-height=\"79\" data-original-format=\"image/png\" data-original-filesize=\"11491\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
后面我们会用这个结论的。<br>
回顾一下机器学习要解决的问题：<br>
<strong>①Ein ≈ Eout</strong><br>
Ein刚刚解释过了，Eout就是这个model在全局所犯的错误，Ein ≈ Eout就是要求这个model是可以反映全局的，如果不能反映，那就是过拟合了。<br>
<strong>②Ein ≈ 0</strong><br>
这个就是训练错误要接近于0，在这一步就容易发生过拟合的现象了。<br>
<strong>而Ein ≈ Eout，也就是泛化能力是被VC dimension限制的，也就是说，越复杂的模型他的VC dimension越复杂。也就是VC bound右边的Ω会很大，VC bound就会很大，导致Ein 远远小于Eout了，因为复杂的模型意味着更加小的Ein。</strong><br>
再提一下，VC dimension就是break point - 1得到的。<br>
如果是这样的话，那么正常来说SVM的VC dimension也会很大啊，因为他的W是和数据维度相关的，数据多少维，那么W就多少个，而W代表的是自由度，通常也就代表这VC dimension，<strong>但是SVM的效果还是很好，为什么呢？是什么东西限制着SVM的VC dimension？</strong><br>
我们来看一个例子：<br>
在一个圆上，有三个点，你想找到一条可以分开的直线，可以得到VC dimension是3（之前有同学看到在一个圆上分类他的VC dimension是无限的的，这是因为有无数多个点给你玩，这里就三个点，无限你又用不了，所以就只能是三个了啦），但是如果加上限制条件，这条线宽是5，那么VC dimension就是0了，因为没有地方塞进去。所以如果是large margin，VC dimension ≤ 3的。如图：<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 546px; max-height: 163px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 29.849999999999998%;\"></div>
<div class=\"image-view\" data-width=\"546\" data-height=\"163\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-16a57b1c9269e835.png\" data-original-width=\"546\" data-original-height=\"163\" data-original-format=\"image/png\" data-original-filesize=\"29695\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 379px; max-height: 101px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 26.650000000000002%;\"></div>
<div class=\"image-view\" data-width=\"379\" data-height=\"101\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-83fc1692fe5c05d3.png\" data-original-width=\"379\" data-original-height=\"101\" data-original-format=\"image/png\" data-original-filesize=\"16357\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
所以，large margin就是SVM的VCdimension的限制条件，导致它的分类效果很好，VC dimension小了自然泛化能力就好了，这里就解决了Ein ≈ Eout的问题，Ein ≈ 0这就是我们后面要用凸优化来解决的问题了。<p></p>
</blockquote>
</blockquote>
<hr>
<hr>
<p>回到正题：<br>
</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 405px; max-height: 58px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 14.32%;\"></div>
<div class=\"image-view\" data-width=\"405\" data-height=\"58\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-abb4381968e7fd99.png\" data-original-width=\"405\" data-original-height=\"58\" data-original-format=\"image/png\" data-original-filesize=\"13749\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
如何优化我们的target function？<br>
上面的讨论我们已经得到了VC dimension ≤ d + 1，<strong>我们悲观估计一下，就算SVM的VC dimension是d + 1了，加上d就是数据维度，加上的1是偏置值b。那么如果数据维度很大的话，计算复杂度是很高的，另外，现在我们所研究的SVM还是linear separable的，如果是来个nonlinear transform，数据维度就更加大了，再加上一般数据数量都是很的，时间会很长。</strong>所以我们要想一个方法来把数据维度d转移到其他地方去或者之间丢了。<br>
而Daul problem恰好就可以解决。<br>
回到origin target function：<br>
<strong>我们需要最小化：</strong><br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 299px; max-height: 111px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 37.12%;\"></div>
<div class=\"image-view\" data-width=\"299\" data-height=\"111\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-c48abf8b94071d14.png\" data-original-width=\"299\" data-original-height=\"111\" data-original-format=\"image/png\" data-original-filesize=\"11639\"></div>
</div>
<div class=\"image-caption\">最小化</div>
</div>
<br>
对原函数做一些变换：<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 581px; max-height: 138px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 23.75%;\"></div>
<div class=\"image-view\" data-width=\"581\" data-height=\"138\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-033a6812be79097a.png\" data-original-width=\"581\" data-original-height=\"138\" data-original-format=\"image/png\" data-original-filesize=\"33652\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
<strong>当这个点是违反了条件的时候，那么约束条件就会 &gt; 0，α &gt; 0，再maximum α那么就是无限大了，这个时候就不可能是最小值，不可能取他，因为是无限大了。</strong><br>
<strong>当这个点是不违反的时候，那么约束条件就会 &lt; 0，α &gt; 0，再maximum α约束条件就是0，minimize w，b之后就还是原来的target function。</strong><br>
所以变换之后：<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 313px; max-height: 39px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 12.46%;\"></div>
<div class=\"image-view\" data-width=\"313\" data-height=\"39\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-e1ca1595c165d7fb.png\" data-original-width=\"313\" data-original-height=\"39\" data-original-format=\"image/png\" data-original-filesize=\"4732\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p></p>
<p><strong>变换之后的问题 == origin target function</strong></p>
<hr>
<hr>
<p>再次停顿一下，考虑一下KKT条件是什么：</p>
<blockquote>
<blockquote>
<h4>⑵KKT 条件的引出</h4>
<p>对于上述装换过的target function，有如下性质：<br>
</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 466px; max-height: 36px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 7.7299999999999995%;\"></div>
<div class=\"image-view\" data-width=\"466\" data-height=\"36\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-150d7930e9754e73.png\" data-original-width=\"466\" data-original-height=\"36\" data-original-format=\"image/png\" data-original-filesize=\"6561\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
那么对于任何的条件都会有左边≥右边的。左边再加上一个w,b取最小：<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 547px; max-height: 39px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 7.13%;\"></div>
<div class=\"image-view\" data-width=\"547\" data-height=\"39\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-84fe3b903185068a.png\" data-original-width=\"547\" data-original-height=\"39\" data-original-format=\"image/png\" data-original-filesize=\"7261\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
同样是大于右边的所有情况，那么自然了，我右边再加上一个取α的最大值也是可以的：<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 647px; max-height: 37px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 5.72%;\"></div>
<div class=\"image-view\" data-width=\"647\" data-height=\"37\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-587ab44bdbd2564d.png\" data-original-width=\"647\" data-original-height=\"37\" data-original-format=\"image/png\" data-original-filesize=\"8506\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
而在右边的我们把条件minimize和maximum调换过的式子就叫做Daul Problem。<strong>所以，原问题是≥对偶问题的。</strong>那么有没有什么办法可以把原问题的求解转换成对偶问题的求解呢？<p></p>
</blockquote>
</blockquote>
<hr>
<hr>
<blockquote>
<blockquote>
<h4>⑶KKT 条件的简单证明</h4>
<p></p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 309px; max-height: 18px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 5.83%;\"></div>
<div class=\"image-view\" data-width=\"309\" data-height=\"18\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-874999150411cd19\" data-original-width=\"309\" data-original-height=\"18\" data-original-format=\"image/png\" data-original-filesize=\"1206\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
对偶的意思就是存在一个最优的解使得两边的等式成立。所以我们假设有一个W和B是最优的，那么有：<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 623px; max-height: 41px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 6.58%;\"></div>
<div class=\"image-view\" data-width=\"623\" data-height=\"41\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-03065c36083904bb\" data-original-width=\"623\" data-original-height=\"41\" data-original-format=\"image/png\" data-original-filesize=\"2821\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
而最后可以看到求出来的解正是我们要求的f(W)原目标函数，而原式子：<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 142px; max-height: 18px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 12.68%;\"></div>
<div class=\"image-view\" data-width=\"142\" data-height=\"18\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-99d98f7eaff544ba\" data-original-width=\"142\" data-original-height=\"18\" data-original-format=\"image/png\" data-original-filesize=\"734\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
代进去也将是这个结果，因为maximum之后ag(x) = 0，所以本质上这个函数还是求f(W)的最小值。所以对偶式子和原式在结果上是没有差别的。根据上面的式子，我们本质就是要求<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 144px; max-height: 31px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 21.529999999999998%;\"></div>
<div class=\"image-view\" data-width=\"144\" data-height=\"31\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-0aa4a4aaf9b80da3\" data-original-width=\"144\" data-original-height=\"31\" data-original-format=\"image/png\" data-original-filesize=\"4052\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
的最小值，当然这里的W,B要替换成原来的变量w，b了。求最小值自然就是求梯度为0了，所以w,b梯度为0的条件就有了。还有一个ag(x) = 0的条件，这个其实是前提条件：<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 167px; max-height: 18px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 10.780000000000001%;\"></div>
<div class=\"image-view\" data-width=\"167\" data-height=\"18\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-faf8aeaab12a20f0\" data-original-width=\"167\" data-original-height=\"18\" data-original-format=\"image/png\" data-original-filesize=\"2712\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
我们之前说这个式子是等同目标函数的，既然要等同自然是要把后面的g(x)和h(x)消去啊！而h(x) = 0本来就消去了，而g(x) &lt; 0，求最大必然就ag(x) = 0了，因为只有这个条件，才能消去后面的ag(x)把这个minimum maximum式子变成minimumf(w)的式子。所以再加上先前的几个拉格朗日条件就组成了KKT条件了。<br>
<strong>所以KKT condition就是：</strong><br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 154px; max-height: 47px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 30.520000000000003%;\"></div>
<div class=\"image-view\" data-width=\"154\" data-height=\"47\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-0524b7004e53ed79.png\" data-original-width=\"154\" data-original-height=\"47\" data-original-format=\"image/png\" data-original-filesize=\"2598\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 154px; max-height: 51px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 33.12%;\"></div>
<div class=\"image-view\" data-width=\"154\" data-height=\"51\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-a5feb090757ded7f.png\" data-original-width=\"154\" data-original-height=\"51\" data-original-format=\"image/png\" data-original-filesize=\"2539\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 181px; max-height: 35px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 19.34%;\"></div>
<div class=\"image-view\" data-width=\"181\" data-height=\"35\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-81069f6819f36c26.png\" data-original-width=\"181\" data-original-height=\"35\" data-original-format=\"image/png\" data-original-filesize=\"2664\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 133px; max-height: 34px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 25.56%;\"></div>
<div class=\"image-view\" data-width=\"133\" data-height=\"34\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-222a8fde5676908a.png\" data-original-width=\"133\" data-original-height=\"34\" data-original-format=\"image/png\" data-original-filesize=\"2013\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 184px; max-height: 36px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 19.57%;\"></div>
<div class=\"image-view\" data-width=\"184\" data-height=\"36\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-6f58b20827cc3a79.png\" data-original-width=\"184\" data-original-height=\"36\" data-original-format=\"image/png\" data-original-filesize=\"2774\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
<strong>最后的几个条件其实是lagrange乘子法的条件。</strong><p></p>
</blockquote>
</blockquote>
<hr>
<hr>
<p>回到正题，既然我们知道了可以利用KKT condition把origin target function转换到daul problem 来求解，那么上面这个问题我们尝试用KKT条件求解一下：<br>
首先对w，b求偏导：<br>
</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 231px; max-height: 124px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 53.68000000000001%;\"></div>
<div class=\"image-view\" data-width=\"231\" data-height=\"124\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-d511cb022acc957e.png\" data-original-width=\"231\" data-original-height=\"124\" data-original-format=\"image/png\" data-original-filesize=\"14415\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
把结果代回到dual problem：<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 588px; max-height: 93px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 15.82%;\"></div>
<div class=\"image-view\" data-width=\"588\" data-height=\"93\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-46569a469c958d4b.png\" data-original-width=\"588\" data-original-height=\"93\" data-original-format=\"image/png\" data-original-filesize=\"34890\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
所以最后我们的target function就变成了这样。<br>
最后我们可以用QP对这个问题进行求解，求出了α之后，我们随便取一个α是非0的，也就是＞0的解，这时候利用α*g(x) = 0的条件得到<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 175px; max-height: 30px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 17.14%;\"></div>
<div class=\"image-view\" data-width=\"175\" data-height=\"30\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-c3909d31a7bcc7e9.png\" data-original-width=\"175\" data-original-height=\"30\" data-original-format=\"image/png\" data-original-filesize=\"2718\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
b就求出来了，对于w直接代换上面的公式就好了。<br>
<strong>而当α&gt;0，由上面的公式可以得到这个点就刚刚好是在边界上，而这些点就叫做support vector，支持向量的点。我们的拟合直线也将会由着些点确定，其他不是support vector的点α就是0。</strong><p></p>
<hr>
<hr>
<p>又停顿一下，我们对这个式子思考一下：</p>
<blockquote>
<blockquote>
<h4>⑷为什么我们需要dual problem</h4>
<p>其实这个最优问题用普通的QP求解也是可以的，但是如果我们的数据维度很大，而经过feature transform之后维度就好更大了，这样就会导致VC dimension会增大，特别是后面用的多项式核，RBF核等等。经过对偶问题KKT条件的变换之后，我们的目标式子：<br>
</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 374px; max-height: 164px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 43.85%;\"></div>
<div class=\"image-view\" data-width=\"374\" data-height=\"164\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-4e6b455df710e9df\" data-original-width=\"374\" data-original-height=\"164\" data-original-format=\"image/png\" data-original-filesize=\"39750\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
<strong>转换成对偶问题之后，变量个数是N个，约束条件也是N个，于VC dimension就没有了关系，从某种意义上是简化了计算复杂度。其实计算复杂度还是没有变，只是把维度的计算提升到了变量之间点的內积罢了。将原始SVM转化为对偶问题，本意是在非线性变化，进行特征转换后，如果d’很大，为了简化计算，消除d’的影响。进一步引入Kernel SVM，根本上解决上述问题。注意了，这里只是从某个角度看确实是消除了d维度的影响，实际上并没有消失，只是转移到了计算內积里面而已。</strong><p></p>
</blockquote>
</blockquote>
<hr>
<hr>
<p>回到正题，我们的target function：</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 374px; max-height: 164px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 43.85%;\"></div>
<div class=\"image-view\" data-width=\"374\" data-height=\"164\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-4e6b455df710e9df\" data-original-width=\"374\" data-original-height=\"164\" data-original-format=\"image/png\" data-original-filesize=\"39750\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
<p>至于这个α怎么求，后面会用SMO算法求解。<br>
到这里linear SVM就算结束了。</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 254px; max-height: 154px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 60.629999999999995%;\"></div>
<div class=\"image-view\" data-width=\"254\" data-height=\"154\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-9d3052c689642e0f.png\" data-original-width=\"254\" data-original-height=\"154\" data-original-format=\"image/png\" data-original-filesize=\"15414\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
这就是分类函数。
<hr>
<hr>
<p>再停顿一下，<strong>什么是支持向量点，为什么非支持向量的点α = 0？</strong>这里仅仅思考linear SVM，如果是soft margin又不一样了。</p>
<blockquote>
<blockquote>
<h4>⑸支持向量</h4>
<p></p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 285px; max-height: 40px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 14.04%;\"></div>
<div class=\"image-view\" data-width=\"285\" data-height=\"40\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-fe1e7588804ec1ae.png\" data-original-width=\"285\" data-original-height=\"40\" data-original-format=\"image/png\" data-original-filesize=\"4319\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
<strong>如果是支持向量，他的function margin是1；而对于不少支持向量的点，function margin &gt; 1，所以右边是负数，为了满足最大，所以α只能为0了，所以非支持向量的点α就是0。</strong><p></p>
</blockquote>
</blockquote>
<hr>
<hr>
<h2>⑤kernel Support Vector Machine</h2>
<p>回到正题，刚刚只是讲了linear SVM，是对于linear separable有效而已，如果是linear inseparable呢？比如一个圆形，这样就玩不了。记得之前linear regression和logistics regression讲到过一个feature transform，如果是非线性的我们可以映射到其他维度进行解决，比如最常见的polynomial transform，但是这样问题来了，刚刚不是才把维度d转移到內积吗？（用dual problem的KKT condition）在来个feature transform那就是φ(x0)φ(x1)了，维度就更大了。</p>
<p>比如polynomial：<br>
</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 569px; max-height: 106px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 18.63%;\"></div>
<div class=\"image-view\" data-width=\"569\" data-height=\"106\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-399f2b8ad925e54d.png\" data-original-width=\"569\" data-original-height=\"106\" data-original-format=\"image/png\" data-original-filesize=\"18686\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
二项式的是这样的，注意到中间好像多了一个X1Xd，这是为了后面计算方便而已。两个做內积：<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 407px; max-height: 168px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 41.28%;\"></div>
<div class=\"image-view\" data-width=\"407\" data-height=\"168\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-b891f02a27d5b861.png\" data-original-width=\"407\" data-original-height=\"168\" data-original-format=\"image/png\" data-original-filesize=\"18810\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
<strong>可以看到，最后的转换就只和原始空间有关系而已，对于转换只后的z空间的维度没有关系。比如x空间是2维的，为了解决nonlinear problem，我们映射到了z空间，在z空间里面维度肯定会比在x空间的原始维度要大，而最后用z空间做內积我们就只需要拿x空间的原始维度就好了，因为我们可以先內积再升维，而不是先升维再內积。</strong><br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 268px; max-height: 77px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 28.73%;\"></div>
<div class=\"image-view\" data-width=\"268\" data-height=\"77\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-217c8587dbd6e2c0.png\" data-original-width=\"268\" data-original-height=\"77\" data-original-format=\"image/png\" data-original-filesize=\"5464\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
这种就叫做核函数了。<br>
最后的分类函数用kernel function替代：<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 232px; max-height: 59px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 25.430000000000003%;\"></div>
<div class=\"image-view\" data-width=\"232\" data-height=\"59\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-326f9eecb4f89ba7.png\" data-original-width=\"232\" data-original-height=\"59\" data-original-format=\"image/png\" data-original-filesize=\"4939\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
刚刚所讲的就是核函数的一种——<strong>polynomial kernel function</strong><br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 573px; max-height: 125px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 21.82%;\"></div>
<div class=\"image-view\" data-width=\"573\" data-height=\"125\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-64d9a6113836f14c.png\" data-original-width=\"573\" data-original-height=\"125\" data-original-format=\"image/png\" data-original-filesize=\"24820\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
加上几个参数，γ就是它的参数了，最后化简一下：<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 300px; max-height: 41px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 13.669999999999998%;\"></div>
<div class=\"image-view\" data-width=\"300\" data-height=\"41\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-7fb5b22f89c79ac2.png\" data-original-width=\"300\" data-original-height=\"41\" data-original-format=\"image/png\" data-original-filesize=\"5629\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
虽然都是二次转换，对应到同一个z空间。但是，如果他们的γ系数不同，内积就会不一样，那么就代表有不同的距离，最终可能会得到不同的SVM margin。所以，系数不同，可能会得到不同的hperplane。<br>
看一下γ系数对于hperplane的影响：<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 572px; max-height: 204px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 35.66%;\"></div>
<div class=\"image-view\" data-width=\"572\" data-height=\"204\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-70a72c7b95517588.png\" data-original-width=\"572\" data-original-height=\"204\" data-original-format=\"image/png\" data-original-filesize=\"33561\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
使用高阶的polynomial kernel的话，得到的Support Vector数量不会太多，分类面不会太复杂，防止过拟合。同样也避开了对升维之后维度的依赖。<p></p>
<p>接下来介绍另外一种kernel function——<strong>Gaussion kernel function</strong><br>
刚刚介绍的Q阶多项式是有限维度的，如果是无限维度的能不能通过kernel来简化计算？？有一个无限维的kernel function——Gaussion kernel<br>
</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 151px; max-height: 43px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 28.48%;\"></div>
<div class=\"image-view\" data-width=\"151\" data-height=\"43\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-53815324954ba458.png\" data-original-width=\"151\" data-original-height=\"43\" data-original-format=\"image/png\" data-original-filesize=\"2537\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
这和我们之前见的有些不同，只是去掉了下面的方差而已，方差是定值没有什么太大的影响。逆推看看它的维度是多少：<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 583px; max-height: 251px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 43.05%;\"></div>
<div class=\"image-view\" data-width=\"583\" data-height=\"251\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-53a77383713c3360.png\" data-original-width=\"583\" data-original-height=\"251\" data-original-format=\"image/png\" data-original-filesize=\"45820\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
推出来后面的维度是无限个（中间用的是Taylor展开，因为e的特殊求导性质可以简化）。<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 441px; max-height: 134px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 30.39%;\"></div>
<div class=\"image-view\" data-width=\"441\" data-height=\"134\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-02442c3fecc0bf18.png\" data-original-width=\"441\" data-original-height=\"134\" data-original-format=\"image/png\" data-original-filesize=\"18140\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
分类函数就出来了。<br>
但是核函数的过拟合还是有一点严重的：<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 569px; max-height: 215px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 37.79%;\"></div>
<div class=\"image-view\" data-width=\"569\" data-height=\"215\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-2475b2c362136ed0.png\" data-original-width=\"569\" data-original-height=\"215\" data-original-format=\"image/png\" data-original-filesize=\"37672\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
γ对于核函数的影响有点大。如果取值很大的话最后就会形成一个一个的小圈圈把那些点圈起来。<p></p>
<hr>
<hr>
<p>又得停顿一下，思考一下核函数的意义以及他们之间的对比：</p>
<blockquote>
<blockquote>
<h4>⑹Comparison of Kernels</h4>
<p>Polynomial Kernel的hyperplanes是由多项式曲线构成。优点：阶数可以灵活设置，更贴近实际分布；缺点：当Q很到的时候，如果kernel里面的值算出来是&lt;1，那就基本接近0了，大于1就会变得很大，增加计算复杂度。而且参数过多，难以选择合适的值。<br>
</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 562px; max-height: 171px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 30.43%;\"></div>
<div class=\"image-view\" data-width=\"562\" data-height=\"171\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-f1c80d8ddee3192d.png\" data-original-width=\"562\" data-original-height=\"171\" data-original-format=\"image/png\" data-original-filesize=\"35081\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
Gaussan Kernel的优点是边界更加复杂多样，能最准确地区分数据样本，数值计算K值波动较小，而且只有一个参数，容易选择；缺点是由于特征转换到无限维度中，w没有求解出来，计算速度要低于linear kernel，而且可能会发生过拟合。mysterious——no w；slower；too powerful。<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 564px; max-height: 218px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 38.65%;\"></div>
<div class=\"image-view\" data-width=\"564\" data-height=\"218\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-0285ef16110711f0.png\" data-original-width=\"564\" data-original-height=\"218\" data-original-format=\"image/png\" data-original-filesize=\"42493\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
之前说过通过对偶问题，我们的把数据维度转移到了內积上，所以从某一方面来看我们确实是做到了简化计算复杂度，但是实际上內积还是属于一个很大的计算。<strong>所以核函数的功能之一，就是简化计算，把升维和计算內积合在了一起，减少计算复杂度。把计算步骤结合在了一起，之前是先映射再计算內积，现在是一起做了。核函数的功能之二，就是可以很好的计算两个样本点的相似性，即內积。既然是代表相似性，我们可不可以使用其他的核函数呢？或者自己创建一个，比如欧氏距离，余弦距离等等？答案是不行。</strong><br>
先来看一下kernel的矩阵：<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 474px; max-height: 125px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 26.369999999999997%;\"></div>
<div class=\"image-view\" data-width=\"474\" data-height=\"125\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-153ca16e1ed4ad3a.png\" data-original-width=\"474\" data-original-height=\"125\" data-original-format=\"image/png\" data-original-filesize=\"18114\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
这有点像之前的协方差矩阵，只是没有减去均值，所以对称半正定是基本性质了。所以自然，我们自己创建或选择的时候也要选择<strong>①symmetric对称②positive semi-definite 半正定。这也是核函数有效性的判断。</strong><p></p>
</blockquote>
</blockquote>
<hr>
<hr>
<p>回到正题，刚刚只是讲了一下对核函数的理解。</p>
<h2>⑥Soft-Margin Support Vector Machine</h2>
<p>上面应用到的Gaussion Kernel貌似还是会出现过拟合，而且还是蛮严重的，这说明large margin已经限制不了Gaussion kernel了，我们需要找其他方法来处理这个问题。<br>
之前有一个比较简单的算法——perceptron learning algorithm<br>
这个算法对于nonlinear problem有一个很好的处理方式，我们不要求一定要分类正确，我们只要求找到一个错误最少的分类就可以了。所以他的function是这样：<br>
</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 285px; max-height: 119px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 41.75%;\"></div>
<div class=\"image-view\" data-width=\"285\" data-height=\"119\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-75a91655f0ba6a1d.png\" data-original-width=\"285\" data-original-height=\"119\" data-original-format=\"image/png\" data-original-filesize=\"8421\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
不正确的就加个1，最小为止。SVM也可以用这种方法来限制。<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 527px; max-height: 125px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 23.72%;\"></div>
<div class=\"image-view\" data-width=\"527\" data-height=\"125\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-1b9839de2d0b83f1.png\" data-original-width=\"527\" data-original-height=\"125\" data-original-format=\"image/png\" data-original-filesize=\"22982\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
加上一个条件，C参数就是对于这些错误惩罚度是多少，条件也变了，正确的≥ 1，错误的不管他。不管是小错还是大错。<br>
整合一下：<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 417px; max-height: 98px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 23.5%;\"></div>
<div class=\"image-view\" data-width=\"417\" data-height=\"98\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-e37445cb947f7b4a.png\" data-original-width=\"417\" data-original-height=\"98\" data-original-format=\"image/png\" data-original-filesize=\"14911\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
这个式子其实没有什么用，<strong>首先不是线性的，用不了二次规划，更不要说对偶这些了，其次大错小错都是同等对待，connot distinguish small error and large error。</strong><br>
对于上述方案继续修正：<br>
我们采用一个ξ作为一个犯错程度，程度越大，惩罚越大。惩罚就是这个式子数值会变大，然后SVM要花更多的力气去处理。<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 528px; max-height: 84px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 15.909999999999998%;\"></div>
<div class=\"image-view\" data-width=\"528\" data-height=\"84\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-0bc62bdb8410274e.png\" data-original-width=\"528\" data-original-height=\"84\" data-original-format=\"image/png\" data-original-filesize=\"15251\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p></p>
<p>接下来就是对偶问题的推导，和之前的hard其实差不多的，lagrange 乘子法加对偶条件：<br>
</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 498px; max-height: 109px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 21.89%;\"></div>
<div class=\"image-view\" data-width=\"498\" data-height=\"109\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-f1fba770025a69cc\" data-original-width=\"498\" data-original-height=\"109\" data-original-format=\"image/png\" data-original-filesize=\"16508\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
同样，KKT条件：<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 199px; max-height: 58px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 29.15%;\"></div>
<div class=\"image-view\" data-width=\"199\" data-height=\"58\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-711e93f237bbb5a1\" data-original-width=\"199\" data-original-height=\"58\" data-original-format=\"image/png\" data-original-filesize=\"2884\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
C - α = β<br>
所以有：0 &lt; α &lt; C<br>
其他的基本一致：w求导为0：<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 132px; max-height: 56px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 42.42%;\"></div>
<div class=\"image-view\" data-width=\"132\" data-height=\"56\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-64de78aa449ee508\" data-original-width=\"132\" data-original-height=\"56\" data-original-format=\"image/png\" data-original-filesize=\"2399\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
b求导：<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 107px; max-height: 45px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 42.059999999999995%;\"></div>
<div class=\"image-view\" data-width=\"107\" data-height=\"45\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-d0d0cdab94b5cf47\" data-original-width=\"107\" data-original-height=\"45\" data-original-format=\"image/png\" data-original-filesize=\"1992\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
接下来就是求b了：<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 269px; max-height: 195px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 72.49%;\"></div>
<div class=\"image-view\" data-width=\"269\" data-height=\"195\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-8b8b51cbc3ec75dd\" data-original-width=\"269\" data-original-height=\"195\" data-original-format=\"image/png\" data-original-filesize=\"24291\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
<strong>求b的公式里面有一个矛盾的地方，就是我们要求b首先得要求出来ξ的值，但是ξ的值也只有b的公式可以求的处理，所以这就有一个鸡生蛋蛋生鸡的问题。所以我们口语去掉这个ξ。我们刚刚用到的是拉格朗日乘子法，后面的β(-ξ)是一个仿射函数，仿射函数有β(-ξ) = 0的性质，所以把β代换一下就得到了上图的公式。那么去掉ξ就是等于0了，那么就只有C-α不等于0才有啊，所以当这个α ∈ （0 ， C）的时候就有ξ为0，而后面我们会讲到当α∈（0，C）的时候这个点其实是支持向量的点。这样就可以求出了b。</strong><br>
接下来看看C取值：<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 567px; max-height: 203px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 35.8%;\"></div>
<div class=\"image-view\" data-width=\"567\" data-height=\"203\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-4517f9f83536b525\" data-original-width=\"567\" data-original-height=\"203\" data-original-format=\"image/png\" data-original-filesize=\"41072\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
直接从我以前在CSDN里面写过的拷贝过来了。<p></p>
<p>接下来看一下一个比较重要的东西：<br>
<strong>physical significance of α</strong><br>
</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 246px; max-height: 69px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 28.050000000000004%;\"></div>
<div class=\"image-view\" data-width=\"246\" data-height=\"69\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-7ef766535a474aca\" data-original-width=\"246\" data-original-height=\"69\" data-original-format=\"image/png\" data-original-filesize=\"5656\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
<strong>为什么βξ = 0？原因和前一个公式是一样的，因为要取最大值，所以这里要等于0，β ≥ 0，而实际公式是negative ξ，所以乘上去要是0才能有最大；第二，如果不是等于0就不等于是原问题的求解了，不等于0就无端端多了一个inequality，和原问题不对等了。之后才能进行daul problem的转换。</strong><br>
<strong>我们主要是从上面这两个公式来看当α取值不同的时候对应的物理意义。</strong><p></p>
<p><strong>当α = 0，得ξ = 0，这个点就是没有放错的点，因为ξ = 0，不需要容忍。而α = 0，所以不是支持向量机的点，所以代表的就是在bound外并且分类正确的点。</strong></p>
<p><strong>当α∈（0，C），还是得到ξ = 0，这时候就不一样了，还没有错误的点，但是第一条式子括号里面等于0了，意味着就是在bound上的点，那么就是支持向量点了。</strong></p>
<p><strong>当α = C，不能确定ξ是不是0了，</strong></p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 190px; max-height: 31px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 16.32%;\"></div>
<div class=\"image-view\" data-width=\"190\" data-height=\"31\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-1473eac422d7702f\" data-original-width=\"190\" data-original-height=\"31\" data-original-format=\"image/png\" data-original-filesize=\"2856\"></div>
</div>
<strong><div class=\"image-caption\"></div></strong>
</div>
<strong><br>
，表示就是错了多少，这种有两种情况，一种是分类正确了，但是距离太近；或者是分类错了。</strong><br>
<strong>当α &gt; C，不存在的，上面都限制了。</strong><p></p>
<p>理一下整个思路。<br>
<strong>①找到最好的hperplane，最宽的那个。<br>
②得到target function<br>
③发现feature transform之后维度对于计算机复杂度有很大影响，用dual problem转移到內积处理<br>
④转移之后发现还是复杂度在的，引出了kernel function<br>
⑤发现kernel function还是有overfitting的情况，于是又引入了soft margin</strong></p>
<hr>
<hr>
<p>在讲SMO算法之前，先讲一下对于error function的理解：</p>
<blockquote>
<blockquote>
<h4>⑺对于SVM error function的理解</h4>
<p>我们把SVM换一种形式。对于ξ，其实他是每一个点距离边界有多远，一种是violating margin，即不满足y(wTz + b) ≥ 1，那么ξ就可以表示成：1 - y(wTz + b) &gt; 0。第二种情况就是not violating margin，即这个点在边界之外，就是满足上述公式了，这个时候ξ就是0，我们整合一下：<br>
ξ = max ( 1 - y(wTz + b) , 0 )，代换进原来的支持向量机公式：<br>
</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 312px; max-height: 61px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 19.55%;\"></div>
<div class=\"image-view\" data-width=\"312\" data-height=\"61\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-c5621dd0c7f62dc4\" data-original-width=\"312\" data-original-height=\"61\" data-original-format=\"image/png\" data-original-filesize=\"5450\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
这个就是支持向量机的error function，先预判了Ein = 0，也就是全对的情况，前面有说到。<br>
这个function有点像我们之前所学的L2 lost function：<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 194px; max-height: 48px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 24.740000000000002%;\"></div>
<div class=\"image-view\" data-width=\"194\" data-height=\"48\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-200124b006f2e8b9\" data-original-width=\"194\" data-original-height=\"48\" data-original-format=\"image/png\" data-original-filesize=\"3248\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
这和logistics regression的L2范式的cost function很相似。<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 196px; max-height: 52px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 26.529999999999998%;\"></div>
<div class=\"image-view\" data-width=\"196\" data-height=\"52\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-9569a973ff5c40d3\" data-original-width=\"196\" data-original-height=\"52\" data-original-format=\"image/png\" data-original-filesize=\"3735\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
其实就差不多是一样的，没有什么差别，但是既然是相同的为什么不用这种方法呢？两个原因，一个是这种无条件的最优化问题无法通过QP解决，即对偶推导和kernel都无法使用；另一个是这种形式中包含的max()项可能造成函数并不是处处可导，这种情况难以用微分方法解决。<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 563px; max-height: 136px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 24.16%;\"></div>
<div class=\"image-view\" data-width=\"563\" data-height=\"136\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-4f401d0188fa327c\" data-original-width=\"563\" data-original-height=\"136\" data-original-format=\"image/png\" data-original-filesize=\"29930\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
<strong>对比发现，L2 regularization和soft margin SVM形式是一样的，两个式子λ和C是互相对应的。soft marginSVM里面的large margin就对应着L2 regularization里面的short w，都是让hypothesis set可以简单点。λ和C也是互相对应，λ大，w就小，正则化的程度就越大；C小，Ein就大，响应这个margin也会打，所以增大C和减小λ是一个意思，所以large margin等同于regularization，都是防止过拟合作用的。</strong><br>
<strong><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 564px; max-height: 110px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 19.5%;\"></div>
<div class=\"image-view\" data-width=\"564\" data-height=\"110\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-6a7a85d4d137de9c\" data-original-width=\"564\" data-original-height=\"110\" data-original-format=\"image/png\" data-original-filesize=\"23277\"></div>
</div>
<div class=\"image-caption\"></div>
</div></strong> <strong>如果是按照我们之前的err0/1，正确为1，错误就是0，那么有：</strong><br>
<strong><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 591px; max-height: 183px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 30.959999999999997%;\"></div>
<div class=\"image-view\" data-width=\"591\" data-height=\"183\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-b7606962426a10d5\" data-original-width=\"591\" data-original-height=\"183\" data-original-format=\"image/png\" data-original-filesize=\"32111\"></div>
</div>
<div class=\"image-caption\"></div>
</div></strong><br>
<strong>可以看到SVM他是大于err0/1的，根据VC bound理论是可以用来代替err0/1分类的。</strong><br>
<strong>后面再加上logic function的cost function：</strong><br>
<strong><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 594px; max-height: 192px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 32.32%;\"></div>
<div class=\"image-view\" data-width=\"594\" data-height=\"192\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-eccbc5344869b973\" data-original-width=\"594\" data-original-height=\"192\" data-original-format=\"image/png\" data-original-filesize=\"42316\"></div>
</div>
<div class=\"image-caption\"></div>
</div></strong><br>
<strong><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 579px; max-height: 73px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 12.61%;\"></div>
<div class=\"image-view\" data-width=\"579\" data-height=\"73\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-642927e70a7f2ec6\" data-original-width=\"579\" data-original-height=\"73\" data-original-format=\"image/png\" data-original-filesize=\"13658\"></div>
</div>
<div class=\"image-caption\"></div>
</div></strong><br>
<strong>而这个几乎就是和L2-regularized logistic regression一样的。Logistic Regression和Soft-Margin SVM都是在最佳化err0/1的上界而已。可以看出，求解regularized logistic regression的问题等同于求解soft-margin SVM的问题。</strong><p></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<h4>⑻损失函数</h4>
<p>常见的损失函数：<br>
<strong>err0/1：</strong><br>
</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 359px; max-height: 58px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 16.16%;\"></div>
<div class=\"image-view\" data-width=\"359\" data-height=\"58\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-fc51be165cb443d2.png\" data-original-width=\"359\" data-original-height=\"58\" data-original-format=\"image/png\" data-original-filesize=\"6187\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
此时soft margin就是这样了，大于0就是1小于就是0。<br>
<strong>不敏感损失函数 —— hinge lost function</strong><br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 372px; max-height: 62px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 16.669999999999998%;\"></div>
<div class=\"image-view\" data-width=\"372\" data-height=\"62\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-5f43bb045c21c52b.png\" data-original-width=\"372\" data-original-height=\"62\" data-original-format=\"image/png\" data-original-filesize=\"6427\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
<strong>还有对数损失函数交叉熵等等。logistics用的是交叉熵，SVM就是用的hinge lost function。支持向量机就是一个结构风险最小化的近似实现，结构风险相当于期望风险(Eout)的一个上界，它是经验风险（Ein）和置信区间(Ω模型复杂度)的和，经验风险依赖于决策函数f的选取，但是置信区间是，F的VC维的增函数，两者是矛盾的。矛盾体现在：当VC维数变大的时候可以选到更好的f使得经验风险比较小，但是此时的置信区间比较大。这就是对应了VC bound理论。还好去听了台湾大学林轩宇老师课程，对这些机器学习理论基础有了解。</strong><p></p>
</blockquote>
</blockquote>
<hr>
<hr>
<p>回到正题，开始SMO算法。</p>
<h2>⑦SMO算法</h2>
<p>target function：<br>
</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 328px; max-height: 256px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 78.05%;\"></div>
<div class=\"image-view\" data-width=\"328\" data-height=\"256\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-886710e3fc64b9d9.png\" data-original-width=\"328\" data-original-height=\"256\" data-original-format=\"image/png\" data-original-filesize=\"12903\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
刚刚我们知道怎么求w,b，但是那是在知道了α的前提下，现在就来求α。<br>
基本思路：<br>
<strong>选择两个变量，固定其他变量，针对两个变量构建一个二次规划问题。每次针对两个变量来求解目标函数的最小值，求解完后，继续寻找新的变量求目标函数，在每次寻找新α的过程中，目标函数将进一步得到优化，直到所有的αi更新完了。而对于α的选取，一个是违反KKT条件最严重的那一个，另一个由约束条件自动确定。</strong><p></p>
<p>首先，假设我们选取了两个变量α1，α2，固定其他变量之后：<br>
</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 139px; max-height: 52px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 37.41%;\"></div>
<div class=\"image-view\" data-width=\"139\" data-height=\"52\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-bb4b8d328f932daf.png\" data-original-width=\"139\" data-original-height=\"52\" data-original-format=\"image/png\" data-original-filesize=\"2258\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
所以只要求出α2，α1就知道了。<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 265px; max-height: 55px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 20.75%;\"></div>
<div class=\"image-view\" data-width=\"265\" data-height=\"55\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-e112c9c8f8111cf6.png\" data-original-width=\"265\" data-original-height=\"55\" data-original-format=\"image/png\" data-original-filesize=\"3447\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
原目标函数化简之后：<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 416px; max-height: 105px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 25.240000000000002%;\"></div>
<div class=\"image-view\" data-width=\"416\" data-height=\"105\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-47b2dbe73e13c87e.png\" data-original-width=\"416\" data-original-height=\"105\" data-original-format=\"image/png\" data-original-filesize=\"11191\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
K11指的就是x1和自己本身做核函数。由于我们已经固定了除了α1和α2，所以自然其他的常量我们可以去掉了，不如优化w+1，和优化w是一样的，去掉固定常数项就留下了上图的公式。<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 282px; max-height: 135px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 47.870000000000005%;\"></div>
<div class=\"image-view\" data-width=\"282\" data-height=\"135\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-63734f70c085a3f2.png\" data-original-width=\"282\" data-original-height=\"135\" data-original-format=\"image/png\" data-original-filesize=\"4825\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
别忘了条件，条件是后面求解的关键。<br>
首先我们要得到α1，α2的范围。<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 282px; max-height: 135px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 47.870000000000005%;\"></div>
<div class=\"image-view\" data-width=\"282\" data-height=\"135\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-63734f70c085a3f2.png\" data-original-width=\"282\" data-original-height=\"135\" data-original-format=\"image/png\" data-original-filesize=\"4825\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
由着两个约束条件限制。<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 472px; max-height: 525px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 111.23%;\"></div>
<div class=\"image-view\" data-width=\"472\" data-height=\"525\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-5b6d4c291a3557b7.png\" data-original-width=\"472\" data-original-height=\"525\" data-original-format=\"image/png\" data-original-filesize=\"263653\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
所以有：<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 656px; max-height: 174px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 26.52%;\"></div>
<div class=\"image-view\" data-width=\"656\" data-height=\"174\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-558d3004a6ac3173.png\" data-original-width=\"656\" data-original-height=\"174\" data-original-format=\"image/png\" data-original-filesize=\"18036\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
<strong>所以当我们更新了α之后，我们还要根据范围剪辑α才可以。</strong><p></p>
<p>我们假设：<br>
</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 214px; max-height: 58px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 27.1%;\"></div>
<div class=\"image-view\" data-width=\"214\" data-height=\"58\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-5db091031c991467.png\" data-original-width=\"214\" data-original-height=\"58\" data-original-format=\"image/png\" data-original-filesize=\"3851\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 178px; max-height: 37px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 20.79%;\"></div>
<div class=\"image-view\" data-width=\"178\" data-height=\"37\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-fd1f1a5a9b250c71.png\" data-original-width=\"178\" data-original-height=\"37\" data-original-format=\"image/png\" data-original-filesize=\"2278\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
剪辑范围：<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 281px; max-height: 77px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 27.400000000000002%;\"></div>
<div class=\"image-view\" data-width=\"281\" data-height=\"77\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-f0327b51a30609e9.png\" data-original-width=\"281\" data-original-height=\"77\" data-original-format=\"image/png\" data-original-filesize=\"6708\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
再假设一个定值，也就是i = 3开始求和的：<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 470px; max-height: 57px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 12.13%;\"></div>
<div class=\"image-view\" data-width=\"470\" data-height=\"57\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-f190b13f7bc25df4.png\" data-original-width=\"470\" data-original-height=\"57\" data-original-format=\"image/png\" data-original-filesize=\"8050\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
目标式子：<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 416px; max-height: 105px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 25.240000000000002%;\"></div>
<div class=\"image-view\" data-width=\"416\" data-height=\"105\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-47b2dbe73e13c87e.png\" data-original-width=\"416\" data-original-height=\"105\" data-original-format=\"image/png\" data-original-filesize=\"11191\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
用上面的vi代换之后：<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 605px; max-height: 52px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 8.6%;\"></div>
<div class=\"image-view\" data-width=\"605\" data-height=\"52\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-d5d502e56fad2919.png\" data-original-width=\"605\" data-original-height=\"52\" data-original-format=\"image/png\" data-original-filesize=\"7492\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
求α2的话自然是求导了：<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 581px; max-height: 55px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 9.47%;\"></div>
<div class=\"image-view\" data-width=\"581\" data-height=\"55\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-34614d5cdd6e560a.png\" data-original-width=\"581\" data-original-height=\"55\" data-original-format=\"image/png\" data-original-filesize=\"7200\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
为0得到：<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 473px; max-height: 41px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 8.67%;\"></div>
<div class=\"image-view\" data-width=\"473\" data-height=\"41\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-76e2008f8dc27fff.png\" data-original-width=\"473\" data-original-height=\"41\" data-original-format=\"image/png\" data-original-filesize=\"5080\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 149px; max-height: 28px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 18.790000000000003%;\"></div>
<div class=\"image-view\" data-width=\"149\" data-height=\"28\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-2a8a8a4db4e74364.png\" data-original-width=\"149\" data-original-height=\"28\" data-original-format=\"image/png\" data-original-filesize=\"2381\"></div>
</div>
<div class=\"image-caption\"></div>
</div>代入得到：<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 520px; max-height: 39px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 7.5%;\"></div>
<div class=\"image-view\" data-width=\"520\" data-height=\"39\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-5be2d11942d443b6.png\" data-original-width=\"520\" data-original-height=\"39\" data-original-format=\"image/png\" data-original-filesize=\"6802\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
这里的化简有点麻烦：<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 566px; max-height: 365px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 64.49000000000001%;\"></div>
<div class=\"image-view\" data-width=\"566\" data-height=\"365\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-a665964211b42d73.png\" data-original-width=\"566\" data-original-height=\"365\" data-original-format=\"image/png\" data-original-filesize=\"236463\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
手动证明一下。<br>
用假设替换一下上面的式子：<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 248px; max-height: 55px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 22.18%;\"></div>
<div class=\"image-view\" data-width=\"248\" data-height=\"55\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-581d092902e8cae5.png\" data-original-width=\"248\" data-original-height=\"55\" data-original-format=\"image/png\" data-original-filesize=\"4271\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
就可以了。<br>
<strong>SMO算法有两个要点：①α1的选择，违反KKT最严重的条件②α2的选择策略</strong><p></p>
<hr>
<hr>
<p>很重要的问题，变量要怎么选择，后面会有例子证明。</p>
<blockquote>
<blockquote>
<h4>⑼变量的选择方式</h4>
<p>SMO称选择第1个变量的过程为外层循环。外层循环在训练样本中选取违反KKT条件最严重的样本点，Violation of the most serious sample of KKT conditions。我第一次看这东西是懵逼的。但是仔细想一下，就是检测哪一个样本是没有满足KKT的条件：</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 212px; max-height: 172px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 81.13%;\"></div>
<div class=\"image-view\" data-width=\"212\" data-height=\"172\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-c18cfb6815b4f8a8.png\" data-original-width=\"212\" data-original-height=\"172\" data-original-format=\"image/png\" data-original-filesize=\"8239\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
<p>首先遍历所有0 &lt; α &lt; C的样本点，看看是不是满足的，如果没有载变量所有的。检测是否满足KKT。所以在SMO迭代的两个步骤中，只要α中有一个违背了KKT条件，这一轮迭代完成后，目标函数的值必然会增大。Generally speaking，KKT条件违背的程度越大，迭代后的优化效果越明显，增幅越大。<br>
α1选完了自然就是选择第二个α了，第二个变量的选择叫做内存循环，我们这里先用普通随机选择，看看效果如何。</p>

</blockquote>
</blockquote>
<hr>
<hr>
<h2>⑧算法实现——version 1</h2>
<p>首先是导入各种各样的包和一个工具了：</p>
<pre><code>import numpy as np  
import matplotlib.pyplot as plt  
import random  
import seaborn as sea  
import pandas as pd  


def get_positive_and_negative():  
  dataSet = pd.read_csv(\'Datas/LogiReg_data.txt\', names=[\'V1\', \'V2\', \'Class\'])  
  dataSet.Class[dataSet.Class == 0] = -1  
  dataSet = dataSet[60 : 80]  
  positive = dataSet[dataSet[\'Class\'] == 1]  
  negative = dataSet[dataSet[\'Class\'] == -1]  
  return positive , negative , dataSet  


def show_picture(positive , negative):  
  columns = [\'V1\', \'V2\']  
  fig, ax = plt.subplots(figsize=(10, 5))  
  ax.scatter(positive[columns[0]], positive[columns[1]], s=30, c=\"b\", marker=\"o\", label=\"class 1\")  
  ax.scatter(negative[columns[0]], negative[columns[1]], s=30, c=\"r\", marker=\"x\", label=\"class -1\")  
  ax.legend()  
  ax.set_xlabel(\'V1\')  
  ax.set_ylabel(\'V3\')  
  plt.show()  

def load_data_set():  
  _ , _ , file = get_positive_and_negative()  
  orig_data = file.as_matrix()  
  cols = orig_data.shape[1]  
  data_mat = orig_data[ : , 0 : cols-1]  
  label_mat = orig_data[ : , cols-1 : cols]  
  return  data_mat , label_mat  

positive , negative , data = get_positive_and_negative()  
show_picture(positive , negative)  
print(data)  
</code></pre>
<p>第一个是得到正负样本，然后显示，最后一个是加载数据，数据随便找一个就好了。</p>
<pre><code>positive , negative , data = get_positive_and_negative()  
show_picture(positive , negative)  
</code></pre>
<p>最后调用一些看看这些点是什么：</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 450px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 53.13%;\"></div>
<div class=\"image-view\" data-width=\"847\" data-height=\"450\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-5099f7b90415a56c.png\" data-original-width=\"847\" data-original-height=\"450\" data-original-format=\"image/png\" data-original-filesize=\"23138\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
<p>还有一些是对α的限制和一下工具函数：</p>

<pre><code>\'\'\'\'\' 
Generate a random number
\'\'\'  
def select_jrand(i , m):  
  j = i  
  while(j == i):  
      j = int(random.uniform(0 , m))  
  return j  
  pass  

\'\'\'\'\' 
restraint the α
\'\'\'  
def clip_alpha(aj , H , L):  
  if aj &gt; H:  
      aj = H  
  elif aj &lt; L:  
      aj = L  
  return aj  
  pass  
</code></pre>
<p><strong>接下来就是实现支持向量机了：</strong></p>
<pre><code>def SVM(data_mat , class_label , C , tolar , max_iter):  

  data_mat = np.mat(data_mat)  
  label_mat = np.mat(class_label)  
  b = 0  
  m , n = np.shape(data_mat)  
  alphas = np.zeros((m , 1))  
  iter = 0  

  while iter &lt; max_iter:  
      #作为迭代变化量  
      alpha_pairs_changed = 0  
      #作为第一个a  
      for i in range(m):  
          WT_i = np.dot(np.multiply(alphas , label_mat).T , data_mat)  
          f_xi = float(np.dot(WT_i , data_mat[i , :].T)) + b  
          Ei = f_xi - float(label_mat[i])  
          if ((label_mat[i]*Ei &lt; -tolar) and (alphas[i] &lt; C)) or ((label_mat[i]*Ei &gt; tolar) and (alphas[i] &gt; 0)):  
              j = Tools.select_jrand(i , m)  
              WT_j = np.dot(np.multiply(alphas , label_mat).T , data_mat)  
              f_xj  = float(np.dot(WT_j , data_mat[j , :].T)) + b  
              Ej = f_xj - float(label_mat[j])  
              alpha_iold = alphas[i].copy()  
              alpha_jold = alphas[j].copy()  

              if (label_mat[i] != label_mat[j]):  
                  L = max(0 , alphas[j] - alphas[i])  
                  H = min(C , C + alphas[j] - alphas[i])  
              else:  
                  L = max(0 , alphas[j] + alphas[i] - C)  
                  H = min(C , alphas[j] + alphas[i])  
              if H == L:  
                  continue  

              eta = 2.0 * data_mat[i, :] * data_mat[j, :].T - data_mat[i, :] * data_mat[i, :].T - data_mat[j, :] * data_mat[j, :].T  
              if eta &gt;= 0: continue  
              alphas[j] = (alphas[j] - label_mat[j]*(Ei - Ej))/eta  
              alphas[j] = Tools.clip_alpha(alphas[j], H, L)  
              if (abs(alphas[j] - alpha_jold) &lt; 0.00001):  
                  continue  
              alphas[i] = alphas[i] + label_mat[j]*label_mat[i]*(alpha_jold - alphas[j])  


              b1 = b - Ei + label_mat[i]*(alpha_iold - alphas[i])*np.dot(data_mat[i,:], data_mat[i,:].T) +\\  
              label_mat[j]*(alpha_jold - alphas[j])*np.dot(data_mat[i,:], data_mat[j,:].T)  
              b2 = b - Ej + label_mat[i]*(alpha_iold - alphas[i])*np.dot(data_mat[i,:], data_mat[j,:].T) +\\  
              label_mat[j]*(alpha_jold - alphas[j])*np.dot(data_mat[j,:], data_mat[j,:].T)  
              if (0 &lt; alphas[i]) and (C &gt; alphas[i]):  
                  b = b1  
              elif (0 &lt; alphas[j]) and (C &gt; alphas[j]):  
                  b = b2  
              else:  
                  b = (b1 + b2)/2.0  
              print(b)  
              alpha_pairs_changed += 1  
              pass  
      if alpha_pairs_changed == 0:  
          iter += 1  
      else:  
          iter = 0  

  support_x = []  
  support_y = []  
  class1_x = []  
  class1_y = []  
  class01_x = []  
  class01_y = []  
  for i in range(m):  
      if alphas[i] &gt; 0.0:  
          support_x.append(data_mat[i, 0])  
          support_y.append(data_mat[i, 1])  
  for i in range(m):  
      if label_mat[i] == 1:  
          class1_x.append(data_mat[i, 0])  
          class1_y.append(data_mat[i, 1])  
      else:  
          class01_x.append(data_mat[i, 0])  
          class01_y.append(data_mat[i, 1])  
  w_best = np.dot(np.multiply(alphas, label_mat).T, data_mat)  
  fig, ax = plt.subplots(figsize=(10, 5))  
  ax.scatter(support_x, support_y, s=100, c=\"y\", marker=\"v\", label=\"support_v\")  
  ax.scatter(class1_x, class1_y, s=30, c=\"b\", marker=\"o\", label=\"class 1\")  
  ax.scatter(class01_x, class01_y, s=30, c=\"r\", marker=\"x\", label=\"class -1\")  
  lin_x = np.linspace(0, 100)  
  lin_y = (-float(b) - w_best[0, 0] * lin_x) / w_best[0, 1]  
  plt.plot(lin_x, lin_y, color=\"black\")  
  ax.legend()  
  ax.set_xlabel(\"factor1\")  
  ax.set_ylabel(\"factor2\")  
  plt.show()  
  return b , alphas  
datamat , labelmat = dataSet.load_data_set()  
b, alphas = SVM(datamat , labelmat , 0.6 , 0.001 , 10)  
print(b , alphas)  
</code></pre>
<p>首先传入的后面几个参数分别是惩罚力度，容忍度。比较重要的应该是这一句：</p>
<pre><code>if ((label_mat[i]*Ei &lt; -tolar) and (alphas[i] &lt; C)) or ((label_mat[i]*Ei &gt; tolar) and (alphas[i] &gt; 0)):  
</code></pre>
<p><strong>这句话翻译过去就是yg(x) &lt; 1 - ξ或者是y(g(x)) &gt; 1+ξ。如果是小于，则这个点是离hperplane比较近，这时候这个点应该是等于C才对的；如果是大于了，也就是远大于边界了，那就是离边界很远了，但是α又大于0，离边界远意味着不是支持向量，所以α应该是0，所以可以改变。</strong><br>
后面的那些就是依据公式来的：<br>
</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 538px; max-height: 414px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 76.95%;\"></div>
<div class=\"image-view\" data-width=\"538\" data-height=\"414\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-039149652841bd04.png\" data-original-width=\"538\" data-original-height=\"414\" data-original-format=\"image/png\" data-original-filesize=\"25884\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
每一条都是对应公式写的。<br>
最后就是打印了。<p></p>
<p><strong>效果：</strong><br>
</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 425px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 49.71%;\"></div>
<div class=\"image-view\" data-width=\"855\" data-height=\"425\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-760a601530dfe6dc.png\" data-original-width=\"855\" data-original-height=\"425\" data-original-format=\"image/png\" data-original-filesize=\"31603\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 419px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 49.01%;\"></div>
<div class=\"image-view\" data-width=\"855\" data-height=\"419\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-1aec5dd575e9d2c4.png\" data-original-width=\"855\" data-original-height=\"419\" data-original-format=\"image/png\" data-original-filesize=\"36786\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 433px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 50.63999999999999%;\"></div>
<div class=\"image-view\" data-width=\"855\" data-height=\"433\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-a235ede5b00b7c32.png\" data-original-width=\"855\" data-original-height=\"433\" data-original-format=\"image/png\" data-original-filesize=\"38014\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
可以看到是极度不稳定。这是几个月前我实现的，后来现在我又重新实现了一个，用了一些改进方法。<strong>为什么会不稳定，我总结了几个原因：<br>
①没有缓存，更新慢，迭代次数不够<br>
②对于α2的选取没有很好的采取策略<br>
③对于n，也就是更新公式：</strong><br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 252px; max-height: 53px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 21.029999999999998%;\"></div>
<div class=\"image-view\" data-width=\"252\" data-height=\"53\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-cfa71a20c6efb6a9.png\" data-original-width=\"252\" data-original-height=\"53\" data-original-format=\"image/png\" data-original-filesize=\"4259\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
<strong>我没有判断是不是大于0的。n是什么东西呢？</strong><br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 138px; max-height: 37px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 26.810000000000002%;\"></div>
<div class=\"image-view\" data-width=\"138\" data-height=\"37\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-2cb47367a041002f.png\" data-original-width=\"138\" data-original-height=\"37\" data-original-format=\"image/png\" data-original-filesize=\"1898\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
<strong>他要是小于0意味着这个kernel matrix就不是半正定的了，K11 + K22 &lt; 2K12；另外，这个n其实是：</strong><br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 588px; max-height: 70px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 11.899999999999999%;\"></div>
<div class=\"image-view\" data-width=\"588\" data-height=\"70\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-13113d9c47661caf.png\" data-original-width=\"588\" data-original-height=\"70\" data-original-format=\"image/png\" data-original-filesize=\"7454\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
<strong>的二阶导数，小于0就不是凸函数了，哪来的凸优化。所以应该是更新的时候遇到这些情况导致不稳定的。</strong><br>
基于上面的缺点更换策略。<p></p>
<h2>⑨算法实现——version 2</h2>
<p>首先要改变的是加上一个缓存，用来保存Ei的值，使得计算更块。<strong>其次就是α2的选择策略，在优化过程中，会通过最大化步长的方式来获得第二个alpha值。第二步优化为，数据集全程扫描策略与在非边界alpha对中进行更新策略交替进行。</strong>对于n，会进行判断是不是大于0，在这里是用-号的，所以n与我们表达式上的是想反方向，所以是大于0。<br>
首先还是工具：</p>
<pre><code>\'\'\'
load data and define some tool function
\'\'\'
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import random

def loadDataSet(filename):
  \'\'\'
  :param filename:
  :return dataset and label:
  \'\'\'

  dataset = []
  label = []
  fr = open(filename)
  for line in fr.readlines():
      lineArr = line.strip().split(\'\\t\')
      dataset.append( [np.float32(lineArr[0]) , np.float32(lineArr[1])] )
      label.append(np.float32(lineArr[2]))
  return dataset , label
  pass

\'\'\'
select alpha2 randomly
\'\'\'
def selectAlphaTwo(i , m):
  \'\'\'
  :param i:
  :param m:
  :return:
  \'\'\'
  j = i
  while(j == i):
      j = int(random.uniform(0 , m))
  return j

def rangeSelectionForAlpha(aj , H , L):
  if aj &gt; H:
      aj = H
  if L &gt; aj:
      aj = L
  return aj
  pass

\'\'\'
calculate Ei
\'\'\'
def calculateEi(os , k):
  fxk = float(np.multiply(os.alphas, os.labels).T * (os.x * os.x[k, :].T)) + os.b
  Ek = fxk - float(os.labels[k])
  return Ek

\'\'\'
put the Ei into the cache when calculate Ei 
\'\'\'
def selectj(i , os , Ei):
  maxk = -1
  maxDeltaE = 0
  Ej = 0
  os.eCache[i] = [1 , Ei]
  validEachlist = np.nonzero(os.eCache[: , 0].A)[0]
  if (len(validEachlist) &gt; 1):
      for k in validEachlist:
          if k == i:
              continue
          Ek = calculateEi(os , k)
          deltaE = np.abs(Ei - Ek)
          if deltaE &gt; maxDeltaE:
              maxk = k
              maxDeltaE = deltaE
              Ej = Ek
      return maxk , Ej
      pass
  else:
      j = selectAlphaTwo(i , os.m)
      Ej = calculateEi(os , j)
  return j , Ej
  pass

\'\'\'
draw picture
\'\'\'
def drawDataset(data , label , x = None , y = None , line = True , alphas = None , kernel = True):
  index_one = []
  index_negative_one = []
  for i in range(100):
      if label[i] == 1:
          index_one.append(data[i])
      else:
          index_negative_one.append(data[i])
  index_one = np.matrix(index_one)
  index_negative_one = np.matrix(index_negative_one)
  plt.scatter(index_one[ : , 0].tolist() , index_one[: , 1].tolist() , c = \'r\' , marker=\'&lt;\' , label = \'class equal one\')
  plt.scatter(index_negative_one[: , 0].tolist() , index_negative_one[: , 1].tolist() , c = \'b\' , marker=\'x\' , label = \'class equal negative one\')
  if line == True:
      plt.plot(x , y)
      pass

  \'\'\'
  draw the support vector,the point which the α not equal zero
  \'\'\'
  if line == True or kernel == True:
      a1 = []
      for i in range(len(alphas)):
          a = alphas[i]
          if a != 0:
             a1.append(data[i])
      a1 =  np.matrix(a1)
      print(\'The number of the support vector : \' , len(a1))
      plt.scatter(a1[: , 0].tolist(),a1[: , 1].tolist(), s=150, c=\'none\', alpha=0.7,
                     linewidth=1.5, edgecolor=\'#AB3319\' , label = \'support vector\')

  plt.legend()
  plt.xlabel(\'X axis\')
  plt.ylabel(\'Y axis\')
  plt.show()

def updateEk(os,k):
  Ek = calculateEi(os,k)
  os.eCache[k]=[1,Ek]

if __name__ == \'__main__\':
  data , label = loadDataSet(\'../Data/testSetRBF.txt\')
  drawDataset(data , label , line=False ,kernel=False)


</code></pre>
<p>SMO算法唯一的一个类：</p>
<pre><code>import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import random
import KernelTransform
class optStruct:
  def __init__(self , dataMat , labels , C , toler):
      self.x = dataMat
      self.labels = labels
      self.C = C
      self.toler = toler
      self.m = np.shape(dataMat)[0]
      self.alphas = np.mat(np.zeros((self.m , 1)))
      self.b = 0
      self.eCache = np.mat(np.zeros((self.m , 2)))
      self.K = np.mat(np.zeros((self.m , self.m)))
      for i in range(self.m):
          self.K[: , i] = KernelTransform.kernelTrans(self.x , self.x[i , :] , kTup=(\'rbf\' , 1.2))
      pass

if __name__ == \'__main__\':
  os = optStruct([1,2] , [3,4] , 1,1)
  a = os.alphas.tolist()[0][0] -  os.alphas.tolist()[1][0]
  print(max(1.0 , a))


</code></pre>
<p>需要解释的应该只有selectj()了，这个是通过计算最大不长来选择α2的。<br>
首先我们假设最大不长是-1，因为相减有绝对值不可能是negative；os.eCache是我们的缓存的Ei，先把Ei存进去，1,表示这个数字不是0，这一步就是得到这个缓存里面所有有效（不为0）的Ei。判断得到的列表是不是有东西，没有就随机选择了。还是再解释一下为什么要这个建立表格吧！<br>
<strong>我们在选择第一个α1的时候，选择的是在边界外的点，也就是非边界的点。 优先选择遍历非边界数据样本，因为非边界数据样本更有可能需要调整，边界数据样本常常不能得到进一步调整而留在边界上。由于大部分数据样本都很明显不可能是支持向量，因此对应的α乘子一旦取得零值就无需再调整。遍历非边界数据样本并选出他们当中违反KKT 条件为止。当某一次遍历发现没有非边界数据样本得到调整时，遍历所有数据样本，以检验是否整个集合都满足KKT条件。如果整个集合的检验中又有数据样本被进一步进化，则有必要再遍历非边界数据样本。这样，不停地在遍历所有数据样本和遍历非边界数据样本之间切换，直到整个样本集合都满足KKT条件为止。以上用KKT条件对数据样本所做的检验都以达到一定精度ε就可以停止为条件。如果要求十分精确的输出算法，则往往不能很快收敛。所以在echa中缓存的第一次选出的α，因为我们选出来的就是非边界上的点，α2选择的时候继续在上面遍历，虽然缓存是存了Ei，但是这个Ei不能直接用，因为那个是旧的值。所以α的迭代策略就是非边界和全局选取两种交替进行了。</strong></p>
<p>之后就是正式的算法了：</p>
<pre><code>import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import random
import Tool
import smo_class
import KernelTransform
def innerL(i ,os):
  Ei = Tool.calculateEi(os , i)
  if ((os.labels[i]*Ei &lt; -os.toler) and
      (os.alphas[i] &lt; os.C)) or ((os.labels[i]*Ei &gt; os.toler) and
                                 (os.alphas[i] &gt; 0)):
      j , Ej = Tool.selectj(i , os , Ei)
      alphaIold = os.alphas[i].copy()
      alphaJold = os.alphas[j].copy()
      if (os.labels[i] != os.labels[j]):
          L = max(0 , os.alphas[j] - os.alphas[i])
          H = min(os.C , os.C + np.array(os.alphas)[j] - np.array(os.alphas)[i])
      else:
          L = max(0 , os.alphas[j] + os.alphas[i] - os.C)
          H = min(os.C , np.array(os.alphas)[j] + np.array(os.alphas)[i])
      if L == H:
          return 0
      eta = 2.0*os.x[i,:]*os.x[j,:].T - os.x[i,:]*os.x[i,:].T - os.x[j,:]*os.x[j,:].T
      if eta &gt;= 0:
          print(\'η&gt; 0，the kernel matrix is not semi-positive definite\')
          return 0
      os.alphas[j] -= os.labels[j]*(Ei - Ej)/eta
      os.alphas[j] = Tool.rangeSelectionForAlpha(os.alphas[j] , H , L)
      Tool.updateEk(os , j)

      if (abs(os.alphas[j] - alphaJold) &lt; 0.00001):
          print(\"j not moving enough\")
          return 0
      os.alphas[i] += os.labels[j] * os.labels[i] * (alphaJold - os.alphas[j])
      Tool.updateEk(os , i)
      b1 = os.b - Ei - os.labels[i] * (os.alphas[i] - alphaIold) * \\
           os.x[i, :] * os.x[i, :].T - os.labels[j] * \\
           (os.alphas[j] - alphaJold) * os.x[i, :] * os.x[j, :].T
      b2 = os.b - Ej - os.labels[i] * (os.alphas[i] - alphaIold) * \\
           os.x[i, :] * os.x[j, :].T - os.labels[j] * \\
           (os.alphas[j] - alphaJold) * os.x[j, :] * os.x[j, :].T
      if (0 &lt; os.alphas[i]) and (os.C &gt; os.alphas[i]):
          os.b = b1
      elif (0 &lt; os.alphas[j]) and (os.C &gt; os.alphas[j]):
          os.b = b2
      else:
          os.b = (b1 + b2) / 2.0
      return 1
  else:
      return 0

def smo(data,labels,C = 0.6,toler = 0.001,maxIter = 40 , kernel = True):
  oS = smo_class.optStruct(np.mat(data),np.mat(labels).transpose(),C,toler)
  iter =0
  entireSet  = True
  alphaPairsChanged = 0
  while(iter &lt; maxIter) and ((alphaPairsChanged &gt;0) or (entireSet)):
      alphaPairsChanged = 0
      if entireSet:
          for i in range(oS.m):
              if kernel == True:
                  alphaPairsChanged += KernelTransform.innerL(i,oS)
              else:
                  alphaPairsChanged += innerL(i, oS)
          print(\"fullSet,iter: %d i: %d,pairs changed %d\" %\\
              (iter,i,alphaPairsChanged))
          iter +=1
      else:
          # 两个元素乘积非零，每两个元素做乘法[0,1,1,0,0]*[1,1,0,1,0]=[0,1,0,0,0]
          nonBoundIs = np.nonzero((oS.alphas.A &gt; 0)*(oS.alphas.A &lt; C))[0]
          for i in nonBoundIs:
              alphaPairsChanged += innerL(i,oS)
              print(\"nou-bound,iter: %d i:%d,pairs changed %d\" % (iter,i,alphaPairsChanged))
          iter +=1
      # entireSet 控制交替的策略选择
      if entireSet:
          entireSet = False
      # 必须有alpha对进行更新
      elif(alphaPairsChanged == 0):
          entireSet = True
      print(\"iteration number：%d\" % iter)
  return oS.b,oS.alphas
</code></pre>
<p>entireSet就是交换策略的标志。貌似没有什么好说的。<br>
之后就是执行函数这些了：</p>
<pre><code>import Tool
import SMO
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import KernelTransform
\'\'\'
calculate w and draw the picture,
the variable which the α not equal zero , 
we call support vector
\'\'\'
def calculateW(alphas , data , labels):
  x = np.mat(data)
  label = np.mat(labels).transpose()
  m , n = np.shape(x)
  w = np.zeros((n , 1))
  for i in range(m):
      w += np.multiply(alphas[i] * label[i] , x[i , :].T)
  return w
  pass

if __name__ == \'__main__\':
  data, label = Tool.loadDataSet(\'../Data/testSet.txt\')
  b,alphas = SMO.smo(data , label , kernel=False)
  w = calculateW(alphas , data , label)
  x = np.arange(0 , 11)
  print(w)
  y = (-b - w[0]*x)/w[1]
  Tool.drawDataset(data , label , x , y.tolist()[0] , line=True , alphas=alphas)

  data, label = Tool.loadDataSet(\'../Data/testSetRBF.txt\')
  b, alphas = SMO.smo(data, label,kernel=True ,maxIter=100)
  svInd = np.nonzero(alphas.A &gt; 0)[0]
  Tool.drawDataset(data, label,  line=False, alphas=alphas)







</code></pre>
<p>有一个是kernel function的，先不用管。<br>
效果：</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 698px; max-height: 480px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 68.77%;\"></div>
<div class=\"image-view\" data-width=\"698\" data-height=\"480\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-4d11f00b9d677ec9.png\" data-original-width=\"698\" data-original-height=\"480\" data-original-format=\"image/png\" data-original-filesize=\"44574\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
<p>圈起来的是支持向量点，好很多了。</p>

<h2>⑩算法实现——version 3</h2>
<p>kernel function加上，先看看原来的数据：</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 698px; max-height: 477px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 68.34%;\"></div>
<div class=\"image-view\" data-width=\"698\" data-height=\"477\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-ec5eb06a66835ef1.png\" data-original-width=\"698\" data-original-height=\"477\" data-original-format=\"image/png\" data-original-filesize=\"34093\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>需要改的其实就是內积就可以了，到处看看哪里有內积就改改他，修改过后的innel和smo：</p>
<pre><code>import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import Tool
def kernelTrans(X,A,kTup):
  m,n = np.shape(X)
  K = np.mat(np.zeros((m,1)))
  if kTup[0]==\'lin\':
      K = X*A.T
  elif kTup[0] ==\'rbf\':
      for j in range(m):
          deltRow = X[j,:]-A
          K[j] = deltRow*deltRow.T
      K = np.exp(K/(-1*kTup[1]**2))
  return K

\'\'\'
update the innel function
\'\'\'
def innerL(i ,os):
  Ei = calculateEi(os , i)
  if ((os.labels[i]*Ei &lt; -os.toler) and
      (os.alphas[i] &lt; os.C)) or ((os.labels[i]*Ei &gt; os.toler) and
                                 (os.alphas[i] &gt; 0)):
      j , Ej = Tool.selectj(i , os , Ei)
      alphaIold = os.alphas[i].copy()
      alphaJold = os.alphas[j].copy()
      if (os.labels[i] != os.labels[j]):
          L = max(0 , os.alphas[j] - os.alphas[i])
          H = min(os.C , os.C + np.array(os.alphas)[j] - np.array(os.alphas)[i])
      else:
          L = max(0 , os.alphas[j] + os.alphas[i] - os.C)
          H = min(os.C , np.array(os.alphas)[j] + np.array(os.alphas)[i])
      if L == H:
          return 0
      eta = 2.0 * os.K[i, j] - os.K[i, i] - os.K[j, j]
      if eta &gt;= 0:
          print(\'η&gt; 0，the kernel matrix is not semi-positive definite\')
          return 0
      os.alphas[j] -= os.labels[j]*(Ei - Ej)/eta
      os.alphas[j] = Tool.rangeSelectionForAlpha(os.alphas[j] , H , L)
      updateEk(os , j)

      if (abs(os.alphas[j] - alphaJold) &lt; 0.00001):
          print(\"j not moving enough\")
          return 0
      os.alphas[i] += os.labels[j] * os.labels[i] * (alphaJold - os.alphas[j])
      updateEk(os , i)
      b1 = os.b - Ei - os.labels[i] * (os.alphas[i] - alphaIold) * \\
           os.K[i , i] - os.labels[j] * \\
           (os.alphas[j] - alphaJold) *  os.K[i , j]
      b2 = os.b - Ej - os.labels[i] * (os.alphas[i] - alphaIold) * \\
           os.K[i , j] - os.labels[j] * \\
           (os.alphas[j] - alphaJold) * os.K[j , j]
      if (0 &lt; os.alphas[i]) and (os.C &gt; os.alphas[i]):
          os.b = b1
      elif (0 &lt; os.alphas[j]) and (os.C &gt; os.alphas[j]):
          os.b = b2
      else:
          os.b = (b1 + b2) / 2.0
      return 1
  else:
      return 0

\'\'\'
updata the Ei
\'\'\'
def calculateEi(os , k):
  fxk = float(np.multiply(os.alphas, os.labels).T * os.K[:, k] + os.b)
  Ek = fxk - float(os.labels[k])
  return Ek
def updateEk(os,k):
  Ek = calculateEi(os,k)
  os.eCache[k]=[1,Ek]
</code></pre>
<p>刚刚那个执行函数其实已经包括了kernel的，所以直接就可以看到效果了：<br>
</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 482px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 67.04%;\"></div>
<div class=\"image-view\" data-width=\"719\" data-height=\"482\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/10624272-6abc9d6ab502f652.png\" data-original-width=\"719\" data-original-height=\"482\" data-original-format=\"image/png\" data-original-filesize=\"55608\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
用的是Gaussion kernel，不知道怎么做拟合，就把支持向量点圈出来就好了。<br>
最后附上所有代码GitHub：<br>
<a href=\"https://github.com/GreenArrow2017/MachineLearning\" target=\"_blank\" rel=\"nofollow\">https://github.com/GreenArrow2017/MachineLearning</a><p></p>

          </div>','1531183691'),
('325563','{975951}{1124052}{1124053}{62}{975714}{2720}{39}{2727}{2349}{364766}{1740}{137634}{190}{975721}{1903}{975755}{2577}{1124055}{158}{2686}{36}{1912}{975728}{1344}{6167}{4921}{72}{110884}{5549}{1277}{975757}{884}{1190}{7678}{975768}{30322}{6534}{1124062}{1698}{126389}{133062}{2229}{2714}{975779}{975913}{975899}{976193}{975741}{1747}{8080}','95）下的mAP。但是置信度却在不同模型会差异较大，可能在我的模型中置信度采用0.eps) 这里最终得到一系列的precision和recall值，并且这些值是按照置信度降低排列统计的，可以认为是取不同的置信度阈值（或者rank值）得到的。然后据此可以计算AP： def voc_ap(rec, prec, use_07_metric=False): \"\"\"Compute VOC AP given precision and recall. If use_07_metric is true, uses the VOC 07 11-point method (default:False).\"','最完整的检测模型评估指标mAP计算指南(附代码)在这里！','<div class=\"show-content-free\">
            <h1>前言</h1>
<p>对于使用机器学习解决的大多数常见问题，通常有多种可用的模型。每个模型都有自己的独特之处，并随因素变化而表现不同每个模型在<strong>“验证/测试”数据集</strong>上来评估性能，性能衡量使用各种统计量如<strong>准确度</strong>（accuracy），<strong>精度</strong>（precision），<strong>召回率</strong>（recall）等。选择的统计量通常针对特定应用场景和用例。 对于每个应用场景，选择一个能够客观比较模型的度量指标非常重要。</p>
<p>这篇文章将介绍目标检测（Object Detection）问题中的最常用评估指标-<em>Mean Average Precision</em>，即mAP。</p>
<p>大多数时候，这些指标很容易理解和计算。例如，在二元分类中，精确度和召回率是一个一个简单直观的统计量。然而，目标检测是一个非常不同且有趣的问题。即使你的目标检测器在图片中检测到猫，但如果你无法定位，它也没有用处。由于你要预测的是图像中各个物体是否出现及其位置，如何计算mAP将非常有趣。</p>
<p>在讲解mAP之前，我们先定义目标检测问题。</p>
<h1><strong>目标检测问题</strong></h1>
<p>在目标检测问题中，给定一个图像，找到它所包含的物体，找到它们的位置并对它们进行分类。目标检测模型通常是在一组特定的类集合上进行训练的，所以模型只会定位和分类图像中的那些类。另外，对象的位置通常采用矩形边界框表示。因此，目标检测涉及图像中物体的定位和分类。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 353px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 44.13%;\"></div>
<div class=\"image-view\" data-width=\"800\" data-height=\"353\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-174aa753df41400f\" data-original-width=\"800\" data-original-height=\"353\" data-original-format=\"image/jpeg\" data-original-filesize=\"54369\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>下面所述的Mean Average Precision特别适用于同时预测物体位置及类别的算法。 因此，从图1可以看出，它对评估定位模型、目标检测模型和分割模型非常有用。</p>
<h1><strong>评估目标检测模型</strong></h1>
<p><strong>1. 为什么是mAP?</strong></p>
<p>目标检测问题中的每个图片都可能包含一些不同类别的物体。如前所述，需要评估模型的物体分类和定位性能。因此，用于图像分类问题的标准指标precision不能直接应用于此。 这就是为什么需要mAP。 我希望读完这篇文章后，你将能够理解它的含义。</p>
<p><strong>2. 关于Ground Truth</strong></p>
<p>对于任何算法，评估指标需要知道ground truth（真实标签）数据。 我们只知道训练、验证和测试数据集的ground truth。对于目标检测问题，ground truth包括图像中物体的类别以及该图像中每个物体的真实边界框。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 640px; max-height: 424px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 66.25%;\"></div>
<div class=\"image-view\" data-width=\"640\" data-height=\"424\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-8dd82a4b46b14400\" data-original-width=\"640\" data-original-height=\"424\" data-original-format=\"image/jpeg\" data-original-filesize=\"60866\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>这里给出了一个实际图片（jpg、png等格式），以及相应的文本注释（边界框坐标(x, y, w, h)和类别），如图中红色框以及文本标签所示。</p>
<p>对于这个特殊例子，模型在训练时需要原始的图片：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 640px; max-height: 424px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 66.25%;\"></div>
<div class=\"image-view\" data-width=\"640\" data-height=\"424\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-d7e61e355b987036\" data-original-width=\"640\" data-original-height=\"424\" data-original-format=\"image/jpeg\" data-original-filesize=\"58238\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>以及ground truth的3个坐标及类别（这里假定图片大小是1000x800px，所有的坐标值都是以像素为单位的近似值）：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 184px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 19.45%;\"></div>
<div class=\"image-view\" data-width=\"946\" data-height=\"184\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-0dfbf34486ffbf2e\" data-original-width=\"946\" data-original-height=\"184\" data-original-format=\"image/png\" data-original-filesize=\"17212\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>下面让我们动一下手，去看如何计算mAP。这里我们不谈论不同的目标检测算法，假定我们已经有了一个训练好的模型，现在只需要在验证集上评估其性能。</p>
<p><strong>03</strong></p>
<h1><strong>mAP含义及计算</strong></h1>
<p>前面展示了原始图像和以及对应的ground truth。训练集和验证集中所有图像都以此方式标注。</p>
<p>训练好的目标检测模型会给出大量的预测结果，但是其中大多数的预测值都会有非常低的置信度（confidence score），因此我们只考虑那些置信度高于某个阈值的预测结果。</p>
<p>将原始图片送入训练好的模型，在经过置信度阈值筛选之后，目标检测算法给出带有边界框的预测结果：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 640px; max-height: 424px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 66.25%;\"></div>
<div class=\"image-view\" data-width=\"640\" data-height=\"424\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-18b947d1aa7a9b5c\" data-original-width=\"640\" data-original-height=\"424\" data-original-format=\"image/jpeg\" data-original-filesize=\"61625\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>现在，由于我们人类是目标检测专家，我们可以知道这些检测结果大致正确。但我们如何量化呢？我们首先需要判断每个检测的正确性。这里采用IoU（Intersection over Union），它可以作为评价边界框正确性的度量指标。 这是一个非常简单的指标。从名称看，有些人会发现这个名字是自解释的，但我们需要更好的解释。这里会以简短的方式解释IoU，如果想深入理解，可以参考Adrian Rosebrock的这篇文章(Intersection over Union (IoU) for object detection)。</p>
<p><strong>1. IoU</strong></p>
<p>IoU是预测框与ground truth的交集和并集的比值。这个量也被称为Jaccard指数，并于20世纪初由Paul Jaccard首次提出。为了得到交集和并集，我们首先将预测框与ground truth放在一起，如图所示。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 640px; max-height: 424px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 66.25%;\"></div>
<div class=\"image-view\" data-width=\"640\" data-height=\"424\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-9bb0cde6be5eecee\" data-original-width=\"640\" data-original-height=\"424\" data-original-format=\"image/jpeg\" data-original-filesize=\"63831\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>对于每个类，预测框和ground truth重叠的区域是交集，而横跨的总区域就是并集。其中horse类的交集和并集如下图所示（这个例子交集比较大）：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 598px; max-height: 646px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 108.03%;\"></div>
<div class=\"image-view\" data-width=\"598\" data-height=\"646\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-6f8728be805c26ff\" data-original-width=\"598\" data-original-height=\"646\" data-original-format=\"image/jpeg\" data-original-filesize=\"69922\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>其中蓝绿色部分是交集，而并集还包括橘色的部分。那么，IoU可以如下计算：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 422px; max-height: 391px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 92.65%;\"></div>
<div class=\"image-view\" data-width=\"422\" data-height=\"391\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-394dcc01deaaa825\" data-original-width=\"422\" data-original-height=\"391\" data-original-format=\"image/png\" data-original-filesize=\"5958\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p><strong>2. 鉴别正确的检测结果并计算precision和recall</strong></p>
<p>为了计算precision和recall，与所有机器学习问题一样，我们必须鉴别出True Positives（真正例）、False Positives（假正例）、True Negatives（真负例）和 False Negatives（假负例）。</p>
<p>为了获得True Positives and False Positives，我们需要使用IoU。计算IoU，我们从而确定一个检测结果（Positive）是正确的（True）还是错误的（False）。最常用的阈值是0.5，即如果IoU&gt; 0.5，则认为它是True Positive，否则认为是False Positive。而COCO数据集的评估指标建议对不同的IoU阈值进行计算，但为简单起见，我们这里仅讨论一个阈值0.5，这是PASCAL VOC数据集所用的指标。</p>
<p>为了计算Recall，我们需要Negatives的数量。由于图片中我们没有预测到物体的每个部分都被视为Negative，因此计算True Negatives比较难办。但是我们可以只计算False Negatives，即我们模型所漏检的物体。</p>
<p>另外一个需要考虑的因素是模型所给出的各个检测结果的置信度。通过改变置信度阈值，我们可以改变一个预测框是Positive还是 Negative，即改变预测值的正负性。基本上，阈值以上的所有预测（Box + Class）都被认为是Positives，并且低于该值的都是Negatives。</p>
<p>对于每一个图片，ground truth数据会给出该图片中各个类别的实际物体数量。我们可以计算每个Positive预测框与ground truth的IoU值，并取最大的IoU值，认为该预测框检测到了那个IoU最大的ground truth。然后根据IoU阈值，我们可以计算出一张图片中各个类别的正确检测值（True Positives, TP）数量以及错误检测值数量（False Positives, FP）。据此，可以计算出各个类别的precision：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 277px; max-height: 98px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 35.38%;\"></div>
<div class=\"image-view\" data-width=\"277\" data-height=\"98\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-46a41abe920ace44\" data-original-width=\"277\" data-original-height=\"98\" data-original-format=\"image/png\" data-original-filesize=\"2809\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>既然我们已经得到了正确的预测值数量（True Positives），也很容易计算出漏检的物体数（False Negatives, FN）。据此可以计算出Recall（其实分母可以用ground truth总数）：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 232px; max-height: 83px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 35.78%;\"></div>
<div class=\"image-view\" data-width=\"232\" data-height=\"83\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-c567e5b6f7870e37\" data-original-width=\"232\" data-original-height=\"83\" data-original-format=\"image/png\" data-original-filesize=\"2494\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p><strong>3. 计算mAP</strong></p>
<p>mAP这个术语有不同的定义。此度量指标通常用于信息检索和目标检测领域。然而这两个领域计算mAP的方式却不相同。这里我们只谈论目标检测中的mAP计算方法。</p>
<p>在目标检测中，mAP的定义首先出现在PASCAL Visual Objects Classes(VOC)竞赛中，这个大赛包含许多图像处理任务，详情可以参考这个<a href=\"http://homepages.inf.ed.ac.uk/ckiw/postscript/ijcv_voc09.pdf\" target=\"_blank\" rel=\"nofollow\">http://homepages.inf.ed.ac.uk/ckiw/postscript/ijcv_voc09.pdf</a>（里面包含各个比赛的介绍以及评估等）。</p>
<p>前面我们已经讲述了如何计算Precision和Recall，但是，正如前面所述，至少有两个变量会影响Precision和Recall，即IoU和置信度阈值。IoU是一个简单的几何度量，可以很容易标准化，比如在PASCAL VOC竞赛中采用的IoU阈值为0.5，而COCO竞赛中在计算mAP较复杂，其计算了一系列IoU阈值（0.05至0.95）下的mAP。但是置信度却在不同模型会差异较大，可能在我的模型中置信度采用0.5却等价于在其它模型中采用0.8置信度，这会导致precision-recall曲线变化。为此，PASCAL VOC组织者想到了一种方法来解决这个问题，即要采用一种可以用于任何模型的评估指标。在paper中，他们推荐使用如下方式计算Average Precision（AP）：</p>
<blockquote>
<p>For a given task and class, the precision/recall curve is computed from a method’s ranked output. Recall is defined as the proportion of all positive examples ranked above a given rank. Precision is the proportion of all examples above that rank which are from the positive class. The AP summarises the shape of the precision/recall curve, and is defined as the mean precision at a set of eleven equally spaced recall levels [0,0.1,…,1]:</p>
</blockquote>
<p>可以看到，为了得到precision-recall曲线，首先要对模型预测结果进行排序（ranked output，按照各个预测值置信度降序排列）。那么给定一个rank，Recall和Precision仅在高于该rank值的预测结果中计算，改变rank值会改变recall值。这里共选择11个不同的recall（[0, 0.1, ..., 0.9, 1.0]），可以认为是选择了11个rank，由于按照置信度排序，所以实际上等于选择了11个不同的置信度阈值。那么，AP就定义为在这11个recall下precision的平均值，其可以表征整个precision-recall曲线（曲线下面积）。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 353px; max-height: 81px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 22.95%;\"></div>
<div class=\"image-view\" data-width=\"353\" data-height=\"81\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-94019bc2d1d76ce4\" data-original-width=\"353\" data-original-height=\"81\" data-original-format=\"image/png\" data-original-filesize=\"9086\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>另外，在计算precision时采用一种插值方法（interpolate）：</p>
<blockquote>
<p>The precision at each recall level r is interpolated by taking the maximum precision measured for a method for which the corresponding recall exceeds r:The intention in interpolating the precision/recall curve in this way is to reduce the impact of the “wiggles” in the precision/recall curve, caused by small variations in the ranking of examples.</p>
</blockquote>
<p>及对于某个recall值r，precision值取所有recall&gt;r中的最大值（这样保证了p-r曲线是单调递减的，避免曲线出现摇摆）:</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 231px; max-height: 49px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 21.21%;\"></div>
<div class=\"image-view\" data-width=\"231\" data-height=\"49\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-794b54e18dc83730\" data-original-width=\"231\" data-original-height=\"49\" data-original-format=\"image/png\" data-original-filesize=\"7824\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>不过这里VOC数据集在2007年提出的mAP计算方法，而在2010之后却使用了所有数据点，而不是仅使用11个recall值来计算AP（详细参考这篇<a href=\"http://host.robots.ox.ac.uk/pascal/VOC/pubs/everingham15.pdf\" target=\"_blank\" rel=\"nofollow\">http://host.robots.ox.ac.uk/pascal/VOC/pubs/everingham15.pdf</a>）：</p>
<blockquote>
<p>Up until 2009 interpolated average precision (Salton and Mcgill 1986) was used to evaluate both classification and detection. However, from 2010 onwards the method of computing AP changed to use all data points rather than TREC-style sampling (which only sampled the monotonically decreasing curve at a fixed set of uniformly-spaced recall values 0, 0.1, 0.2,…, 1). The intention in interpolating the precision–recall curve was to reduce the impact of the ‘wiggles’ in the precision–recall curve, caused by small variations in the ranking of examples. However, the downside of this interpolation was that the evaluation was too crude to discriminate between the methods at low AP.</p>
</blockquote>
<p>对于各个类别，分别按照上述方式计算AP，取所有类别的AP平均值就是mAP。这就是在目标检测问题中mAP的计算方法。可能有时会发生些许变化，如COCO数据集采用的计算方式更严格，其计算了不同IoU阈值和物体大小下的AP（详情参考<a href=\"http://cocodataset.org/#detection-eval\" target=\"_blank\" rel=\"nofollow\">http://cocodataset.org/#detection-eval</a>）。</p>
<p>当比较mAP值，记住以下要点：</p>
<ul>
<li><p>mAP通常是在一个数据集上计算得到的。</p></li>
<li><p>虽然解释模型输出的绝对量化并不容易，但mAP作为一个相对较好的度量指标可以帮助我们。 当我们在流行的公共数据集上计算这个度量时，该度量可以很容易地用来比较目标检测问题的新旧方法。</p></li>
<li><p>根据训练数据中各个类的分布情况，mAP值可能在某些类（具有良好的训练数据）非常高，而其他类（具有较少/不良数据）却比较低。所以你的mAP可能是中等的，但是你的模型可能对某些类非常好，对某些类非常不好。因此，建议在分析模型结果时查看各个类的AP值。这些值也许暗示你需要添加更多的训练样本。</p></li>
</ul>
<p><strong>04</strong></p>
<p><strong>代码实现</strong></p>
<p>Facebook开源的Detectron包含VOC数据集的mAP计算(<a href=\"https://github.com/facebookresearch/Detectron/blob/05d04d3a024f0991339de45872d02f2f50669b3d/lib/datasets/voc_eval.py\" target=\"_blank\" rel=\"nofollow\">https://github.com/facebookresearch/Detectron/blob/05d04d3a024f0991339de45872d02f2f50669b3d/lib/datasets/voc_eval.py</a>)，这里贴出其核心实现，以对mAP的计算有更深入的理解。首先是precision和recall的计算：</p>
<pre><code># 按照置信度降序排序sorted_ind = np.argsort(-confidence)
BB = BB[sorted_ind, :]   # 预测框坐标image_ids = [image_ids[x] for x in sorted_ind] # 各个预测框的对应图片id# 便利预测框，并统计TPs和FPsnd = len(image_ids)
tp = np.zeros(nd)
fp = np.zeros(nd)for d in range(nd):
    R = class_recs[image_ids[d]]
    bb = BB[d, :].astype(float)
    ovmax = -np.inf
    BBGT = R[\'bbox\'].astype(float)  # ground truth

    if BBGT.size &gt; 0:        # 计算IoU
        # intersection
        ixmin = np.maximum(BBGT[:, 0], bb[0])
        iymin = np.maximum(BBGT[:, 1], bb[1])
        ixmax = np.minimum(BBGT[:, 2], bb[2])
        iymax = np.minimum(BBGT[:, 3], bb[3])
        iw = np.maximum(ixmax - ixmin + 1., 0.)
        ih = np.maximum(iymax - iymin + 1., 0.)
        inters = iw * ih        # union
        uni = ((bb[2] - bb[0] + 1.) * (bb[3] - bb[1] + 1.) +
               (BBGT[:, 2] - BBGT[:, 0] + 1.) *
               (BBGT[:, 3] - BBGT[:, 1] + 1.) - inters)

        overlaps = inters / uni
        ovmax = np.max(overlaps)
        jmax = np.argmax(overlaps)    # 取最大的IoU
    if ovmax &gt; ovthresh:  # 是否大于阈值
        if not R[\'difficult\'][jmax]:  # 非difficult物体
            if not R[\'det\'][jmax]:    # 未被检测
                tp[d] = 1.
                R[\'det\'][jmax] = 1    # 标记已被检测
            else:
                fp[d] = 1.
    else:
        fp[d] = 1.# 计算precision recallfp = np.cumsum(fp)
tp = np.cumsum(tp)
rec = tp / float(npos)# avoid divide by zero in case the first detection matches a difficult# ground truthprec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)
</code></pre>
<p>这里最终得到一系列的precision和recall值，并且这些值是按照置信度降低排列统计的，可以认为是取不同的置信度阈值（或者rank值）得到的。然后据此可以计算AP：</p>
<pre><code>def voc_ap(rec, prec, use_07_metric=False):
    \"\"\"Compute VOC AP given precision and recall. If use_07_metric is true, uses
    the VOC 07 11-point method (default:False).
    \"\"\"
    if use_07_metric:  # 使用07年方法
        # 11 个点
        ap = 0.
        for t in np.arange(0., 1.1, 0.1):            if np.sum(rec &gt;= t) == 0:
                p = 0
            else:
                p = np.max(prec[rec &gt;= t])  # 插值
            ap = ap + p / 11.
    else:  # 新方式，计算所有点
        # correct AP calculation
        # first append sentinel values at the end
        mrec = np.concatenate(([0.], rec, [1.]))
        mpre = np.concatenate(([0.], prec, [0.]))                # compute the precision 曲线值（也用了插值）
        for i in range(mpre.size - 1, 0, -1):
            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])                # to calculate area under PR curve, look for points
        # where X axis (recall) changes value
        i = np.where(mrec[1:] != mrec[:-1])[0]                # and sum (\\Delta recall) * prec
        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])    return ap
</code></pre>
<p>计算各个类别的AP值后，取平均值就可以得到最终的mAP值了。但是对于COCO数据集相对比较复杂，不过其提供了计算的API，感兴趣可以看一下<a href=\"https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocotools/cocoeval.py\" target=\"_blank\" rel=\"nofollow\">https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocotools/cocoeval.py</a>。</p>
<hr>
<blockquote>
<p>个人技术博客：<a href=\"https://blog.csdn.net/u013709270\" target=\"_blank\" rel=\"nofollow\">https://blog.csdn.net/u013709270</a><br>
微信：TonyJeemy520      加个人微信拉你进机器学习、深度学习交流群，请备注 : 来自简书<br>
QQ交流群：651616387   请备注 : 来自简书<br>
微信公众号：机器学习算法工程师   ----二维码见下图</p>
</blockquote>
<hr>
<p><strong>扫码关注微信号：机器学习算法工程师，更多干货分享， 或加个人微信，拉你进机器学习、深度学习交流群</strong><br>
</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 430px; max-height: 430px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 100.0%;\"></div>
<div class=\"image-view\" data-width=\"430\" data-height=\"430\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-0bdf004ac80c0197\" data-original-width=\"430\" data-original-height=\"430\" data-original-format=\"image/jpeg\" data-original-filesize=\"32906\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p></p>

          </div>','1531183694'),
('325564','{975728}{2430}{1090}{4057}{62}{3872}{1124071}{554}{975757}{293451}{36}{975919}{139}{459}{1530}{4447}{3576}{1912}{3536}{30184}{976193}{63}{703}{3826}{380}{3432}{50056}{3832}{975721}{778}{975876}{1743}{979369}{975880}{975768}{975878}{189}{976063}{975903}{1314}{9498}{975812}{27946}{11407}{44590}{1124073}{975773}{2535}{518998}{975798}','Kaggle比赛 (toxic comment classification) 1. Abstract 本次Kaggle比赛是做NLP的情感分类，要求我们将六种不同的情感分类找出来（toxic(恶意),severetoxic(穷凶极恶),obscene(猥琐),threat(恐吓),insult(侮辱)','Kaggle比赛 (toxic comment classification)','<div class=\"show-content-free\">
            <div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 700px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 100.0%;\"></div>
<div class=\"image-view\" data-width=\"1200\" data-height=\"1200\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6673934-1c21a1684c382551.png\" data-original-width=\"1200\" data-original-height=\"1200\" data-original-format=\"image/png\" data-original-filesize=\"263230\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<h1>1. Abstract</h1>
<p>本次Kaggle比赛是做NLP的情感分类，要求我们将六种不同的情感分类找出来（toxic(恶意),severetoxic(穷凶极恶),obscene(猥琐),threat(恐吓),insult(侮辱),identityhate(种族歧视)）而这些label并不是互斥的。这是我第一次参加的Data mining的比赛，误打误撞拿到了铜牌，算是个不错的成绩吧。虽然比赛结束了一段时间了，但我还是需要对之前比赛的工作做出一些总结，也为日后的实验工作累计经验。</p>
<h1>2. Main work</h1>
<h2>2.1 Data Clean &amp; Data analysis</h2>
<p>首先这个比赛当中，数据集看起来挺干净的（没有任何的na值），里面全是id，文本和各个label。但通过baseline模型分类找出分类错误的一些样本中，我们发现其实这些评论是很“脏”的。评论当中不仅仅是英文还有日文和一些他国语言，文本中还会掺杂一些html标签。因此在做一些文本处理是需要先清洗一下这些评论的。另外我们会发现评论中为了避免“文本过滤”，那些恶意评论都是FU****K，但显然这些都辱骂的词语，流露的情感都是toxic的。这也是NLP里面的一个问题oov(out of vocab)。因此我们需要对这些词语进行不抓和清洗。</p>
<h2>2.2 Feature Engineering</h2>
<p>在Feature  Engineering中，我们使用一下这些特征：</p>
<ol>
<li><p>TF-IDF：我们不仅仅针对word，而且还对char做。对于char而言可能存在一些常出现的字母组合，因为一些关键字通常会给评论者通过拼写错误来让系统过滤器混淆，使用char某种程度可以捕抓到这些字词。</p></li>
<li><p>统计特征：我们统计了一下comment的长度，标点符号的个数，评论词数与评论单词总数的比值等等。它用于补充了TF-IDF不足的东西。</p></li>
<li><p>trained word2vec：有两种word2vec，一种是GloVe，另一种是FastText。他们主要是用于做DeepLearning的使用使用的。</p></li>
</ol>
<h2>2.3 Model</h2>
<p>首先，我使用LR做一个baseline来精度如何。最后我们使用了NB-SVM，LGBM, LR, NN做了模型并融合。对于NN，我使用了BI-GRU，CNN，BI-GRU -&gt; CNN，发现Bi-GRU CNN单模型效果最好，但是在融合的时候并没有什么另外两个融合效果好。通常来说，RNN在做文本的效果比CNN好，主要是因为CNN会丢失到文本当中的时序信息。</p>
<h2>2.4 Other ticks</h2>
<p>我们在做数据分析的是否发现数据是及其不平衡的。我记得其中一个label只用400多个正样本，而其他10w+个都是负样本。所以data augmentation是十分重要的。有大佬使用了TTA，其实他就是把文本翻译成其他语言，使用不同的语言来做预测，这种做法提高了模型的泛化能力同时也增加了训练的正样本。</p>
<p>另外在比赛中我们发现了，训练集和测试集两者的分布有所不同。所以有些朋友使用pseudo-labelling来解决，pseudo-labelling是一种半监督训练方法，简单来说就是先对test集做预测，然后使用test集和部分的train集合做训练，用validation数据做模型评估，最后得到模型对test集做预测。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 629px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 87.36%;\"></div>
<div class=\"image-view\" data-width=\"720\" data-height=\"629\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6673934-e2f6348c7e879b2a.png\" data-original-width=\"720\" data-original-height=\"629\" data-original-format=\"image/png\" data-original-filesize=\"215822\"></div>
</div>
<div class=\"image-caption\">pseudo-labelling</div>
</div>
<h1>3 Other solution</h1>
<ul>
<li>35th 关于做详细做FE：<a href=\"https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52645\" target=\"_blank\" rel=\"nofollow\">https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52645</a>
</li>
<li>这个是关于FE的kernel（受其启发，加入一些统计feature）：<a href=\"https://www.kaggle.com/eikedehling/feature-engineering\" target=\"_blank\" rel=\"nofollow\">https://www.kaggle.com/eikedehling/feature-engineering</a>
</li>
<li>一般word2vec就仅仅对word，而加上char做deeplearning效果会更好，就像char tf-idf：<a href=\"https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52702\" target=\"_blank\" rel=\"nofollow\">https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52702</a>
</li>
<li>15th 通过使用BPEmb和Spell Correction来尽可能解决oov的问题：<a href=\"https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52563\" target=\"_blank\" rel=\"nofollow\">https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52563</a>
</li>
<li>对此比赛的总结：<a href=\"https://zhuanlan.zhihu.com/p/34899693\" target=\"_blank\" rel=\"nofollow\">https://zhuanlan.zhihu.com/p/34899693</a>
</li>
<li>一位大佬的做法和总结：<a href=\"https://zhuanlan.zhihu.com/p/34922134\" target=\"_blank\" rel=\"nofollow\">https://zhuanlan.zhihu.com/p/34922134</a>
</li>
</ul>
<h1>4 Summary</h1>
<p>这次是我第一次参加Kaggle比赛同时也是第一次做NLP项目。个人提升还是挺大的，因为这个比赛我认识了解了XGB和LightGBM这些模型武器。当然最重要的是在kernels上面不停的偷师抄代码。当然我发现自己对文本数据的敏感度不是特别够，如Fuck这类词出现的次数其实还是很多的，所以在tf-idf里面idf会将其权值降低。因此我们需要显式提取这类词语。另外我对评估features的重要性还是不够，因为feature过多会增加训练的时间复杂。我现在只会一些用cross valid和corr来评估哪些features是比较好的。另外我认为kaggle上面的比赛应该多看kernel和forum，因为里面真的给到很多建议我且讨论到一些很多关键点。</p>

          </div>','1531183694'),
('325565','{222}{234}{1124076}{975714}{2841}{39}{7465}{36}{408}{442}{975892}{453}{72}{247}{2152}{975951}{761}{378}{417}{1191}{975717}{975757}{785}{576}{2328}{975830}{74}{255}{975827}{3538}{955}{1968}{210}{975755}{975786}{600}{2787}{142}{975911}{975729}{975838}{976193}{2098}{6020}{251}{6003}{1124077}{6813}{2323}','池化窗大小是四维向量[1, height, width, 1]，batch和channels一般缺省为1；步长设为2，即将原尺寸的长和款各除以2；填','Fashion MNIST with Tensorflow + CNN','<div class=\"show-content-free\">
            <p>Fashion MNIST 是德国一家时尚公司提供的数据集，包含十个品类的七万中商品。其数据格式，图片尺寸，数据集大小都保持和手写数字 MNIST 一模一样，完全可以起到替代的作用，而且可以提升挑战难度和算法优化的空间。</p>
<p>之前已经尝试过不用 Tensorflow 自行实现梯度下降(<a href=\"https://www.jianshu.com/p/9386a0c99da2\" target=\"_blank\">手工打造神经网络: 透视分析</a>)，现在来看看用 Tensorflow 在 Fashion MNIST 数据集的尝试。关于 Tensorflow 的安装可以看我这篇文章: <a href=\"https://www.jianshu.com/p/65c67d103124\" target=\"_blank\">CUDA+cuDNN+Tensorflow-GPU Install</a></p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 700px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 100.0%;\"></div>
<div class=\"image-view\" data-width=\"840\" data-height=\"840\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/5274420-238dd120cdfeb4d0\" data-original-width=\"840\" data-original-height=\"840\" data-original-format=\"image/png\" data-original-filesize=\"385817\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>Tensorflow 官方教程有两篇以MNIST为例的资料, 入门篇跳过直接看深度卷积神经网络的实现 -  <a href=\"https://www.tensorflow.org/versions/r1.1/get_started/mnist/pros\" target=\"_blank\" rel=\"nofollow\">Deep MNIST for Experts</a>，官方教程中的代码不是那么好懂，可以像庖丁解牛一样把教程拆解开来一点点理解。</p>
<p>我们首先来看看CNN的三个重要组成部分: 卷积，池化和全连接层。</p>
<p><strong>卷积</strong><br>
卷积可以通过从输入的一小块数据中抽取图像的特征，并保留像素间的空间关系。美图秀秀的各种特效和滤镜其实就可以看做是卷积操作的实例。</p>
<p></p>
<p></p>
假设我们有一个5 x 5 的原始图像，它的像素值仅为 0 或者 1<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 127px; max-height: 115px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 90.55%;\"></div>
<div class=\"image-view\" data-width=\"127\" data-height=\"115\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/5274420-31854129d92447bd.png\" data-original-width=\"127\" data-original-height=\"115\" data-original-format=\"image/png\" data-original-filesize=\"7256\"></div>
</div>
<div class=\"image-caption\">image.png</div>
</div>
<p></p>
<p></p>
再设定一个 3 x 3 的矩阵作为卷积核<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 74px; max-height: 63px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 85.14%;\"></div>
<div class=\"image-view\" data-width=\"74\" data-height=\"63\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/5274420-8a1e46c9058f2247.png\" data-original-width=\"74\" data-original-height=\"63\" data-original-format=\"image/png\" data-original-filesize=\"2080\"></div>
</div>
<div class=\"image-caption\">image.png</div>
</div>
<p>卷积核也叫滤波器，通过在图像上滑动滤波器并计算点乘得到矩阵叫做卷积特征或者特征图。卷积特征由深度，步长和填充三个参数决定。如果卷积核尺寸过大，会导致提取图像的特征过于复杂，尺寸过小，难以表示有用的特征。实际应用中一般选取5x5或者7x7的最佳。卷积核是奇数的，这样就有了中心和半径的概念，也可以保证特征图的输入尺寸与输出尺寸一致。</p>

<p></p>
<p></p>
让卷积核在原始图像上滑动，在每个位置上计算对应元素的乘积，并把乘积的和作为最后的结果，得到输出矩阵（粉色）中的每一个元素的值。<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 268px; max-height: 196px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 73.13%;\"></div>
<div class=\"image-view\" data-width=\"268\" data-height=\"196\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/5274420-8b04ff6a57641f33.gif\" data-original-width=\"268\" data-original-height=\"196\" data-original-format=\"image/gif\" data-original-filesize=\"32176\"></div>
</div>
<div class=\"image-caption\">图 7</div>
</div>
<p><strong>池化</strong><br>
空间池化的主要目的是降维，在保持原有空间特征的基础上最大限度将数组的维度变小。空间池化有下面几种方式：最大化、平均化、加和等等，如最大池化就是在每个2x2的空间邻域取出最大值。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 494px; max-height: 421px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 85.22%;\"></div>
<div class=\"image-view\" data-width=\"494\" data-height=\"421\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/5274420-a811ba23c39dba67.png\" data-original-width=\"494\" data-original-height=\"421\" data-original-format=\"image/png\" data-original-filesize=\"77721\"></div>
</div>
<div class=\"image-caption\">image.png</div>
</div>我们可以对原始图片应用多个滤波器再分别池化，如下图所示:<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 319px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 42.65%;\"></div>
<div class=\"image-view\" data-width=\"748\" data-height=\"319\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/5274420-633f541cd257f772.png\" data-original-width=\"748\" data-original-height=\"319\" data-original-format=\"image/png\" data-original-filesize=\"98125\"></div>
</div>
<div class=\"image-caption\">image.png</div>
</div> 有时候我会想，难道 Adobe 就是世界上最早应用卷积神经网络来处理图片的公司吗?<p></p>
<p><strong>全连接</strong><br>
全连接层的目的是将卷积和池化层的输出的高级特征把输入图像基于训练数据集进行分类。对于MNIST这样的多分类任务来说当然还是选择 softmax 好了。</p>
<p><strong>实施步骤</strong></p>
<blockquote>
<p>Step 0: 获取数据，设置占位符，初始化变量并启动 Session<br>
这段简单代码就不解释了</p>
</blockquote>
<pre><code class=\"swift\">mnist = input_data.read_data_sets(\"d:/dev/fashion-mnist/\", one_hot=True)

sess = tf.InteractiveSession()
# 原始图片的shape为28x28, 单通道
x = tf.placeholder(\"float\", shape=[None, 784]) 
# 穿戴类的10个分类
y_ = tf.placeholder(\"float\", shape=[None, 10])

W = tf.Variable(tf.zeros([784,10]))
b = tf.Variable(tf.zeros([10]))

sess.run(tf.initialize_all_variables())
</code></pre>
<blockquote>
<p>Step 1: 初始化所有的滤波器，设置随机权重</p>
</blockquote>
<p>权重矩阵的形状和以前一样也是784x10, 但官方用到了 truncated_normal 来初始化权重，因为模型需要创建很多权重，在初始化时最好加入少量的噪声来打破对称性以及避免0梯度。</p>
<p>这个函数从截断的正态分布中输出随机值，截断的逻辑是当随机数与平均值的标准偏差大于两倍。</p>
<pre><code class=\"swift\">tf.truncated_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None) 
</code></pre>
<p>下面写段小代码对\"截断的正态分布\"做一个可视化展示。</p>
<pre><code class=\"swift\">import tensorflow as tf
import matplotlib.pyplot as plt

A = tf.truncated_normal([10000, 10])
with tf.Session() as sess:
    a = sess.run(A)

plt.hist(a, 100, (-3, 3));
</code></pre>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 373px; max-height: 249px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 66.75999999999999%;\"></div>
<div class=\"image-view\" data-width=\"373\" data-height=\"249\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/5274420-be8876b8393720e8.png\" data-original-width=\"373\" data-original-height=\"249\" data-original-format=\"image/png\" data-original-filesize=\"6901\"></div>
</div>
<div class=\"image-caption\">image.png</div>
</div>
<p>对于ReLU神经元，比较好的做法是用一个较小的正数来初始化偏置项，以避免神经元节点输出恒为零，这里用到的初始值是0.1</p>
<p>因为权重和偏置会设置多次，所以定义了两个函数复用</p>
<pre><code class=\"swift\">def weight_variable(shape):
  initial = tf.truncated_normal(shape, stddev=0.1)
  return tf.Variable(initial)

def bias_variable(shape):
  initial = tf.constant(0.1, shape=shape)
  return tf.Variable(initial)
</code></pre>
<blockquote>
<p>Step 2：以一张训练图像作为输入，通过前向传播过程（卷积，ReLU 和池化操作，以及全连接层的前向传播），找到各个类的输出概率。</p>
</blockquote>
<p>卷积和池化操作会调用多次，所以也定义了两个函数复用:<br>
卷积函数的四个参数分别是训练图像，卷积核，步长，填充。卷积核具有[filter_height, filter_width, in_channels, out_channels]这样的 shape，具体含义是[卷积核的高度，卷积核的宽度，图像通道数，卷积核个数], 它的第三维就是训练图形的第四维; 步长设为1; padding=\'SAME\' 表示卷积核扫描的时候可以停留在图像边缘，在矩阵周边会补一圈零，所以当步长为1时生成尺寸不变，如果这个参数为\'VALID\'则生成窄卷积，结果比原始图片小。</p>
<pre><code class=\"swift\">def conv2d(x, W):
  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=\'SAME\')
</code></pre>
<p>池化函数的四个参数分别是池化输入，池化窗大小，步长，填充。池化层一般在卷积层后面，这里的输入就是卷积输出的特征图，具有[batch, height, width, channels]这样的shape; 池化窗大小是四维向量[1, height, width, 1]，batch和channels一般缺省为1；步长设为2，即将原尺寸的长和款各除以2；填充参数和卷积函数一样。</p>
<pre><code class=\"swift\"># 求最大值池化，长宽缩小一半
def max_pool_2x2(x):
  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\'SAME\')
</code></pre>
<p>2.1 第一次卷积和池化操作</p>
<pre><code class=\"swift\"># 卷积核为5x5矩阵，in_channels 1, out_channels 32[提取32个特征]
W_conv1 = weight_variable([5, 5, 1, 32])
b_conv1 = bias_variable([32])

# 原始图片转为4维tensor满足卷积和池化要求
x_image = tf.reshape(x, [-1,28,28,1])

# 卷积后RELU, output size 28x28x32
h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)
# 池化后 output size 14x14x32
h_pool1 = max_pool_2x2(h_conv1)
</code></pre>
<p>2.2 第二次卷积和池化操作</p>
<pre><code class=\"swift\">#卷积核为5x5矩阵，in_channels 32, out_channels 64[提取64个特征]
W_conv2 = weight_variable([5, 5, 32, 64])
b_conv2 = bias_variable([64])

# 卷积后RELU, output size 14x14x64
h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)
# 池化后 output size 7x7x64
h_pool2 = max_pool_2x2(h_conv2)
</code></pre>
<p>2.3 全连接层前向传播</p>
<pre><code class=\"swift\"># 隐藏层1024个神经元
W_fc1 = weight_variable([7 * 7 * 64, 1024])
b_fc1 = bias_variable([1024])

# 4维张量转2维张量，第一维是样本数，第二维是神经元个数3136个
h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])
h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)

# 随机关闭一些神经元防止过拟
keep_prob = tf.placeholder(\"float\")
h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)

# 从1024个神经元映射到10个神经元
W_fc2 = weight_variable([1024, 10])
b_fc2 = bias_variable([10])

y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)
</code></pre>
<p>Dropout 预防过拟合，随机将x矩阵中一部分元素变为零，剩下的变成原值的 “1/keep_prob” 倍。</p>
<pre><code class=\"swift\">tf.nn.dropout(x, keep_prob, noise_shape=None, seed=None, name=None)
</code></pre>
<blockquote>
<p>Step 3: 在输出层计算总误差，执行计算图<br>
这里的损失函数是目标类别和预测类别之间的交叉熵，训练优化器用的是AdamOptimizer，在代码之后会讲。</p>
</blockquote>
<pre><code class=\"swift\"># 计算交叉熵损失
cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))
# 创建优化器
train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)
#计算准确率， tf.argmax函数 在 label 中找出数值最大的那个元素的下标
correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))
sess.run(tf.initialize_all_variables())
</code></pre>
<p>Tensorflow 中所有的<strong>优化器</strong>实现都是基于tf.train.Optimizer这个基类的，常用的有以下这些实现</p>
<p><em>GradientDescentOptimizer<br>
AdagradOptimizer<br>
AdagradDAOptimizer<br>
MomentumOptimizer<br>
AdamOptimizer<br>
FtrlOptimizer<br>
RMSPropOptimizer</em></p>
<p>先挑两个来简单描述，要了解细节可以翻墙看这篇文章 <a href=\"http://sebastianruder.com/optimizing-gradient-descent/\" target=\"_blank\" rel=\"nofollow\">Optimizing gradient descent</a>，对各种优化器的实现原理和性能详细说明。</p>
<p><strong>tf.train.GradientDescentOptimizer</strong></p>
<p>将梯度下降算法进行了封装，tensorflow里的实现应该是随机下降SGD，构造函数只要给个学习率就行了。</p>
<pre><code class=\"swift\">tf.train.GradientDescentOptimizer.__init__(learning_rate, use_locking=False,name=’GradientDescent’)
</code></pre>
<p><strong>tf.train.AdamOptimizer</strong></p>
<p>寻找全局最优点的优化算法，引入了二次方梯度校正。相比于基础SGD算法不容易陷于局部优点且速度更快。</p>
<pre><code class=\"swift\">tf.train.AdamOptimizer.__init__(learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, use_locking=False, name=’Adam’)
</code></pre>
<blockquote>
<p>Step 4: 开始训练<br>
循环两万次，每次加载50个样本，每循环100次打印一次结果</p>
</blockquote>
<pre><code class=\"swift\">for i in range(20000):
  batch = mnist.train.next_batch(50)
  if (i%100 == 0):
    train_accuracy = accuracy.eval(feed_dict={
        x:batch[0], y_: batch[1], keep_prob: 1.0})
    print (\"step %d, training accuracy %g\"%(i, train_accuracy))
  # 运行训练模型
  train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})

print (\"test accuracy %g\"%accuracy.eval(feed_dict={
    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))
</code></pre>
<p></p>
<p></p>
代码到这里就结束了，以上所有步骤可视化展示如下: (这里依然借用数字MNIST的例子)<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 424px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 56.68%;\"></div>
<div class=\"image-view\" data-width=\"748\" data-height=\"424\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/5274420-45ac929cfe887d5b.png\" data-original-width=\"748\" data-original-height=\"424\" data-original-format=\"image/png\" data-original-filesize=\"310907\"></div>
</div>
<div class=\"image-caption\">image.png</div>
</div>
<p></p>
<p></p>
最后看下训练结果，经过两万次循环后的准确率是91%，如果是数字MNIST达到99%不是难事，对于Fashion-MNIST则挑战大很多，但是调参以后也应该要达到95%的水平。这段代码里其实有很多参数可以调整，权重的初始值，不同的激活函数，卷积核大小，池化窗口大小，优化器的选择，卷积和池化的次数，dropout比例，隐藏层的个数，神经元的个数等等，不过这东西就像玄学一样大家都是凭感觉去试试，所以有人说训练的过程就是50%时间用来调参，49%时间用来对抗过拟/欠拟，最后1%的时间用来修改网上拷贝来的代码 :)<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 250px; max-height: 497px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 198.8%;\"></div>
<div class=\"image-view\" data-width=\"250\" data-height=\"497\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/5274420-a150c67c0a120e8a.png\" data-original-width=\"250\" data-original-height=\"497\" data-original-format=\"image/png\" data-original-filesize=\"21980\"></div>
</div>
<div class=\"image-caption\">image.png</div>
</div>
<p>下一次我会继续写如何调参优化提升训练的准确率，如果能达到95%水平的话。</p>
<p><strong><em>References:</em></strong><br>
<em><a href=\"https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/\" target=\"_blank\" rel=\"nofollow\">An Intuitive Explanation of Convolutional Neural Networks</a></em> - by <a href=\"https://ujjwalkarn.me/author/ujwlkarn/\" target=\"_blank\" rel=\"nofollow\">ujjwalkarn</a><br>
<em><a href=\"https://www.tensorflow.org/versions/r1.1/get_started/mnist/pros\" target=\"_blank\" rel=\"nofollow\">Deep MNIST for Experts</a></em><br>
<em><a href=\"https://blog.csdn.net/mao_xiao_feng/article/details/53453926\" target=\"_blank\" rel=\"nofollow\">【TensorFlow】tf.nn.max_pool实现池化操作</a></em> - by <a href=\"https://blog.csdn.net/mao_xiao_feng\" target=\"_blank\" rel=\"nofollow\">xf__mao</a><br>
<em><a href=\"https://www.cnblogs.com/welhzh/p/6607581.html\" target=\"_blank\" rel=\"nofollow\">【TensorFlow】tf.nn.conv2d是怎样实现卷积的？</a></em>- by 楼里打扫</p>

          </div>','1531183696'),
('325566','{2661}{1124082}{1124083}{975724}{975757}{17}{9073}{72}{975714}{975776}{72851}{975703}{1124084}{126}{1124085}{1124086}{112135}{39}{30885}{1124087}{1124088}{975843}{975915}{1352}{408}{976136}{975727}{884}{975935}{975728}{975904}{1642}{62}{975768}{530}{1825}{976042}{63}{818}{4658}{19982}{3307}{975755}{778}{1328}{975919}{296}{1648}{442}{724}','θ) 1.2 contrastive loss 首先介绍一下正样本和负样本的概念。 正样本：物品q和p是相似的，通常被标注为y=1，那么我们称这样的训练数据对(p,q)是positive pair; 如图，我们定义一个边界值Margin(m)来判定两个物品是否相似，例如对于图中的p与q被标注为正样本，在图中体现在两者的特征值会比较相近(圆内);','论文 | 图像检索经典论文解读《Learning visual similarity for pro','<div class=\"show-content-free\">
            <h1>一 写在前面</h1>
<p>最近想尝试一下CHINA-MM 2018中的京东AI挑战赛，其中的一个子任务就是单品搜索，其实就是图像内容检索任务。</p>
<p>因为之前并没有接触过这一块任务，所以找了一些资料学习（其实都是受惠于导师与师兄ω嘻嘻），分享出来给有兴趣的同学：</p>
<ul>
<li><a href=\"https://www.jianshu.com/p/ebf6fbdb44e9\" target=\"_blank\">[TPAMI重磅综述]SIFT与CNN的碰撞：万字长文回顾图像检索任务十年探索历程（上篇）</a></li>
<li><a href=\"https://www.jianshu.com/p/cda7aec5ff0c\" target=\"_blank\">[TPAMI重磅综述]SIFT与CNN的碰撞：万字长文回顾图像检索任务十年探索历程（下篇）</a></li>
<li>论文：<a href=\"http://xueshu.baidu.com/s?wd=paperuri%3A%28859efbbc3e0698aa3da75d747d4ccfc3%29&amp;filter=sc_long_sign&amp;tn=SE_xueshusource_2kduw22v&amp;sc_vurl=http%3A%2F%2Fdl.acm.org%2Fcitation.cfm%3Fid%3D2809654.2766959&amp;ie=utf-8&amp;sc_us=16474410094205442206\" target=\"_blank\" rel=\"nofollow\">Learning visual similarity for product design with convolutional neural networks</a>
</li>
<li>论文：<a href=\"https://arxiv.org/abs/1503.03832\" target=\"_blank\" rel=\"nofollow\">FaceNet: A Unified Embedding for Face Recognition and Clustering</a>
</li>
</ul>
<p>简单说明一下，前两篇是图像检索方面的综述文章，总结的非常的好，对于刚刚接触这个领域的人真的很有必要阅读，以便于形成整个概念~</p>
<p>了解基本概念之后，其实脑子里就会有一个粗略的网络模型，这个时候再去看几篇具体的论文是非常恰到好处的~</p>
<h1>二 文章简介</h1>
<blockquote>
<p>本文主要是对<a href=\"http://xueshu.baidu.com/s?wd=paperuri%3A%28859efbbc3e0698aa3da75d747d4ccfc3%29&amp;filter=sc_long_sign&amp;tn=SE_xueshusource_2kduw22v&amp;sc_vurl=http%3A%2F%2Fdl.acm.org%2Fcitation.cfm%3Fid%3D2809654.2766959&amp;ie=utf-8&amp;sc_us=16474410094205442206\" target=\"_blank\" rel=\"nofollow\">Learning visual similarity for product design with convolutional neural networks</a>这篇文章的总结，整理了自己觉得最重要的部分，也作为学习记录，至于对具体的训练细节以及实验部分感兴趣的同学还请阅读原文哦~~~</p>
</blockquote>
<h2>1 整体介绍</h2>
<p>其实通过文章名就可以知道文章研究的是用卷积神经网络学习产品相似性问题。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 292px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 36.449999999999996%;\"></div>
<div class=\"image-view\" data-width=\"801\" data-height=\"292\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/9933353-7cd23d28bd44ec7a.png\" data-original-width=\"801\" data-original-height=\"292\" data-original-format=\"image/png\" data-original-filesize=\"209930\"></div>
</div>
<div class=\"image-caption\">图1</div>
</div>
<p>如图，文章解决了以下两个问题：</p>
<ol>
<li>Query1：对于一张给定区域的图像，找出与这个区域中包含物品最相似的其他物品；</li>
<li>Query2：对于一个物品，找到包含相似物品的室内设计图</li>
</ol>
<p>这篇文章针对的具体场景是室内设计中物品与具体场景设计。具体来说Query1可以解决的问题是人们常常会在装修网站询问“这张图片中的台灯挺好的，从哪里可以买到相似的？”；而Query2可以解决的问题是“这条椅子可以摆放房间的哪里？”</p>
<p>在给定物品的区域框的情况下，其实我觉得<strong>两个问题的本质都在于学习两个物品的相似度</strong>，当然后期利用相似度我们可以很自然的完成检索任务。</p>
<h2>2 文章主要贡献</h2>
<p>这篇文章是2015年发在<em>ACM Transactions on Graphics</em>上面的文章，在图像检索还是蛮有代表性的，以下为作者自己总结的几点主要贡献：</p>
<ol>
<li>生成物品与对应场景的数据集；</li>
<li>使用孪生CNN网络(siamese network)并结合contrastive loss &amp; classfication loss完成了整个模型的训练；</li>
<li>将网络应用到实际中；</li>
</ol>
<p>作者在论文中花了较多篇幅介绍数据集的收集与处理，但在下文中就重点介绍第2部分，其余的还是请阅读原文哦~~~</p>
<h1>三 网络结构介绍</h1>
<p>可能大家对于什么是siamese network, 什么是contrastive loss都有点陌生，接下来我会先介绍这两部分的内容，然后再总结论文中使用的结构及一些重要的处理细节。</p>
<h2>1 background: 用孪生网络进行距离度量</h2>
<h3>1.1 CNN模型建模</h3>
<ul>
<li>我们的卷积网络模型其实都可以看成是一个函数f，只不过这个函数比我们直观接触过的线性函数、二次函数等要复杂和很多，anyway，当参数θ确下来以后它就是一个从输入到输出的映射关系f；</li>
<li>而对于每一张输入图像I，我们都能得到对应的输出特征x；</li>
<li>所以我们得到这样的关系：x = f(I;θ)</li>
</ul>
<h3>1.2 contrastive loss</h3>
<p>首先介绍一下正样本和负样本的概念。</p>
<ul>
<li>正样本：物品q和p是相似的，通常被标注为y=1，那么我们称这样的训练数据对(p,q)是positive pair;</li>
<li>
<p>负样本：物品q和n是不相似的，通常被标注为y=0，那么我们称这样的训练数据对(p,n)是negative pair;</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 438px; max-height: 137px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 31.28%;\"></div>
<div class=\"image-view\" data-width=\"438\" data-height=\"137\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/9933353-ad993d8e4f4ca687.png\" data-original-width=\"438\" data-original-height=\"137\" data-original-format=\"image/png\" data-original-filesize=\"52792\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
</li>
</ul>
<p>如图，我们定义一个边界值Margin(m)来判定两个物品是否相似，例如对于图中的p与q被标注为正样本，在图中体现在两者的特征值会比较相近(圆内); 反之，p与q为负样本，那么他们的特征值之差一定大于m。</p>
<p>接下来我们来学习一下如何用正负样本计算损失值contrastive loss。<br>
如下图所示为损失函数L的定义</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 434px; max-height: 148px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 34.1%;\"></div>
<div class=\"image-view\" data-width=\"434\" data-height=\"148\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/9933353-533676c786804087.png\" data-original-width=\"434\" data-original-height=\"148\" data-original-format=\"image/png\" data-original-filesize=\"19007\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>对于公式作如下解读：</p>
<ul>
<li>loss由两部分组成；</li>
<li>Lp表示对相似的图片(正样本)得到的特征值太远的惩罚（理论应该是越近越好）；</li>
<li>Ln表示对不相似的图片（负样本)得到的特征值太近的惩罚（理论应该至少大于m）;</li>
</ul>
<p>如下图为更形象一点的解释：</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 436px; max-height: 346px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 79.36%;\"></div>
<div class=\"image-view\" data-width=\"436\" data-height=\"346\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/9933353-e3c862a9466b177c.png\" data-original-width=\"436\" data-original-height=\"346\" data-original-format=\"image/png\" data-original-filesize=\"80682\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<ul>
<li>正样本(p,q)通过CNN得到的特征值x如果离得太远，会在loss优化的时候慢慢拉回来；</li>
<li>负样本(p,n)通过CNN得到的特征值x如果小于阈值m，会在loss优化的时候慢慢拉大；</li>
</ul>
<p>这样用正负样本去训练神经网络的参数，我们就能得到一个能够正确判断物品相似性的结果。</p>
<h3>1.3 siamese network</h3>
<p>孪生网络顾名思义就是使用一样的网络结构，因为在我们的算法中，需要同时对需要正样本（p,q）或者是负样本(p,n)计算特征值，为了提高效率，设计了这样的结构~</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 179px; max-height: 178px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 99.44%;\"></div>
<div class=\"image-view\" data-width=\"179\" data-height=\"178\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/9933353-b2b38894eedaea4c.png\" data-original-width=\"179\" data-original-height=\"178\" data-original-format=\"image/png\" data-original-filesize=\"12282\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>其实本质就是两个CNN网络共享参数θ，这个结构在上面那张图中也已经展示出来了。</p>
<h2>2 Our-approach：文章网络结构具体介绍</h2>
<h3>2.1 网络结构</h3>
<p>在上面background的基础上，文章提出了4种网络结构：</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 695px; max-height: 350px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 50.36000000000001%;\"></div>
<div class=\"image-view\" data-width=\"695\" data-height=\"350\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/9933353-a66ffcce40a650a0.png\" data-original-width=\"695\" data-original-height=\"350\" data-original-format=\"image/png\" data-original-filesize=\"72820\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>都不是很复杂，上面的看懂的话应该很快就能看懂这4个结构图。</p>
<ul>
<li>A：将本来用于分类任务网络，将最后一层softmax改成特征提取层</li>
<li>B：最基本的siamese CNN + contrastive loss结构</li>
<li>C：在B的基础上增加了一个图像分类任务，用多任务更好的表示loss</li>
<li>D：在C的基础上对提取到的特征多做了一次L2归一化操作</li>
</ul>
<p>文章最后给出的应用演示中用的是D结构，如下图所示为用D结构，并结合t-SNE算法做出来的可视化效果图:</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 589px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 62.73%;\"></div>
<div class=\"image-view\" data-width=\"939\" data-height=\"589\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/9933353-1d7ead55394420ec.png\" data-original-width=\"939\" data-original-height=\"589\" data-original-format=\"image/png\" data-original-filesize=\"736146\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<h3>2.2 重要处理细节</h3>
<p><strong>正负样本处理</strong><br>
我们直接拿到的标注数据一般只有正样本对(p,q),那么如何更好的利用这些正样本数据及生成负样本数据呢？<br>
个人觉得本文的处理方式还是挺有参考意义的，所以整理在这里：</p>
<ol>
<li>生成负样本：对于每个图片p，都随机选取80张属于同一类别的图像+随机选取20张不属于这个类别的图像，这样每个图像p，都能生成100对负样本(p,n)；</li>
<li>为了平衡正样本与负样本之比，正样本对*5，这样正样本对：负样本对=5:100=1:20；</li>
<li>对于每个图像，我们都有这个物体对应的框。为了数据增强，采用在原来的框外面额外选取像素大小为{0,8,16,32,48,64,80}的边框，这边边框其实就作为物品的背景，从而达到训练数据增强的效果；</li>
<li>最后将数据分成train、val、test(图像p应该没有交叉)</li>
</ol>
<blockquote>
<p><strong>讨论&amp;求助</strong>：以上是我整理出来的核心处理思路，作为学习来说应该够用了，但我用论文中实际的数据量代入作者描述的方法，不能得到论文中的数据，就是为什么101,945的原始正样本对最后得到的是63,820,250的训练数据、3,667,769的验证数据以及6,391的测试数据。非常希望有同学能帮我解一下疑惑，ヾ(°°)</p>
</blockquote>
<p><strong>其他小细节</strong></p>
<ul>
<li>margin(m)的初始化，文章探索了{1,√10, √100,√1000}, 最后用D结构的时候用了√2</li>
<li>网络结构的初始化可以考虑用模型迁移</li>
<li>距离度量使用的是cosine</li>
<li>处理在测试阶段对region加了padding，在测试阶段也用了16的padding效果最好。</li>
</ul>
<h1>四 写在最后</h1>
<p>啊啊啊啊啊啊，终于写完了，看了一下时间，写了2个小时。<br>
而且是在完！全！集！中！精！力！的情况下~</p>
<p>个人很喜欢分享，但真的不喜欢没有任何招呼也不注明作者信息的情况下就被网站拿去全文使用，有点小委屈╭(╯^╰)╮</p>
<p>oooooh，恢复正常，总结的有问题还请简友们一定要提出来，特别是期待有人能解决我文中的困惑，感激不尽！</p>
<p>最后，这篇文章用的是contrastive loss,在前面推荐的另一篇文章中用的是FaceNet网络中用的是另一种图像相似度检测中常用的triplet loss，具体的等我读完文章以后会再整理一篇，谢谢关注嘻嘻~~~</p>
<h1>参考文献</h1>
<ul>
<li><a href=\"http://xueshu.baidu.com/s?wd=paperuri%3A%28859efbbc3e0698aa3da75d747d4ccfc3%29&amp;filter=sc_long_sign&amp;tn=SE_xueshusource_2kduw22v&amp;sc_vurl=http%3A%2F%2Fdl.acm.org%2Fcitation.cfm%3Fid%3D2809654.2766959&amp;ie=utf-8&amp;sc_us=16474410094205442206\" target=\"_blank\" rel=\"nofollow\">Learning visual similarity for product design with convolutional neural networks</a></li>
</ul>

          </div>','1531183697'),
('325567','{1011432}{1124103}{1295}{1124105}{1124108}{1124109}{1124112}{975755}{975761}{1124113}{975721}{36}{975728}{62}{296}{503}{1103}{975776}{1580}{1740}{1901}{17}{587}{975717}{24}{442}{25243}{73871}{1124114}{1124115}{4129}{975915}{976193}{975714}{89357}{4601}{975903}{975892}{979369}{4982}{2437}{884}{975738}{1124116}{24433}{517}{351}{975830}{975729}{976042}','R-CNN系列其三：Fast_R-CNN 介绍 R-CNN的创始人Girshick（同样来自微软）对R-CNN与SPP-Net有了更多思考后，对这种需要多阶段进行训练的目标检测框架做了重大更新。使得整体的目标检测这一部分不再由多阶段来完成，而是统一使用一个CNN网络来完成特征提取，区域提案分类及后期对检测框的微调等三样工作。这个创新算是不小，相对于之间流程的复杂，简单后了的模型不只实现了训','R-CNN系列其三：Fast_R-CNN','<div class=\"show-content-free\">
            <h1>介绍</h1>
<p>R-CNN的创始人Girshick（同样来自微软）对R-CNN与SPP-Net有了更多思考后，对这种需要多阶段进行训练的目标检测框架做了重大更新。使得整体的目标检测这一部分不再由多阶段来完成，而是统一使用一个CNN网络来完成特征提取，区域提案分类及后期对检测框的微调等三样工作。这个创新算是不小，相对于之间流程的复杂，简单后了的模型不只实现了训练与推理速度的提升，同时也带来了目标检测精度的增加。大牛水平就是高啊！往往能够通过将模型简单化来使得问题得到更好的解决。高山仰止！</p>
<h1>R-CNN目标检测框架的缺点</h1>
<ul>
<li>它的训练是多阶段的：我们在使用像Selective Search等方法获得图片的区域提案后，先使用这些提案作为输入来训练出一个收敛、精度还算可以的CNN网络；然后去掉此CNN网络最后的Softmax层/FC层后，直接使用网络最后的CNN层输出的feature maps作为一个SVM分类器的输入，对SVM分类器进行训练以识别出此区域提案可能的分类；最后第三个阶段同样是利用第一阶段训练得到的CNN前端网络拿到处理过的CNN层的feature maps，将之作为输入来训练一个逻辑回归模型以来对区域位置进行较正；多阶段训练本身就是复杂的，难操作的；</li>
<li>训练时的空间与时间复杂度高：训练第二、三阶段所用的SVM与逻辑回归分类器都需要将图片区域提案通过第一阶段训练得到的CNN网络以生成相应的特征向量；一般这些特征向量都会存储在硬盘当中；对于VGG16的前端CNN网络，一般需要2.5天的GPU时间来处理5000张VOC07 图片，而这些图片产生的特征向量则需要数百GB的硬盘空间来存储；</li>
<li>对象检测慢：因为R-CNN网络需要对每个图片之上的每一个区域提案进行三阶段处理，因此需要的时间极长，当使用VGG16网络时一般需要47秒来处理一副图片。</li>
</ul>
<h1>SPP-Net应用在目标检测框架上的缺点</h1>
<ul>
<li>它的训练同样是多阶段的，所以与前面章节所说的R-CNN有着类似的缺点；</li>
<li>由于SPP层的引入，它可以对整张图直接计算，以节省同张图上面多个区域提案在前端网络计算上所需的时，但在使用finetue方式进行训练模型增强时只对SPP层之后的FC层进行finetune，因此整体模型的准确度会受些影响。</li>
</ul>
<h1>Fast_R-CNN带来的改进</h1>
<ul>
<li>它在多个数据集上取得了比R-CNN与SPP-Net更高的mAP准确率；</li>
<li>使用单个阶段完成目标区域检测；（可以说是最本质的创新）</li>
<li>训练时对所有的层进行同步更新；</li>
<li>因为是单阶段训练所以不需要额外的硬盘空间来存储中间特征。</li>
</ul>
<h1>Fast_R-CNN网络结构</h1>
<p>下图所示为Fast_R-CNN的基本网络结构，同过跟上篇的SPP-Net的网络结构对比，我们能够发现它结构最本质的创新即在于直接将最后FC层后得到的特征向量分别使用Softmax层与Regressor层来直接对区域方案的类别与位置进行预测与调整，这一网络结构改进后来也为其它的模型像Yolo系列与SSD所采用。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 582px; max-height: 423px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 72.68%;\"></div>
<div class=\"image-view\" data-width=\"582\" data-height=\"423\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/5971313-e9d131db4465e16a.JPG\" data-original-width=\"582\" data-original-height=\"423\" data-original-format=\"image/jpeg\" data-original-filesize=\"79258\"></div>
</div>
<div class=\"image-caption\">Fast_R-CNN网络结构</div>
</div>
<ul>
<li>ROI池化层： ROI层的引入本质上是Fast_R-CNN对SPP-Net里面idea的最大借鉴；ROI池化层有两个参数H，W，分别表示此池化层处理后最终能得到的feature map的高与宽；任意一个ROI特征（经过之间的CNN网络处理后）都可以视为一个（r,c,h,w）的四元组，其中（r,c）表示区域左上角的位置，（h,w）则为此区域的高宽大小；ROI层在对这么一个ROI特征四元组进行处理时，会先将其分别H x W个网格单元，每个单元的大小为h/H x w/W，然后分别对每个单元做MaxPool处理以得到一个极大值，这样最终就能得到一个大小为H x W的特征输出。就此我们容易看出本质上ROI层是只有一个空间池化级别的金字塔式空间池化层。</li>
<li>前端CNN网络的初始化：任意一个CNN分类网络都可作为它的前端CNN网络；不过对用于分类的CNN网络我们需要进行一些变形以来满足Fast_R-CNN需求，首先需要将最后一个MaxPool层替换为ROI层以来输出与接下来FC层输入大小相匹配的feature maps；另外则需要将CNN分类网络最后端的FC层（如对于Imagenet相关的CNN网络即其最后的1000维输入的FC层）替换为两个兄弟层，一个为用于分类的Softmax层，另一个则用于区域位置检测的回归层，一般为L1回归；最后整个CNN网络的输入需要由两部分组成，分别为输入图片数据与对应的ROI区域位置提案。</li>
<li>Fast_R-CNN的训练损失函数：如下所示，可以知道它共有两个部分构成，分别对应两个层的输出（Softmax层与L1回归层）；<br>
L(P, U, Tu, V) = Lcls(P, U) + λ[U ≥ 1]Lloc(Tu, V), 其中Lcls(P, U) =  log Pu是ROI计算得到的相对于正确类别U的交叉熵损失；而Lloc 则是此ROI最终调整过后得到的目标区域与正确目标区域之间的L1损失。</li>
</ul>
<h1>Fast_R-CNN检测模型部署</h1>
<p>一旦我们训练出了正确的Fast_R-CNN模型，那么可按以下步骤来进行模型部署。</p>
<ul>
<li>以一张图片及其上的R个目标提案作为输入；(R一般为2000左右)</li>
<li>一个前向运算后我们能得出每个ROI所对应的类别分布与相应的目标位置框的位置偏移（相对于输入的RoI窗口位置）；</li>
<li>接下来可使用与R—CNN类似的NMS（非最大值抑制）方法来减少预测得到的目标检测框的数目，最终得到的类别及框位置即为此图片所检测得到的类别与框。</li>
</ul>
<h1>实验得到的发现</h1>
<ul>
<li>作者通过实验证实通过多目标学习来一起学习目标类别与位置相对于像之前R-CNN或SPP-Net那样对目标类别与位置分别进行学习相比，多目标学习可取得更好的mAP值；</li>
<li>较深的网络在进行图片特征学习时会自动地学会图片大小无关的特征提取方法；因此作者认为在使用像VGG16这样较深的网络时，只使用单一缩放图片数据集进行模型训练在时间与精度权衡上效果更好。</li>
</ul>
<h1>Fast_R-CNN Caffe代码</h1>
<p>本质上它相对于SPP-Net的主要改进在于将原来R-CNN框架下分为三个阶段去做的事情整合为一个阶段，反映在caffe model 上面即是最终的loss层实现了多目标损失函数学习。另外因为ROI层也是SPP层的一个特例，因此我们也放在这里。</p>
<p>---------------ROI层-----------------------</p>
<pre><code>layer {
  name: \"roi_pool5\"
  type: \"ROIPooling\"
  bottom: \"conv5\"
  bottom: \"rois\"
  top: \"pool5\"
  roi_pooling_param {
    pooled_w: 6
    pooled_h: 6
    spatial_scale: 0.0625 # 1/16
  }
}
</code></pre>
<p>---------------最后几层的损失函数层----------------------</p>
<pre><code>layer {
  name: \"cls_score\"
  type: \"InnerProduct\"
  bottom: \"fc7\"
  top: \"cls_score\"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 21
    weight_filler {
      type: \"gaussian\"
      std: 0.01
    }
    bias_filler {
      type: \"constant\"
      value: 0
    }
  }
}

layer {
  name: \"bbox_pred\"
  type: \"InnerProduct\"
  bottom: \"fc7\"
  top: \"bbox_pred\"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 84
    weight_filler {
      type: \"gaussian\"
      std: 0.001
    }
    bias_filler {
      type: \"constant\"
      value: 0
    }
  }
}

layer {
  name: \"loss_cls\"
  type: \"SoftmaxWithLoss\"
  bottom: \"cls_score\"
  bottom: \"labels\"
  top: \"loss_cls\"
  loss_weight: 1
}

layer {
  name: \"loss_bbox\"
  type: \"SmoothL1Loss\"
  bottom: \"bbox_pred\"
  bottom: \"bbox_targets\"
  bottom: \"bbox_loss_weights\"
  top: \"loss_bbox\"
  loss_weight: 1
}
</code></pre>
<h1>参考文献</h1>
<ul>
<li>Fast R-CNN, Ross Girshick, 2015</li>
<li><a href=\"https://github.com/rbgirshick/fast-rcnn/\" target=\"_blank\" rel=\"nofollow\">https://github.com/rbgirshick/fast-rcnn/</a></li>
</ul>

          </div>','1531183699'),
('325568','{392745}{975755}{1124121}{1076}{1124122}{160788}{9080}{36}{2396}{832}{975757}{975714}{976193}{1124123}{24}{3352}{24841}{1124124}{1124125}{186861}{672451}{411410}{1124126}{975878}{975890}{62}{2341}{975721}{4354}{1145}{733}{63}{1912}{17}{975717}{749}{975779}{1190}{975786}{1747}{975935}{775}{54190}{975891}{7158}{975918}{975835}{975741}{3102}{6041}','25，这部分的实验结果如下图所示，图中左下角的图，图中每个点代表一个类，分别对应训练数据和生成数据的MS-SSIM的数值，蓝色的线依旧为y=x的函数线。但是这块并不是看生成数据MS-SSIM比训练数据的MS-SSIM大或者小的部分（因为这并没有什么意义，生成的数据多样性比训练数据高或低没有太多意义），而是要看红色的分界线，红色的线为MS-SSIM为0.','Conditional Image Synthesis With Auxiliary Classif','<div class=\"show-content-free\">
            <p>用辅助分类器的GANs的条件图像合成</p>
<p>摘要<br>
合成高分辨率并且真实的图片在机器学习是一个一直存在的挑战。在本文中我们引入新的方法，来提高用于图像合成的生成对抗网络（GANs）的训练。我们利用标签条件构造了GANs的变种，它产生了128<em>128像素的样本并展示出了全局一致性。我们通过扩展之前用于图像质量评估的工作，来提供两种新的分析方法来评估以类为条件的图片合成模型中，生成样本的可分辨性和多样性。这些分析方法表明高分辨率的样本提供了低分辨率样本中没有的类别信息。在ImageNet的1000类别中，128</em>128的样本比起人工变小的32*32大小的样本的分辨性的两倍以上。除此之外，比起真实的ImageNet数据，84.7%的这些类有样本显示出了多样性。</p>
<p>全文：<a href=\"https://arxiv.org/abs/1610.09585\" target=\"_blank\" rel=\"nofollow\">https://arxiv.org/abs/1610.09585</a></p>
<p>本文提出了一个新的条件对抗生成网络的框架，其具体的网络结果如下图所示：生成器的输入除了GAN结构中常见的噪声Z以外，还加入了类标签，而分辨器的判别也不再仅限于输入数据真伪的判断，同时会给出数据所属类标签的判断，因而分辨器成了强化的分类器（虽然之前也是二分类的分类器，但是其监督信号并不是很强）。如此设计的GAN在文中被称作AC-GAN（auxiliary classifier GAN，有辅助分类器的GAN）。<br>
因为修改了分辨器，所以AC-GAN的训练Loss更改如下：</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 660px; max-height: 81px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 12.27%;\"></div>
<div class=\"image-view\" data-width=\"660\" data-height=\"81\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6080435-99f26bafc0238e3b.png\" data-original-width=\"660\" data-original-height=\"81\" data-original-format=\"image/png\" data-original-filesize=\"14602\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>其中分辨器的Loss为最大化Ls+Lc，生成器的Loss为最大化Lc-Ls，其中S表示判断图片的来源(Source)，C表示判断图片的类标签（Class Label），这样的描述可能比较复杂，因而转换成MaxMin Game描述Loss如下：</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 549px; max-height: 21px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 3.83%;\"></div>
<div class=\"image-view\" data-width=\"549\" data-height=\"21\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6080435-8caddb784a783c63.png\" data-original-width=\"549\" data-original-height=\"21\" data-original-format=\"image/png\" data-original-filesize=\"1908\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 299px; max-height: 493px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 164.88%;\"></div>
<div class=\"image-view\" data-width=\"299\" data-height=\"493\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6080435-557aa8f3fe14adc9.png\" data-original-width=\"299\" data-original-height=\"493\" data-original-format=\"image/png\" data-original-filesize=\"23279\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>具体的网络构成，可以见文章的附录部分。</p>
<p>随后，本文就提出的图片生成模型，认为生成图片的网络，不能是将低分辨率的图片进行简单的线性插值而生成高分辨率的图片，与此同时，生成的图片不能犯GAN常见的模式崩塌的问题，产生单一并不多样化的图片。</p>
<p>测试生成的图片的分辨力<br>
如上所述，生成高分辨率的图片，需要不是简单的将低分辨率的图片进行线性插值来生成，因而要量化的分析生成的图片的质量，可以从其分辨力。从低分辨率通过插值生成的高分辨率图片，其本质上没有增加多余信息，只是低分辨率的模糊版。结合这样的思路，高分辨率的图片提供了更多的信息，这些信息结合到AC-GAN结构，每个生成图片都有其对应的标签，因而这个更多的信息，可以通过分类来表明，也就是说更多的信息，可以用于分类，也就是文中所说的分辨力（Discriminability）。<br>
因此，文中采用了Inception网络对于生成的图片进行分类，查看其被分类为正确类别的比率，以此来判定生成的图片质量。下图中，图中左下的图，黑色的线，是真实图片，因而其达到的准确率可以说是生成图片的准确率的上限，红色的线表示的是生成的128<em>128分辨率的图片的准确率表现，蓝色的线是生成的64</em>64分辨率的图片的准确率表现，对于比其高或低分辨率的图片的准确率，是通过插值的方式缩放以后得到的图片得出的准确率表现，可以看到，降低分辨率确实降低了准确率，明确表明低分辨率的类信息更少；同样通过插值方式提高分辨率并不会带来更多的类信息，同时也不会损害已有的类信息，因而准确率保持不变。图中右下的图，每个点代表不同的类别，其坐标分别代码不同的分辨率下的准确率，其中蓝色的线是y=x的函数线，也就表明位于蓝色线上方的点，含义是该类别的图片在32<em>32分辨率条件下准确率高于128</em>128条件下的准确率，反之，在下方的点表明32<em>32的准确率低于128</em>128的准确率。文中统计了在线下方的点的比例为84.4%，也可以说大部分的图片在高分辨率的情况下，用于分类准确率会高于低分辨率。<br>
</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 524px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 74.97%;\"></div>
<div class=\"image-view\" data-width=\"943\" data-height=\"707\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6080435-96c50b87d5c74d4e.png\" data-original-width=\"943\" data-original-height=\"707\" data-original-format=\"image/png\" data-original-filesize=\"380854\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p></p>
<p>测试图片的多样性<br>
GAN有个最常见的问题就是模式坍塌的问题，就是模型找到一种方式，无论输入的内容是什么，生成的图片都只有一种，然而这种图片能大概率欺骗过分辨器。因而，产生的图片具有多样性，也是可以评估GAN模型好坏的指标。文中采用了图片的多尺度结构相似度来衡量图片与图片之间的相似度（multi-scale structural similarity，MS-SSIM），这个相似度在0和1之间取值，越大说明图片之间越相似。文中在一个给定类中取图片对，计算两者之间的MS-SSIM，如果图片多样性程度越高，那么这个MS-SSIM的分数应该越低。ImageNet的训练数据的平均MS-SSIM值最高的为0.25，这部分的实验结果如下图所示，图中左下角的图，图中每个点代表一个类，分别对应训练数据和生成数据的MS-SSIM的数值，蓝色的线依旧为y=x的函数线。但是这块并不是看生成数据MS-SSIM比训练数据的MS-SSIM大或者小的部分（因为这并没有什么意义，生成的数据多样性比训练数据高或低没有太多意义），而是要看红色的分界线，红色的线为MS-SSIM为0.25，因而低于0.25的数据，可以说生成的数据是比较接近真实图片的，文中统计了这根红线下面的类的数量为847个，也就是说84.7%的AC-GAN生成的类数据的多样性超过了训练集中最小变化量的类（仔细思考下，这样的比较方式可能存在一点问题，个人觉得用所有类的MS-SSIM的平均值可能会更具有代表性）。图中右下角的图，文中表示红色的线是生成数据的MS-SSIM平均值在训练过程中的变化（一直在上升，是不是说明有崩塌的可能，而且接近1了）；同时图中黑色的线，应该是训练完成后生成的图片的多样性（文中提及同样标准测试了训练数据，和完成后的生成样本，但是这里存在问题的是，训练完成后的横坐标按理是不存在的，或者，这个缺陷应该不会波动很大，这里也可以理解为横坐标为训练最大次数，然后计算平均的MS-SSIM的数值，不过这里确实没交代太清楚）。</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 670px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 95.78%;\"></div>
<div class=\"image-view\" data-width=\"948\" data-height=\"908\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6080435-b38c2d0ab5c3fb24.png\" data-original-width=\"948\" data-original-height=\"908\" data-original-format=\"image/png\" data-original-filesize=\"889142\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>文中除了分开的探索生成图片的分辨力和多样性，也探索了生成图片的分辨力和多样性的相关性，如下图所示，文中得到两者相关性为负相关（相关系数r=-0.16），因此认为AC-GAN的模型并没有以分辨力为代价，来产生多样性的样本。（这里得注意，分辨力的参数是越大越好，多样性的参数是越小越好）。</p>
<p>除了说明生成的图片具有分辨力的同时也具有多样性外，文中通过Inception Score比较了AC-GAN生成的样本的质量，获得了8.25±0.07的分数。同时由于在生成ImageNet的1000个类的数据时，采用了100个AC-GAN来生成数据，每个AC-GAN只需要关注10个类的数据的生成（GAN在存在多个类的情况下，生成的样本效果并不好，这也是GAN的一个研究方向），因此在附录中，文中还探索了这样划分类是否会使得AC-GAN生成效果更好。</p>
<p>在探索AC-GAN是否存在过拟合的实验中，文中提出了两种思路，第一种思路是比较L1距离最近的生成的图片，看起是否类似于训练的数据，从而判定是否存在过拟合，给出的实验结果图比较，依旧是人为的评定是否相似。除此方法之前，文中提出了线性插值噪声z和类标签c，查看其变化，其认为如果是过拟合的模型，那么产生的图片在插值的输入面前会发生图片，结果依旧需要认为判断，不过这样判断是否发生突变会比之前的容易（感觉可以用MS-SSIM来确定插值后产生的图片是否依旧比较大之类的）。<br>
总结<br>
本文提出的GAN结构，修改了Generator，除了输入噪声z之外，还提供了需要生成数据的类标签c；修改了Discriminator，除了判断图片的真伪之外，还需要判断图片的类标签。这样，在加入了监督信号的情况下，提升了GAN生成图片质量，并且没有出现GAN容易出现的模式坍塌现象。在评估GAN生成的图片验证上，提出了采用生成图片分类结果准确性来证实生成图片的质量，与此同时采用MS-SSIM参数的评估，来检验AC-GAN生成数据的多样性，提出这些数值评估的情况，在某种程度上而言，这些量化的分析，都可以设计成对应的Loss近一步提升GAN的生成图片的能力。除此之外，还定性分析了AC-GAN模型的是否产生过拟合的问题。</p>

          </div>','1531183700'),
('325569','{39}{72}{975741}{442}{975728}{2064}{975714}{8022}{17}{975761}{394}{1124150}{1124151}{36}{17593}{1124152}{1077}{1635}{1124084}{975915}{1094}{62}{234}{1124153}{975971}{2163}{296}{979369}{975835}{831}{975757}{1124154}{1124155}{975721}{68890}{1314}{1124087}{3918}{6632}{7464}{1785}{1277}{1080}{1580}{975779}{255}{3483}{3289}{1137}{1124156}','## 【TPAMI重磅综述】 SIFT与CNN的碰撞：万字长文回顾图像检索任务十年探索历程（下篇） 翻译 编辑：李中梁 前言 本文是《SIFT Meets CNN: A Decade Survey of Instance Retrieval》的下篇。在上 篇中概述了图像检索任务极其发展历程，介绍了图像检索系统的基本架构和设计难点，详细展示了基于图像局部特征（以SIFT为代表）的检索流程以及关键环节的核心算法。 在下篇中将介绍基于CNN特征','## 【TPAMI重磅综述】 SIFT与CNN的碰撞：万字长文回顾图像检索任务十年探索历程（下篇）','<div class=\"show-content-free\">
            <p>翻译&amp;编辑：李中梁</p>
<h1>前言</h1>
<p>本文是《SIFT Meets CNN: A Decade Survey of Instance Retrieval》的下篇。在<a href=\"http://mp.weixin.qq.com/s?__biz=MzUyMjE2MTE0Mw==&amp;mid=2247486346&amp;idx=1&amp;sn=139309de32ae1b72fc3ce5e81fd7811a&amp;chksm=f9d15512cea6dc0449ba0ba223ab5b8fb790b8892d7dc755766475e7877b5dcf88e2cfe74863&amp;scene=21#wechat_redirect\" target=\"_blank\" rel=\"nofollow\">上 篇</a>中概述了图像检索任务极其发展历程，介绍了图像检索系统的基本架构和设计难点，详细展示了基于图像局部特征（以SIFT为代表）的检索流程以及关键环节的核心算法。</p>
<p>在下篇中将介绍基于CNN特征的图像检索系统的流程及关键问题，并在几个常见数据集上测试六个经典检索系统的性能。最后文章将对图像检索领域的发展趋势进行展望。</p>
<p>这篇刊登在 TPAMI 2018年5月刊上的综述《SIFT Meets CNN: A Decade Survey of Instance Retrieval》全面调研了十多年来图像检索任务中所使用的图像编码、检索算法，并对比了各种方法在各大数据集上的实验结果，旁征博引，内容详实。如果您刚接触图像检索领域，可以通过本篇文章摸清其概貌；如果您在领域内深耕多年，也可以在本文中查漏补缺，裨益良多。</p>
<p>TPAMI是计算机视觉领域顶级期刊，此文的质量也不必多言，我在此斗胆将这篇综述加以整理，翻译成文，若有不当之处还望指出。</p>
<hr>
<h1><strong>标题：</strong></h1>
<h1><strong>当SIFT邂逅CNN：图像检索任务跨越十年的探索历程</strong></h1>
<h1><strong>作者：</strong></h1>
<h1><strong>Liang Zheng, Yi Yang, and Qi Tian</strong></h1>
<hr>
<h1>4 基于CNN的图像检索系统</h1>
<p>基于CNN的图像检索方法近年来不断被提出，并且在逐渐取代基于手工检测器和描述符的方法。在这篇综述中，基于CNN的方法被分为三类：使用预训练的CNN模型，使用微调的CNN模型以及使用混合模型。前两类方法使用单向传递网络来提取全局特征，混合模型方法可能需要多个网络传递。如图2所示（译者注：由于图2在上篇中，因此摘取到本文中方便阅读）。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 405px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 37.5%;\"></div>
<div class=\"image-view\" data-width=\"1080\" data-height=\"405\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-f0ad94deb63b9afe\" data-original-width=\"1080\" data-original-height=\"405\" data-original-format=\"image/jpeg\" data-original-filesize=\"65710\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>图2：基于SIFT特征与CNN特征的图像检索流程</p>
<h3>4.1 使用预训练CNN模型的图像检索系统</h3>
<p>由于预训练CNN模型是单通模式，因此这种方法在特征计算中非常高效。考虑到传输特性，它的成功在于特征提取和编码步骤。我们将首先描述一些常用的数据集和网络进行预训练，然后进行特征计算。</p>
<h5>4.1.1 预训练的CNN模型</h5>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 639px; max-height: 210px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 32.86%;\"></div>
<div class=\"image-view\" data-width=\"639\" data-height=\"210\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-47df8ec65d7457bb\" data-original-width=\"639\" data-original-height=\"210\" data-original-format=\"image/jpeg\" data-original-filesize=\"37693\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>表2：可供使用的预训练CNN模型</p>
<p><strong>流行的CNN网络结构。</strong> AlexNet，VGGNet，GoogleNet以及ResNet这几个CNN网络适用于特征提取，详见表2.简单来说，CNN网络可以视为一系列非线性函数的集合，它由如卷积，池化，非线性等多个层组成。CNN是一个分层次的结构。自网络的底层到顶层，图像经过滤波器的卷积，同时这些图像滤波器的感受野随增长而增加。同一层的滤波器尺寸相同但是参数不同。AlxNet是这些网络中最早被提出的的，它有五个卷积层和三个全连接（FC）层。它的第一层大小96个11×11×3的滤波器，在第五层中有256个大小为3×3×192的滤波器。Zeiler等人观察到滤波器对某些视觉模式十分敏感，这些模式从底层的低级的图像纹理演变到顶层的高级的图像目标。对于低层次和简单的视觉刺激，CNN滤波器类似局部手工制作的特征中的检测器，但是对于高层次和复杂的刺激，CNN滤波器具有不同于SIFT类检测器的特质。AlxNET已被证明被新的的如具有最大数量参数的VGGNet超越。ResNet和GoogleNet分别赢得了ILSVRC 2014和2015的挑战，表明CNN网络的效果和网络层数成正比。如果要调研全部这些网络超出了本文的范围，我们建议读者参阅《Imagenet classification with deep convolutional neural networks》，《Return of the devil in the details: Delving deep into convolutional nets》和《Very deep convolutional networks for large-scale image recognition》中的细节。</p>
<p><strong>用于预训练模型的数据集。</strong> 一些大规模的识别数据集被用于CNN网络的预训练。在其中，ImageNet数据集常被研究员拿来使用。它包含1000个语义类的120万个图像，并且通常被认为是具有普适性的。用于预训练模型的另一个数据集是PASES-205，它的数据规模是ImageNet的两倍但图像种类却要比ImageNet少五倍。它是一个以场景为主的数据集，描绘了各种室内场景和室外场景。在《Learning deep features for scene recognition using places database》中，混合了ImageNet和PASES-205的数据集也同样会被拿来用于模型的预训练。HybridNet在《Going deeper with convolutions》，《Deep residual learning for image recognition》，《Factors of transferability for a generic convnet representation》和《A practical guide to cnns and fisher vectors for image instance retrieval》中被用于实例检索任务的评估。</p>
<p><strong>迁移问题。</strong> 最近的一些工作综合评估了各种CNN网络在实例检索任务中的表现，模型迁移是大家都比较关心的一个问题。在《Factors of transferability for a generic convnet representation》中将实例检索任务认为是距离原始数据集最远的（计算机视觉）目标。首先，在模型迁移过程中，从不同层提取的特征表现出不同的检索性能。实验表明高层网络的泛化能力要低于较低层的网络。例如，在ImageNet上预训练的网络AlexNet表明，FC6、FC7和FC8在检索精度上呈递减顺序。《Particular object retrieval with integral max-pooling of cnn activations》和《Good practice in cnn feature transfer》也指出，当使用适当的编码技术时，AlexNet和VGGNet的pool5层特征甚至优于FC6层特征。其次，当原始的训练集不同时，模型的准确率也会受到影响。例如，Azizpour等人指出HybridNet在Holidays数据集上展现出的性能要劣于PCA。他们同样发现在ImageNet上预训练的AlexNet模型在包含常见物体而非建筑场景图像的Ukbench数据集上的表现要好于PlacesNet和HybridNet（译者注：AlexNet，PlacesNet和HybridNet预训练模型使用的训练集不同）。因此，当使用预训练的CNN模型时，源和目标的相似度在实例检索中起着至关重要的作用。</p>
<h5>4.1.2 特征提取</h5>
<p><strong>FC描述符。</strong> 最直接的想法就是网络的全连接层（FC layer）提取描述符，在AlexNet中就是FC6或FC7中的描述符。FC描述符是在与输入图像卷积的层之后生成的，具有全局表示性，因此可以被视为全局特征。它在欧几里德距离下产生较好的检索精度，并且可以使用指数归一化来提高检索精度。</p>
<p><strong>中间局部特征。</strong> 许多最新的检索方法专注于研究中间层的描述符。在这种方法中，低层网络的卷积核用于检测局部视觉模式。作为局部检测器，这些滤波器具有较小的感受野并密集地应用于整张图像。与全局FC特征相比，局部检测器对于诸如截断和遮挡的图像变换更鲁棒，其方式类似于局部不变量检测器。</p>
<p></p>
<p></p>
局部描述符与这些中间局部检测器紧密耦合，换而言之，它们是输入图像对这些卷积运算的响应。另一方面，在卷积运算后等到的激活图层可以看做是特征的集成，在这篇综述中将其称为“列特征”。例如，在AlexNet中第一层有<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 53px; max-height: 16px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 30.19%;\"></div>
<div class=\"image-view\" data-width=\"53\" data-height=\"16\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-1dd08f09eb560d6f\" data-original-width=\"53\" data-original-height=\"16\" data-original-format=\"image/png\" data-original-filesize=\"366\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p></p>
<p></p>
个检测器（卷积滤波器）。这些滤波器产生了<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 53px; max-height: 16px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 30.19%;\"></div>
<div class=\"image-view\" data-width=\"53\" data-height=\"16\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-9715493f91259ff0\" data-original-width=\"53\" data-original-height=\"16\" data-original-format=\"image/png\" data-original-filesize=\"366\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p></p>
<p></p>
张大小为<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 59px; max-height: 17px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 28.810000000000002%;\"></div>
<div class=\"image-view\" data-width=\"59\" data-height=\"17\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-6f469425292308d9\" data-original-width=\"59\" data-original-height=\"17\" data-original-format=\"image/png\" data-original-filesize=\"402\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p></p>
<p></p>
的热力图（在最大池化后）。热力图中的每个像素点具有大小为<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 59px; max-height: 19px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 32.2%;\"></div>
<div class=\"image-view\" data-width=\"59\" data-height=\"19\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-24c0e4a0e73e6d09\" data-original-width=\"59\" data-original-height=\"19\" data-original-format=\"image/png\" data-original-filesize=\"399\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p></p>
<p></p>
的感受野，同时记录了图像对滤波器的响应。因此列特征的大小是<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 83px; max-height: 20px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 24.099999999999998%;\"></div>
<div class=\"image-view\" data-width=\"83\" data-height=\"20\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-5ae245401691f04a\" data-original-width=\"83\" data-original-height=\"20\" data-original-format=\"image/png\" data-original-filesize=\"437\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>，它可以看作是对原始图像中某个图像块的描述。该描述符的每个维度表示相应检测器的激活程度，并且在某种程度上类似于SIFT描述符。列特征最早出现在《Visual instance retrieval with deep convolutional networks》中，Razavian等人首先在分好块的特征图上进行最大池化，然后将它们连接在所有过滤器上，最终生成列特征。在《Hypercolumns for object segmentation and fine-grained localization》中，来自多层的列特征被连接形成“超列”（hypercolumn）特征。</p>
<h5>4.1.3 特征编码与池化</h5>
<p>当提取列特征时，图像由一组描述符表示。为了将这些描述符聚合为全局表示，目前采用了两种策略：编码和直接池合并（如图2所示）。<br>
<strong>编码。</strong> 一组列特征类似于一组SIFT特征,因此可以直接使用标准编码方案。常用的方法就是VLAD和FV算法，两个算法的简要介绍可以参加本文3.3.2节。一个里程碑式的工作发布于《Exploiting local features from deep networks for image retrieval》，文中后首次将列特征用VLAD算法编码。这个想法后来扩展为CNN的微调。BoW编码同样也可以使用，具体工作可以参见《Hybrid multi-layer deep cnn/aggregator feature for image classification》。每个层内的列特征被聚集成一个BoW向量，然后跨层连接。《Bags of local convolutional features for scalable instance search》是固定长度表示的一个例外，这篇文章将列特征用大小为25K的码本量化，还采用了倒排索引结构来提升效率。</p>
<p><strong>池化。</strong> CNN特征与SIFT的主要区别在于前者在每个维度上都有明确的含义,也就是对输入图像的特定区域的滤波器响应。因此，除了上面提到的编码方案之外，直接池化技术也可以产生具有区分度的特征。</p>
<p>这方面的一项里程碑工作包括Tolias等人提出的最大卷积激活（MAC）。在没有扭曲或裁剪图像的情况下，MAC用单个前向传递来计算全局描述符。特别地，MAC计算每个中间特征映射的最大值，并将所有这些值串联在一个卷积层内。在其多区域版本中，使用积分图算法和最似最大算子进行快速计算。随后局部的MAC描述符随着一系列归一化和PCA白化操作被一起合并。我们在本次调研中也注意到了其他一些工作同样也采用了相似的思想，在中间特征映射上采用最大或平均池化，其中Razavian等人的《Visual instance retrieval with deep convolutional networks》是打开先河的工作。同时大家也发现最后一层卷积层（如VGGNet的pool5）在池化后达到的准确率要高于FC描述符以及其他卷积层。</p>
<p>除了直接特征池化，在池化之前给每个层内的特征图分配一些特定的权重也是有益的。在《Aggregating local deep features for image retrieval》中，Babenko等人提出“目标对象往往出现在图像中心”这样一个先验知识，并在总池化前对特征图施加一个2-D高斯掩膜。Xie等人在《Interactive: Inter-layer activeness propagation》中改进了MAC表示法，他们将高层语义和空间上下文传播到底层神经元，以提高这些底层激活神经元的描述能力。Kalantidis等人使用了一个更常规的加权策略，他们同时执行特征映射和信道加权以突出高激活的空间响应，同时减少异常突发情况的影响。</p>
<h3>4.2 使用微调CNN模型的图像检索系统</h3>
<p>虽然预先训练的CNN模型已经取得了令人惊叹的检索性能，但在指定训练集上对CNN模型进行微调也是一个热门话题。当采用微调的CNN模型时，图像级的描述符通常以端到端的方式生成，那么网络将产生最终的视觉表示，而不需要额外的显式编码或合并步骤。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 533px; max-height: 164px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 30.769999999999996%;\"></div>
<div class=\"image-view\" data-width=\"533\" data-height=\"164\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-38a6242232c46d70\" data-original-width=\"533\" data-original-height=\"164\" data-original-format=\"image/jpeg\" data-original-filesize=\"25105\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>表3：用于微调网络方法的实例检索级的数据集统计</p>
<h5>4.2.1 用于微调网络的数据集</h5>
<p>微调网络时使用的数据集对学习高区分度的CNN特征具有至关重要的作用。ImageNet仅提供了图像的类别标签，因此预训练的CNN模型可以对图像的类别进行分类，但却难以区分同一类的图像。因此要面向任务数据集进行CNN模型微调。</p>
<p>近年来用于微调网络方法数据集统计在表3中。数据集主要集中于建筑物和普通物体中。微调网络方向一个里程碑式的工作是《Neural codes for image retrieva》。这篇文章通过一个半自动化的方法收集<strong>地标数据集</strong>：在Yandex搜索引擎中自动地爬取流行的地标，然后手动估计排名靠前的相关图像的比例。该数据集包含672类不同的地标建筑，微调网络在相关的地标数据集，如Oxford5k和假日数据集上表现优异，但是在Ukbench数据集（包含有普通物体）上性能降低了。Babenko等人也在含有300个多角度拍摄的日常物品图像的<strong>多视图RGB-D数据集</strong>上对CNN模型进行了精细调整，以提高在Ukbench数据集上的性能。地标数据集后来被Gordo等人使用，他们使用基于SIFT匹配的自动清洗方法后再微调网络。在《Cnn image retrieval learns from bow: Unsupervised fine-tuning with hard examples》中，Radenovi等人利用检索和运动结构的方法来构建三维地标模型，以便将描述相同建筑的图像进行分组。使用这个标记的数据集，线性判别投影方法（在表5中表示为</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 22px; max-height: 21px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 95.45%;\"></div>
<div class=\"image-view\" data-width=\"22\" data-height=\"21\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-c0636008d72c1f92\" data-original-width=\"22\" data-original-height=\"21\" data-original-format=\"image/png\" data-original-filesize=\"256\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p></p>
<p>）优于先前的白化方法。另一个名为** Tokyo Time Machine**的数据集使用谷歌街景时间机器工具来收集图像，谷歌提供的这个工具可以提供同一地点不同时间的图像。上述的大部分数据集主要关注了地标图像，而Bell等人则建立了一个由家具组成的产品数据集，通过开发众包流程来绘制现场的目标和相应产品之间的连接。对所得到的查询集进行微调也是可行的，但是这种方法可能不适合于新的查询类型。</p>
<h5>4.2.2 微调的网络</h5>
<p>用于微调的CNN结构主要分为两类：基于分类的网络和基于验证的网络。基于分类的网络被训练以将建筑分类为预定义的类别。由于训练集和查询图像之间通常不存在类重叠，因此在AlexNet中如FC6或FC7的学习到的嵌入特征用于基于欧氏距离的检索。该训练/测试策略采用在方框中，其中最后的FC层被修改为具有对应于地标数据集中类的数目的672个节点。在《Neural codes for image retrieval》中采用训练/测试策略，其网络最后的FC层被修改为672个节点，对应于地标数据集中类别数目。</p>
<p></p>
<p></p>
验证网络可以使用孪生网络（siamese network）结合成对损失函数（pairwise loss）或三元损失函数（triplet loss），这种方法已经被更广泛地用于微调网络任务中。在《Learning visual similarity for product design with convolutional neural networks》中采用了基于AlexNet的孪生网络和对比损失函数。在《Cnn image retrieval learns from bow: Unsupervised fine-tuning with hard examples》中Radenovic等人提出用MAC成代替全连接层。更进一步地，可以通过建立的3维建筑模型挖掘训练对。基于共同观测的3D点云（匹配的SIFT特征）的数目来选择正例图像对，而CNN描述符中距离较小的那些图像对被认为是负例样本。这些图像输入到孪生网络中，并且用<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 15px; max-height: 24px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 160.0%;\"></div>
<div class=\"image-view\" data-width=\"15\" data-height=\"24\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-a98ad44dcec8f8a8\" data-original-width=\"15\" data-original-height=\"24\" data-original-format=\"image/png\" data-original-filesize=\"242\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>正则后的MAC层输出计算对比损失函数。与《Cnn image retrieval learns from bow: Unsupervised fine-tuning with hard examples》同时进行的一项工作是《Deep image retrieval: Learning global representations for image search》，Gordo等人在Landmark数据库上对三元损失网络和区域提取网络进行微调。《Deep image retrieval: Learning global representations for image search》这项工作的的优越性在于物体其定位能力，它很好地在特征学习和提取步骤中排除了图像背景。在这两项工作中，微调模型在landmark，OxFoD5K、PARIS6K和Holidays数据集上表现出了最先进的精度，以及在UKBayes数据集上表现出良好的泛化能力（将表5）。在《Netvlad: Cnn architecture for weakly supervised place recognition》中，在最后一个卷积层中插入一个类似VLAD编码层，通过反向传播进行训练。与此同时，设计了一个新的三元损失函数来利用弱监督的Google Street View Time Machine数据。</p>
<h3>4.3 基于CNN模型的混合式方法</h3>
<p>混合式方法中使用多网络传递方式。许多图像块从输入图像中获得并被输入网络中进行特征提取，随后进行编码/池化。由于“检测器+描述符”的方式和基于SIFT的方法很相似，因此我们称其为“混合式”方法。这种方法的效率通常比单通传递要低。</p>
<h5>4.3.1 特征提取</h5>
<p>在混合方法中，特征提取过程包括图像块检测和描述符生成。对第一步而言，主要有三种区域检测器。第一种检测器是网格化图像块。例如，在《Multi-scale orderless pooling of deep convolutional activation features》中使用了两个尺寸滑动窗口的策略来生成图像块。在《Cnn features off-the-shelf: an astounding baseline for recognition》中首先对数据集进行裁剪和旋转，然后将其划分为不同尺度的图像块。第二类是具有不变性的关键点/区域检测器。例如高斯差分特征点在《Learning to compare image patches via convolutional neural networks》中使用。MSER区域检测器在《Descriptor matching with convolutional neural networks: a comparison to sift》中被使用。第三种是区域建议方法，它也同样提供了潜在对象可能的位置信息。Mopuri等人使用选择性搜索策略来提取图像块，而边缘区域方法在《Fisher encoded convolutional bag-of-windows for efficient image retrieval and social image tagging》中使用。在《Faster r-cnn features for instance search》中使用区域建议网络（RPN）来对目标进行定位。</p>
<p>《Descriptor matching with convolutional neural networks: a comparison to sift》证实了CNN一类的区域描述是有效的，并且在出模糊图像之外的图像匹配任务繁重要优于SIFT描述符。对于给定的图像块，混合CNN方法通常使用全连接层或池化的方法来整合CNN特征，相关文献对此均有研究。这些研究从多尺度的图像区域中提取4096维FC特征或目标建议区域。另一方面，Razavian等人还在最大池化后采用中间描述符来作为区域描述符。</p>
<p>上述方法采用预训练模型进行图像块特征提取。以手工检测器为基础，图像块描述符也可以通过有监督或无监督方式进行CNN训练学习，这相对于之前关于SIFT描述符学习的工作有所改进。Yi等人进一步提出了一种在单个流程中集成了区域检测器、方向估计和特征描述符结果的端到端学习方法。</p>
<h5>4.3.2 特征编码与索引</h5>
<p>混合方法的编码/索引过程类似于基于SIFT的检索，如同在小码本下的VLAD / FV编码或大码本下的倒排索引。</p>
<p>VLAD/FV编码过程紧随SIFT特征提取后，在上文已经详细描述过这样的流程，不再赘述。另一方面，有一些工作研究探索了图像块的CNN特征的倒排索引。同样，在SIFT方法流程中诸如HE之类的编码方法也被使用。除了上述提到的编码策略，我们注意到《Cnn features off-the-shelf: an astounding baseline for recognition》，《Visual instance retrieval with deep convolutional networks》，《Image classification and retrieval are one》这些工作提取每个图像的多个区域描述符进行多对多匹配，这种方法称为称为“空间搜索”。该方法提高了检索系统对平移和尺度变化的鲁棒性，但可能会遇到效率问题。另一种使用CNN最高层特征编码的策略是在基于SIFT编码（如FV）的最后面建立一个CNN结构（主要由全连接层组成）。通过在自然图像上训练一个分类模型，中间的全连接层可以被用来进行检索任务。</p>
<h3>4.4 讨论</h3>
<h5>4.4.1 基于SIFT和CNN的方法间的关系</h5>
<p>在本篇综述中，我们将现有的文献分为六个精细的类，表1和表5总结了六个类别的差异和代表性作品。我们的观察结果如下。</p>
<p>第一，混合方法可被视为从SIFT-到基于CNN的方法的过渡方法，除了将CNN特征提取为局部描述符之外，它在所有方面都类似于基于SIFT的方法。由于在图像块特征提取期间需要多次访问网络，因此特征提取步骤的效率可能会受到影响。</p>
<p>第二，单向CNN方法倾向于将SIFT和混合方法中的各个步骤结合起来。在表5中，“预训练单向网络”一类方法整合了特征检测和描述步骤；在“微调单向网络”中，图像级描述符通常是在端到端模式下提取的，因此不需要单独的编码过程。在《Deep image retrieval: Learning global representations for image search》中，集成了类似“PCA”层以减少区分维数，进一步完善了端到端的特征学习。</p>
<p>第三，出于效率上的考虑，特征编码的固定长度表示方法越来越流行。它可以通过聚集局部描述符（SIFT或CNN）、直接汇或端到端特征计算的方法来获得。通常，诸如PCA的降维方法可以在固定长度的特征表达中使用，ANN搜索方法（如PQ或哈希）可用于快速检索。</p>
<h5>4.2.2 哈希与实例检索</h5>
<p>哈希方法是最似最近邻问题的主流解决方案。它可以被分类类为局部敏感哈希（LSH）算法和哈希学习方法。LSH是数据无关的且常通过学习哈希来获得更优异的性能。对于学习哈希方法，最近的一项调研《A survey on learning to hash》将其归类为量化和成对相似性保留这两类。我们在3.3.2节已经详细讨论过量化方法热，不再赘述。成对相似性保留方法包括一些常用的手工设计哈希方法，如谱哈希，LDA哈希等。</p>
<p>近年来随着深度网络的发展，哈希方法也从手工设计的方式转变到受监督的训练方式。这些方法将原始图像作为输入，并在二值化之前生成学习的特征。然而，这些方法大多集中于图像分类式的检索任务，与本次调研所中讨论的实例图像检索不同。实例检索任务中，当可以收集到足够的训练数据时（例如建筑和行人和数据）时，深度散列方法可能是至关重要的。</p>
<h1>5 实验比较</h1>
<h3>5.1 图像检索数据集</h3>
<p>在本次调研中使用了五个流行的实例检索数据集，这些数据集的统计数据如表4所示。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 542px; max-height: 199px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 36.720000000000006%;\"></div>
<div class=\"image-view\" data-width=\"542\" data-height=\"199\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-9e16f285611e63eb\" data-original-width=\"542\" data-original-height=\"199\" data-original-format=\"image/jpeg\" data-original-filesize=\"26896\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>表4：流行的实例检索数据集统计</p>
<p><strong>Holidays</strong>数据集由Jégou等人从个人假日相册中收集，因此图像包含各种各样的场景。该数据库由500组1,491幅相似图像组成，每组图像有一条查询记录，共计500条。除了《Efficient representation of local geometry for large scale object retrieval》和《Learning a fine vocabulary》，大多数基于SIFT的方法手动地将图像旋转成直立方向。最近许多基于CNN的方法也使用了旋转版的Holidays数据集。在表5中这两个版本数据集上的结果用”/“间隔，旋转图像可以带来2%-3%的mAP值。</p>
<p><strong>Ukbench</strong>数据集包括10,200种不同内容的图像，如物体、场景和CD封面。所有图像被分为2,550组，每个组有四个图像描述相同的物体/场景，在不同的角度，照明，形变，等情况下的表现。该数据集中的每个图像依次作为查询记录，因此有10,200条查询记录。</p>
<p><strong>Oxford5k</strong>数据集用牛津11个地标名从Flickr网站爬取共计5062幅图像组建数据集。该数据集通过手绘边界框为每个地标的定义五个查询记录，从而总共存在55个感兴趣区域（ROI）查询记录。每个数据库图像被分配了<em>好的</em>，<em>还可以的</em>，<em>垃圾的</em>，或<em>坏的</em>四个标签之一。前两个标签表示与查询的感兴趣区域是匹配的，而“坏”表示不匹配。在糟糕的图像中，只有不到25％的对象是可见的，或者它们遭受严重的遮挡或变形，因此这些图像对检索精度影响不大。</p>
<p><strong>Flickr100k</strong>数据集包括99,782张来Flickr网站145个最流行标签的高清图像。在文献中，通常将该数据集添加到Oxford5k中，以测试检索算法的可扩展性。</p>
<p><strong>Paros6k</strong>数据集从11指定的巴黎建筑查询中爬出6,412中图像。每个地标有五个查询记录，因此这个数据集同样有55个带有边界框的查询记录。数据库图像使用和Oxford5k一样的四种类型的标签作为Oxford5k数据集标签。针对Oxford5k和Paris6k数据集有两个评估准则。对于基于SIFT的方法，被裁剪的区域通常用于查询。对于基于CNN的方法，有些工作采用的是全尺寸图像，有些工作采用的是将裁剪的ROI传入CNN或者用CNN提取全图特征后再裁剪得到ROI。使用完整的图像的方法可以提高mAP指标。详细的指标可以参见表5。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 596px; max-height: 843px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 120.47000000000001%;\"></div>
<div class=\"image-view\" data-width=\"596\" data-height=\"718\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-ed2518a97925e041\" data-original-width=\"596\" data-original-height=\"718\" data-original-format=\"image/jpeg\" data-original-filesize=\"135086\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>表5：一些有代表性的图像检索方法在六个数据集上的表现</p>
<h3>5.2 评价指标</h3>
<p><strong>精准度-召回率。</strong>召回指的是返回的正确匹配数占数据库中总数或正确匹配数的比率，而精准度是指返回结果中真实匹配的那部分图像。给定一个集合含有</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 13px; max-height: 30px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 230.77%;\"></div>
<div class=\"image-view\" data-width=\"13\" data-height=\"30\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-2cac7a9b516477f8\" data-original-width=\"13\" data-original-height=\"30\" data-original-format=\"image/png\" data-original-filesize=\"220\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p></p>
<p></p>
<p></p>
张返回的图像，假设其中有<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 20px; max-height: 27px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 135.0%;\"></div>
<div class=\"image-view\" data-width=\"20\" data-height=\"27\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-a2df8cb472d293c4\" data-original-width=\"20\" data-original-height=\"27\" data-original-format=\"image/png\" data-original-filesize=\"274\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p></p>
<p></p>
张正确匹配的图像，而整个数据集中有<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 23px; max-height: 27px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 117.39%;\"></div>
<div class=\"image-view\" data-width=\"23\" data-height=\"27\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-a793dd91ca631a42\" data-original-width=\"23\" data-original-height=\"27\" data-original-format=\"image/png\" data-original-filesize=\"311\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p></p>
<p></p>
张正确匹配的图像，那么召回率<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 78px; max-height: 21px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 26.919999999999998%;\"></div>
<div class=\"image-view\" data-width=\"78\" data-height=\"21\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-73a35ee7a4d483ec\" data-original-width=\"78\" data-original-height=\"21\" data-original-format=\"image/png\" data-original-filesize=\"2591\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p></p>
<p></p>
和精准度<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 82px; max-height: 26px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 31.71%;\"></div>
<div class=\"image-view\" data-width=\"82\" data-height=\"26\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-379b596da015aa96\" data-original-width=\"82\" data-original-height=\"26\" data-original-format=\"image/png\" data-original-filesize=\"2777\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p></p>
<p></p>
分别计算为<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 21px; max-height: 35px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 166.67000000000002%;\"></div>
<div class=\"image-view\" data-width=\"21\" data-height=\"35\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-d7381e0b200b18a2\" data-original-width=\"21\" data-original-height=\"35\" data-original-format=\"image/png\" data-original-filesize=\"353\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p></p>
<p></p>
和<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 20px; max-height: 38px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 190.0%;\"></div>
<div class=\"image-view\" data-width=\"20\" data-height=\"38\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-328b6de6c54ee168\" data-original-width=\"20\" data-original-height=\"38\" data-original-format=\"image/png\" data-original-filesize=\"361\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p></p>
<p></p>
。在图像检索中，给定一张待查询图像和返回列表，可以根据<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 313px; max-height: 25px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 7.99%;\"></div>
<div class=\"image-view\" data-width=\"313\" data-height=\"25\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-c4d00dae1b8b6ffb\" data-original-width=\"313\" data-original-height=\"25\" data-original-format=\"image/png\" data-original-filesize=\"6496\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p></p>
<p></p>
这些点绘制精准度-召回率曲线，其中是<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 20px; max-height: 30px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 150.0%;\"></div>
<div class=\"image-view\" data-width=\"20\" data-height=\"30\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-8dcfe30533e5f6ab\" data-original-width=\"20\" data-original-height=\"30\" data-original-format=\"image/png\" data-original-filesize=\"281\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>数据库中的图像数目。</p>
<p><strong>平均准确率和平均精度。</strong> 为了更加清晰地记录图像检索系统的性能，我们使用平均准确率（average precision）对其进行衡量，它相当于精准度-召回率曲线下的面积。通常，较大的AP值意味着更高的精准度-召回率曲线，亦即更好的检索性能。由于图像检索数据集通常具有多个查询图像，所以对它们各自的AP值进行平均，以产生最终的性能评价，即平均精度（mean average precision, mAP）。传统地，我们使用mAP来评估检索系统在Oxford5k、Paris6k和Holidays数据集上的准确度。</p>
<p><strong>N-S得分。</strong> N-S得分专用于Ukbench数据集，它是以David Nistér 和Henrik Stewénius的名字来命名的。N-S得分其实等价于精准度或者召回率，因为在Ukbench数据集中的每个查询在数据库中都有四个正确的匹配项。N-S得分用总排名列表中前四中的真实匹配的平均数量来计算。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 312px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 28.89%;\"></div>
<div class=\"image-view\" data-width=\"1080\" data-height=\"312\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-1b6f99d10ae2d79e\" data-original-width=\"1080\" data-original-height=\"312\" data-original-format=\"image/jpeg\" data-original-filesize=\"70009\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>图6：多年来在Holidays(a), Ukbench(b), Oxford5k(c)数据集上的最优表现</p>
<h3>5.3 比较与分析</h3>
<h5>5.3.1 多年来性能的改进</h5>
<p>我们在图6中展示了过去十年图像检索精度的改善以及在表5中展示了一些有代表性的方法。实验结果通过在独立的数据集上建立的编码本来计算。我们可以清楚地看到，实例检索的领域一直在不断改进。10多年前提出的基线方法（HKM）在Holidays, Ukbench, Oxford5k, Oxford5k+Flickr100k以及Paris6k数据集上的准确率分别仅为59.7%, 2.85, 44.3%, 26.6%以及46.5%。从基线方法开始，通过引入高区分度编码本、空间约束和互补描述符，大规模编码本方法开始稳定地提升。对于中型编码本方法来说，随着Hamming嵌入及其改进的方法，在2008年至2010年间它见证了最显著的精度提升。从那时起，主要的改进来自特征融合的强度，特别是使用在Holiday和Ukbench数据集上提取的的颜色和CNN特征。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 321px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 29.720000000000002%;\"></div>
<div class=\"image-view\" data-width=\"1080\" data-height=\"321\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-af533ce8cdea0789\" data-original-width=\"1080\" data-original-height=\"321\" data-original-format=\"image/jpeg\" data-original-filesize=\"69884\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>图7：特征维度对图像检索系统准确率的影响</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 695px; max-height: 243px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 34.96%;\"></div>
<div class=\"image-view\" data-width=\"695\" data-height=\"243\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-ebdabc8a4309f31d\" data-original-width=\"695\" data-original-height=\"243\" data-original-format=\"image/jpeg\" data-original-filesize=\"43891\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>表6：不同类别方法间的效率和准确度的比较</p>
<p>另一方面，基于CNN的检索模型在图像例检索中迅速显示出其优势。在2012年AlexNet刚提出时，当时的FC特征的性能与SIFT模型相比仍然远不能令人满意。例如，在ImageNet上预训练的AlexNet，其FC描述符在Holidays，Ukbench和Oxford5k数据集上的AP，N-S得分和mAP上的得分分别为 64.2%，3.42,43.3%。这些指标是要比《Contextual weighting for vocabulary tree based image retrieval》在Holidays和Ukbench数据集上的成绩低13.85%和0.14，比《Object retrieval and localization with spatially-constrained similarity measure and k-nn re-ranking》在Oxford5k上的成绩低31.9%。然而，然而，CNN网络结构和微调策略的进步，基于CNN的方法的性能迅速提高，在Holidays和Ukbench数据集上极具竞争力，并且在Oxford5k数据集上的指标略低，但它具的内存消耗更小。</p>
<h5>5.3.2 准确率比较</h5>
<p>不同数据集上不同类别的检索精度可以在图6，表5和表6中查看。从这些结果中，我们有三个发现。</p>
<p>第一，在基于SIFT的方法中，中等规模编码本对的表现要优于小规模编码本。一方面，由于大的沃罗诺伊方格，中等规模编码本的视觉词汇可以使相关匹配的召回率变高。HE方法的进一步集成在很大程度上提高了模型区分度，实现了匹配图像召回率和精度之间较好的平衡。另一方面，虽然小规模编码本中的视觉词具有最高的匹配召回率，但由于聚合过程和维度小，它们的图像区分能力没有显著提高。因此它的表现可以认为是不佳的。</p>
<p>第二，在基于CNN的方法中，微调的模型在特定任务（如地标/场景检索）中的表现要有很大优势，这些任务的数据一般和训练集数据分布相似。虽然这一观察是在预期之内，有趣的是我们发现在《Deep image retrieval: Learning global representations for image search》中提出的微调模型在通用检索（例如Ukbench数据集）上的表现极具竞争力，而它与训练集的数据分布并不同。事实上，Babenko等人在《Neural codes for image retrieval》中表明，在Landmarks数据集上进行微调的CNN特征会降低在Ukbench上的的准确率。《Deep image retrieval: Learning global representations for image search》这项工作的泛化能力可以归因于对区域提取网络的有效训练。相比之下，使用预先训练模型可以在Ukbench上表现出较高的精度，但在landmarks数据集上的表现中等。相似地，混合方法在所有的任务中的表现都相当，但它仍然可能遇到效率问题时。</p>
<p>第三，比较这六中方法，“CNN微调模型”和“SIFT中等编码本”方法具有最好的总体准确度，而“SIFT小编码本”类别具有相对较低的准确度。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 317px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 29.349999999999998%;\"></div>
<div class=\"image-view\" data-width=\"1080\" data-height=\"317\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-27eed89b68ecfb33\" data-original-width=\"1080\" data-original-height=\"317\" data-original-format=\"image/jpeg\" data-original-filesize=\"63991\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>图8：在 Oxford5k数据集上的存储代价和检索准确率</p>
<h5>5.3.3 效率比较</h5>
<p><strong>特征计算时间。</strong> 在基于SIFT的方法中，主要的步骤就是局部特征的提取。通常情况下，根据图像的复杂度（纹理），CPU提取640×480大小图像的基于Hessian仿射区域的SIFT描述符需要1-2s。对于基于CNN的方法，在TitanX卡上通过VGG16网络对一个224×224和1024×768的图像进行单向传递分别需要0.082s和0.34 7s。据报道，四幅图像（最大边724像素）可以在1s内处理。预训练列特征的编码（VLAD或FV）的时间非常快。对于CNN混合方法，提取几十个区域的CNN特征可能需要几秒钟。总体而言，CNN预训练模型和微调模型在用GPU进行特征计算时的效率高。同样应该注意的是，当使用GPU进行SIFT提取时，也可以实现高效率。</p>
<p><strong>检索时间。</strong> 最似最近邻搜索算法用于“SIFT大编码本”，“SIFT小编码本”，“CNN预训练模型”和“CNN微调模型”时都是相当高效的，这是因为倒排列表对于适当训练的大码本来说是简短的，并且因为后者三有一个紧凑的表示，用像PQ这样的ANN搜索方法来加速是可行的。中等规模编码本的效率较低，因为它的倒排索引与大码本相比包含更多的条目，并且汉明嵌入方法的过滤效果只能在一定程度上修正这个问题。如4.3节所述，混合方法的检索复杂度会因为多对多匹配策略的影响而变得低效率。</p>
<p><strong>训练时间。</strong> 用AKM或HKM训练大型或中型编码本通常需要几个小时，使用小型编码本可以缩短训练时间。对于微调模型，Gordo等人在一块K40 GPU上花费了5天训练三元损失模型。可能在孪生网络或者分类模型上这会花费更少的时间，但是要比生成SIFT编码本的时间长得多。因此，在训练方面，使用直接池或小码本的效率更高。</p>
<p><strong>存储代价。</strong> 表5和图8表明具有大码本的SIFT方法和紧凑方法在存储成本上都是高效的。还可以使用PQ或其他有效的量化/散列方法将紧凑表示压缩成紧凑编码，从而可以进一步减少它们的存储消耗。相比之下，使用中等码本的方法是最消耗内存的，因为二进制签名应该存储在倒排索引中。混合方法总要有混合存储成本，因为多对多策略需要存储每个图像的多个区域描述符，而其他一些方法则采用高效的编码方法。</p>
<p><strong>空间验证与查询拓展。</strong> 空间验证通常和QE算法一起使用，可以使得检索结果排列表更加精准。RANSAC验证在《Object retrieval with large vocabularies and fast spatial matching》中提出，它的复杂度为</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 51px; max-height: 24px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 47.06%;\"></div>
<div class=\"image-view\" data-width=\"51\" data-height=\"24\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-8a4f1ad963f9cb09\" data-original-width=\"51\" data-original-height=\"24\" data-original-format=\"image/png\" data-original-filesize=\"486\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p></p>
<p></p>
<p></p>
，其中<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 13px; max-height: 28px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 215.38%;\"></div>
<div class=\"image-view\" data-width=\"13\" data-height=\"28\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-32ddde007b97e405\" data-original-width=\"13\" data-original-height=\"28\" data-original-format=\"image/png\" data-original-filesize=\"232\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p></p>
<p></p>
是匹配的特征数目，可以看出算法的复杂度较高。ADV方法的复杂度相对较小，为<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 78px; max-height: 24px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 30.769999999999996%;\"></div>
<div class=\"image-view\" data-width=\"78\" data-height=\"24\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-2ef36e458a2af0c1\" data-original-width=\"78\" data-original-height=\"24\" data-original-format=\"image/png\" data-original-filesize=\"2732\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p></p>
<p></p>
，因为它能够避免不相关的Hough选票。《Hough pyramid matching: Speeded-up geometry re-ranking for large scale image retrieval》和《A vote-and-verify strategy for fast spatial verification in image retrieval》提出的方法最有效，复杂度仅为<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 42px; max-height: 24px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 57.14%;\"></div>
<div class=\"image-view\" data-width=\"42\" data-height=\"24\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-2188f568968889c1\" data-original-width=\"42\" data-original-height=\"24\" data-original-format=\"image/png\" data-original-filesize=\"445\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>，同时后一项工作进一步地输出QE的变换和内值。</p>
<p>从查询扩展的角度来看，由于提出了新的查询，搜索效率会受到影响。例如，由于新查询，AQE的搜索时间几乎增加了一倍。对于递归AQE和带尺度递归QE方法，搜索时间更加长了，因为要执行好几个新的搜索。其他QE变体所提出的改进只比执行另一搜索增加了边际成本，因此它们的复杂性类似于QE方法。</p>
<h5>5.3.4 重要的参数</h5>
<p>我们总结编码本大小对使用SIFT特征的大/中型码本的影响，以及维数对包括SIFT小编码本和基于CNN方法的紧凑表示的影响。</p>
<p><strong>编码本规模。</strong> 图8展示了模型在Oxford5k上的mAP结果，对大规模编码本和中规模编码本的方法进行对比。有两点值得注意。第一，mAP值通常随着编码本增大而增加，但当码本足够大时aMP值可能达到饱和。这是因为更大的码本提高了匹配精度，但是如果它太大，匹配的召回率变低，导致性能饱和甚至损害性能。第二，当编码本规模变化时，使用中等规模编码本的方法表现更稳定。这可以归因于HE方法，它对更小的码本贡献更多，弥补了较低的基线方法的性能。</p>
<p><strong>维数。</strong> 维数对紧凑向量的影响在图7中给出。我们的发现检索精度通常在较大的尺寸下较为稳定，而当维数低于256或128时精度迅速下降。我们第二个发现是关于区域提取的。这些方法在各种特征长度下都表现出非常出色的性能，这可能是由于它们在目标定位方面的优越能力。</p>
<h5>5.3.5 讨论</h5>
<p>我们简要地讨论何时使用CNN或SIFT以及其他相关方法。上文对两者特征进行了详细的比较。</p>
<p>一方面，表示向量长度固定的CNN方法几乎在所有的基准数据集上的性能都占有优势。具体而言，在两种情况下基于CNN的方法可以考虑优先使用。第一种是对于特定对象的检索（例如建筑物、行人），当提供的训练数据足够时，可以充分利用CNN网络嵌入学习的能力。第二种，对于常见的对象检索或类检索，预训练的CNN模型是有竞争力的。</p>
<p>另一方面，尽管基于CNN方法的通常是具有优势的，我们仍认为SIFT特征在某些情况下仍然具有优势。例如，当查询或一些目标图像是灰度图像时，CNN可能不如SIFT有效，因为SIFT是在灰度图像上计算而不诉诸于颜色信息。当物体颜色变化非常剧烈时也同样如此。另外，在小对象检索中或当查询对象被严重遮挡时，使用诸如SIFT之类的局部特征是更好的选择。在书籍/CD封面检索等应用中，由于丰富的纹理，我们也可以期待SIFT的良好性能。</p>
<h1>6 未来的研究方向</h1>
<h3>6.1 面向通用任务的实例检索</h3>
<p>图像检索一个非常重要的方向就是使用搜索引擎实现通用检索。为了实现这个目标需要解决两个重要问题。</p>
<p>第一，需要引入大规模图像数据集。虽然如表3所示展示了多个图像数据集，但这些数据集通常包含特定类型的实例，例如地标或室内物品。虽然Gordo等人在《Deep image retrieval: Learning global representations for image search》中使用的RPN结构除了在构建数据集之外，还在在Ukbench数据集上表现得富有竞争力，但如果在更通用的数据集上训练CNN能否带来进一步的改进，则仍然是未知数。因此，社区迫切需要大规模的图像数据集或一种可以以监督或非监督的方式生成这样一个数据集的有效方法。</p>
<p>第二，设计新的CNN网络和学习策略对于充分利用训练数据具有重要意义。先前有工作采用标准分类模型，成对损失或三重损失模型对CNN网络进行微调。Faster R-CNN在实例检索中的引入对更精确的对象定位来说是一个良好的开始。此外，在另一个检索任务中采用微调模型时，迁移学习方法也是非常重要。</p>
<h3>6.2 面向专用任务的实例检索</h3>
<p>另一方面，在专用实例检索中的研究也越来越多。例如地点检索，行人检索，车辆检索，标志检索等。在这些任务中的图像具有特定的先验知识。例如在行人检索任务中，循环神经网络（RNN）可以连接身体部分的描述符，在车辆检索任务中，在特征学习期间可以推断视图信息，同时牌照图像也可以提供关键信息。</p>
<p>同时，训练数据的收集过程可以进一步研究。例如，可以通过谷歌街景收集不同地点的训练图像。车辆图像可以通过监视视频或互联网图像来获取。在这些特定的数据集上探索新的学习策略以及研究迁移学习的效果将是有趣的。最后，紧凑向量编码或短编码也将在现实的检索任务设置中变得重要。</p>
<h1>7 结语</h1>
<p>本篇综述回顾了基于SIFT和CNN特征的实例检索方法。根据编码本的规模，我们将基于SIFT的方法分为三类：使用大，中，小规模的编码本。基于CNN的方法也被分为了三类：使用预训练模型，微调模型和混合模型的方法。在每个类别下都对先前的方法进行了全面的调研。从各种方法的演变可以看出，混合方法处于SIFT和CNN方法的过渡位置，紧凑编码方法越来越流行，并且实例检索正朝着端到端的特征学习和提取的方向发展。</p>
<p>通过在几个基准数据集上收集的实验结果，对六种方法进行了比较。我们发现CNN微调模型策略在不同的检索任务上都得到了较高准确率，并且在效率上也具有优势。未来的研究可能集中于学习更通用的特征表示或更特定场景的检索任务。</p>
<p>****-------------<strong>原文结束</strong>-------------****</p>
<h1>写在最后</h1>
<p>毋庸置疑，这是一篇极高水准的综述，本人斗胆将其翻译成文愿对大家有所帮助。</p>
<p>图像检索系统作为计算机视觉领域的一项关键技术，以其诱人的前景不断激励着一代代的科研工作者攻克难点，追求卓越。从文中我们可以看到，仅仅十多年的时间，图像检索方面的研究已经取得了不俗的成绩，尤其是近年来兴起的深度学习技术又为该领域的发展添加了助燃剂。</p>
<p>然而，我们也应该意识到挑战仍然是存在且巨大的。更加鲁棒的图像特征描述符，更加快速的检索算法都是我们需要继续研究的课题。同志们，任重道远啊！</p>
<hr>
<p>欢迎与我交流</p>
<p>github:</p>
<p><a href=\"https://github.com/keloli\" target=\"_blank\" rel=\"nofollow\">https://github.com/keloli</a></p>
<p>blog:</p>
<p><a href=\"https://www.jianshu.com/u/d055ee434e59\" target=\"_blank\">https://www.jianshu.com/u/d055ee434e59</a></p>
<hr>
<h1>参考资料</h1>
<ul>
<li><p><a href=\"(https://ieeexplore.ieee.org/abstract/document/7935507/)\" target=\"_blank\" rel=\"nofollow\">SIFT Meets CNN: A Decade Survey of Instance Retrieval @TPAMI  Volume 40 Issue 5  May 2018</a></p></li>
<li><p><a href=\"https://arxiv.org/abs/1608.01807\" target=\"_blank\" rel=\"nofollow\">SIFT Meets CNN: A Decade Survey of Instance Retrieval @arxiv</a></p></li>
<li><p><a href=\"https://blog.csdn.net/happyer88/article/details/47054503\" target=\"_blank\" rel=\"nofollow\">VLAD特征(vector of locally aggregated descriptors)</a></p></li>
<li><p><a href=\"https://www.cnblogs.com/mafuqiang/p/7161592.html\" target=\"_blank\" rel=\"nofollow\">乘积量化（Product Quantization）</a></p></li>
</ul>
<hr>
<blockquote>
<p>微信：TonyJeemy520      加个人微信拉你进机器学习、深度学习交流群，请备注 : 来自简书<br>
QQ交流群：651616387   请备注 : 来自简书<br>
微信公众号：机器学习算法工程师   ----二维码见下图</p>
</blockquote>
<hr>
<p><strong>扫码关注微信号：机器学习算法工程师，更多干货分享， 或加个人微信，拉你进机器学习、深度学习交流群</strong><br>
</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 430px; max-height: 430px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 100.0%;\"></div>
<div class=\"image-view\" data-width=\"430\" data-height=\"430\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-0bdf004ac80c0197\" data-original-width=\"430\" data-original-height=\"430\" data-original-format=\"image/jpeg\" data-original-filesize=\"32906\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p></p>

          </div>','1531183712'),
('325570','{30}{975728}{62}{72}{36}{503}{6697}{46050}{1124209}{39033}{975757}{39032}{975761}{1077}{131}{975969}{975717}{17}{22017}{2123}{1024396}{1095191}{975910}{425514}{33307}{13013}{408}{5669}{975915}{975714}{975830}{975797}{247}{257}{26}{1635}{975879}{975776}{785}{975900}{975768}{975779}{975733}{6261}{990}{975919}{2790}{779}{517}','老旧黑白片修复机——使用卷积神经网络图像自动着色实战（附PyTorch代码） 摘要：照片承载了很多人在某个时刻的记忆，尤其是一些老旧的黑白照片，尘封于脑海之中，随着时间的流逝，记忆中对当时颜色的印象也会慢慢消散，这确实有些可惜。技术的发展会解决一些现有的难题，深度学习恰好能够解决这个问题。 人工智能和深度学习技术逐渐在各行各业中发挥着作用，尤','老旧黑白片修复机——使用卷积神经网络图像自动着色实战（附PyTorch代码）','<div class=\"show-content-free\">
            <blockquote><p><b>

摘要：</b>照片承载了很多人在某个时刻的记忆，尤其是一些老旧的黑白照片，尘封于脑海之中，随着时间的流逝，记忆中对当时颜色的印象也会慢慢消散，这确实有些可惜。技术的发展会解决一些现有的难题，深度学习恰好能够解决这个问题。</p></blockquote>
<p>人工智能和深度学习技术逐渐在各行各业中发挥着作用，尤其是在计算机视觉领域，深度学习就像继承了某些上帝的功能，无所不能，令人叹为观止。照片承载了很多人在某个时刻的记忆，尤其是一些老旧的黑白照片，尘封于脑海之中，随着时间的流逝，记忆中对当时颜色的印象也会慢慢消散，这确实有些可惜。但随着科技的发展，这些已不再是比较难的问题。在这篇文章中，将带领大家领略一番深度学习的强大能力——将灰度图像转换为彩色图像。文章使用PyTorch从头开始构建一个机器学习模型，自动将灰度图像转换为彩色图像，并且给出了相应代码及图像效果图。整篇文章都是通过iPython Notebook中实现，对性能的要求不高，读者们可以自行动手实践一下在各自的计算机上运行下，亲身体验下深度学习神奇的效果吧。</p>
<p>PS：不仅能够对旧图像进行着色，还可以对视频（每次对视频进行一帧处理）进行着色哦！闲话少叙，下面直接进入正题吧。</p>
<h4>简介</h4>
<p>在图像着色任务中，我们的目标是在给定灰度输入图像的情况下生成彩色图像。这个问题是具有一定的挑战性，因为它是多模式的——单个灰度图像可能对应许多合理的彩色图像。因此，传统模型通常依赖于重要的用户输入以及输入的灰度图像内容。</p>
<p>最近，深层神经网络在自动图像着色方面取得了显着的成功——从灰度到彩色，无需额外的人工输入。这种成功的部分原因在于深层神经网络能够捕捉和使用语义信息（即图像的实际内容），尽管目前还不能够确定这些类型的模型表现如此出色的原因，因为深度学习类似于黑匣子，暂时无法弄清算法是如何自动学习，后续会朝着可解释性研究方向发展。</p>
<p>在解释模型之前，首先以更精确地方式阐述我们所面临的问题。</p>
<h4>问题</h4>
<p>我们的目的是要从灰度图像中推断出每个像素（亮度、饱和度和色调）具有3个值的全色图像，对于灰度图而言，每个像素仅具有1个值（仅亮度）。为简单起见，我们只能处理大小为256 x 256的图像，所以我们的输入图像大小为256 x 256 x 1（亮度通道），输出的图像大小为256 x 256 x 2（另两个通道）。</p>
<p>正如人们通常所做的那样，我们不是用RGB格式的图像进行处理，而是使用<a href=\"https://en.wikipedia.org/wiki/CIELAB_color_space\" target=\"_blank\" rel=\"nofollow\">LAB色彩空间</a>（亮度，A和B）。该色彩空间包含与RGB完全相同的信息，但它将使我们能够更容易地将亮度通道与其他两个（我们称之为A和B）分开。在稍后会构造一个辅助函数来完成这个转换过程。</p>
<p>此外将尝试直接预测输入图像的颜色值（即回归）。还有其他更有趣的<a href=\"http://richzhang.github.io/colorization/\" target=\"_blank\" rel=\"nofollow\">分类方法</a>，但目前坚持使用回归方法，因为它很简单且效果很好。</p>
<h4>数据</h4>
<p>着色数据无处不在，这是由于我们可以从任何一张彩色图像中提取出灰度通道。对于本文项目，我们将使用<a href=\"http://places.csail.mit.edu/\" target=\"_blank\" rel=\"nofollow\">MIT地点数据集</a>中的一个子集，该子数据集包含地点、景观和建筑物。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 697px; max-height: 86px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 12.34%;\"></div>
<div class=\"image-view\" data-width=\"697\" data-height=\"86\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2509688-9f90bd043756dcde.png\" data-original-width=\"697\" data-original-height=\"86\" data-original-format=\"image/png\" data-original-filesize=\"5780\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 695px; max-height: 199px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 28.63%;\"></div>
<div class=\"image-view\" data-width=\"695\" data-height=\"199\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2509688-9abf79e1c53c9b82.png\" data-original-width=\"695\" data-original-height=\"199\" data-original-format=\"image/png\" data-original-filesize=\"15171\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 256px; max-height: 256px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 100.0%;\"></div>
<div class=\"image-view\" data-width=\"256\" data-height=\"256\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2509688-8ab1f22bb78db594.png\" data-original-width=\"256\" data-original-height=\"256\" data-original-format=\"image/png\" data-original-filesize=\"134703\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<h4>工具</h4>
<p>   本文使用PyTorch构建和训练搭建的模型。此外，我们还了使用torchvision工具，该工具在PyTorch中处理图像和视频时很有用，以及使用了scikit-learn工具，用于在RGB和LAB颜色空间之间进行转换。</p>
<p><br></p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 695px; max-height: 62px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 8.92%;\"></div>
<div class=\"image-view\" data-width=\"695\" data-height=\"62\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2509688-a7e81230fe00baaf.png\" data-original-width=\"695\" data-original-height=\"62\" data-original-format=\"image/png\" data-original-filesize=\"4114\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 698px; max-height: 322px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 46.129999999999995%;\"></div>
<div class=\"image-view\" data-width=\"698\" data-height=\"322\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2509688-8034964bff62c053.png\" data-original-width=\"698\" data-original-height=\"322\" data-original-format=\"image/png\" data-original-filesize=\"17485\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 697px; max-height: 65px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 9.33%;\"></div>
<div class=\"image-view\" data-width=\"697\" data-height=\"65\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2509688-00c72a98b9b7b72b.png\" data-original-width=\"697\" data-original-height=\"65\" data-original-format=\"image/png\" data-original-filesize=\"3273\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<h4>模型</h4>
<p>模型采用卷积神经网络构建而成，与传统的卷积神经网络模型类似，首先应用一些卷积层从图像中提取特征，然后将反卷积层应用于高级（增加空间分辨率）特征。</p>
<p>具体来说，模型采用的是迁移学习的方法，基础是ResNet-18模型，ResNet-18网络具有18层结构以及剩余连接的图像分类网络层。我们修改了该网络的第一层，以便它接受灰度输入而不是彩色输入，并且切断了第六层后面的网络结构：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 690px; max-height: 348px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 50.43%;\"></div>
<div class=\"image-view\" data-width=\"690\" data-height=\"348\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2509688-f57736ebfde89c70.png\" data-original-width=\"690\" data-original-height=\"348\" data-original-format=\"image/png\" data-original-filesize=\"148676\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>现在，在代码中定义后续的网络模型，将从网络的后半部分开始，即上采样层：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 694px; max-height: 787px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 112.53999999999999%;\"></div>
<div class=\"image-view\" data-width=\"694\" data-height=\"781\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2509688-fc4572637cac2af0.png\" data-original-width=\"694\" data-original-height=\"781\" data-original-format=\"image/png\" data-original-filesize=\"50337\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>现在通过下面的代码创建整个模型：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 698px; max-height: 48px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 6.88%;\"></div>
<div class=\"image-view\" data-width=\"698\" data-height=\"48\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2509688-210ba46d66ce1ec6.png\" data-original-width=\"698\" data-original-height=\"48\" data-original-format=\"image/png\" data-original-filesize=\"1882\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<h4>训练</h4>
<p><b>损失函数</b></p>
<p>   由于使用的是回归方法，所以使用的仍然是均方误差损失函数：尝试最小化预测的颜色值与真实（实际值）颜色值之间的平方距离。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 696px; max-height: 48px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 6.9%;\"></div>
<div class=\"image-view\" data-width=\"696\" data-height=\"48\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2509688-cf32d8172ee5beb3.png\" data-original-width=\"696\" data-original-height=\"48\" data-original-format=\"image/png\" data-original-filesize=\"1814\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>由于问题的多形式性，上述损失函数对于着色有一点小的问题。例如，如果一件灰色的衣服可能是红色或蓝色，而模型若选择错误的颜色时，则会受到严厉的惩罚。因此，构建的模型通常会选择与饱和度鲜艳的颜色相比不太可能“非常错误”的不饱和颜色。关于这个问题已经有了重要的研究（参见Zhang等人），但是本文将坚持这种损失函数，就是这么任性。</p>
<h4>优化</h4>
<p>   使用Adam优化器优化选定的损失函数（标准）。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 698px; max-height: 47px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 6.7299999999999995%;\"></div>
<div class=\"image-view\" data-width=\"698\" data-height=\"47\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2509688-85137087be90d56f.png\" data-original-width=\"698\" data-original-height=\"47\" data-original-format=\"image/png\" data-original-filesize=\"3069\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<h4>加载数据</h4>
<p>   使用torchtext来加载数据，由于我们需要LAB空间中的图像，所以首先必须定义一个自定义数据加载器（dataloader）来转换图像。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 698px; max-height: 353px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 50.57000000000001%;\"></div>
<div class=\"image-view\" data-width=\"698\" data-height=\"353\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2509688-8049d651816301be.png\" data-original-width=\"698\" data-original-height=\"353\" data-original-format=\"image/png\" data-original-filesize=\"24542\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p> 接下来，对训练数据和验证数据定义变换。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 661px; max-height: 315px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 47.660000000000004%;\"></div>
<div class=\"image-view\" data-width=\"661\" data-height=\"315\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2509688-54731440e2a016b6.png\" data-original-width=\"661\" data-original-height=\"315\" data-original-format=\"image/png\" data-original-filesize=\"22968\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<h4>辅助函数</h4>
<p>   在进行训练之前，定义了辅助函数来跟踪训练损失并将图像转换回RGB图像。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 661px; max-height: 696px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 105.3%;\"></div>
<div class=\"image-view\" data-width=\"661\" data-height=\"696\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2509688-04c134bc269c8393.png\" data-original-width=\"661\" data-original-height=\"696\" data-original-format=\"image/png\" data-original-filesize=\"50150\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<h4>验证</h4>
<p>   在验证过程中，使用torch.no_grad（）函数简单地运行下没有反向传播的模型。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 497px; max-height: 1122px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 160.35999999999999%;\"></div>
<div class=\"image-view\" data-width=\"497\" data-height=\"797\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2509688-08fe94a217ea62df.jpg\" data-original-width=\"497\" data-original-height=\"797\" data-original-format=\"image/jpeg\" data-original-filesize=\"326634\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<h4>训练</h4>
<p>   在训练过程中，使用loss.backward（）运行模型并进行反向传播过程。我们首先定义了一个训练一个epoch的函数：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 497px; max-height: 1050px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 150.1%;\"></div>
<div class=\"image-view\" data-width=\"497\" data-height=\"746\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2509688-fa5867fd88bdc3c9.jpg\" data-original-width=\"497\" data-original-height=\"746\" data-original-format=\"image/jpeg\" data-original-filesize=\"273896\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>  接下来，我们定义一个循环训练函数，即训练100个epoch：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 697px; max-height: 90px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 12.91%;\"></div>
<div class=\"image-view\" data-width=\"697\" data-height=\"90\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2509688-2ff6a828f4c5e8c8.png\" data-original-width=\"697\" data-original-height=\"90\" data-original-format=\"image/png\" data-original-filesize=\"4521\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 698px; max-height: 146px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 20.919999999999998%;\"></div>
<div class=\"image-view\" data-width=\"698\" data-height=\"146\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2509688-14cee61c43498c28.png\" data-original-width=\"698\" data-original-height=\"146\" data-original-format=\"image/png\" data-original-filesize=\"9341\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 661px; max-height: 267px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 40.39%;\"></div>
<div class=\"image-view\" data-width=\"661\" data-height=\"267\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2509688-f0526afbc760aca9.png\" data-original-width=\"661\" data-original-height=\"267\" data-original-format=\"image/png\" data-original-filesize=\"18429\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 697px; max-height: 47px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 6.74%;\"></div>
<div class=\"image-view\" data-width=\"697\" data-height=\"47\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2509688-f6e63fc5a258eb4f.png\" data-original-width=\"697\" data-original-height=\"47\" data-original-format=\"image/png\" data-original-filesize=\"1825\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<h4>预训练模型</h4>
<p>   如果你想运用预训练模型而不想从头开始训练的话，我已经为你训练了好一个模型。该模型在少量时间内接受相对少量的数据训练，并且能够工作正常。可以从下面的链接下载并使用它：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 697px; max-height: 88px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 12.629999999999999%;\"></div>
<div class=\"image-view\" data-width=\"697\" data-height=\"88\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2509688-37b2e8ccc73d7600.png\" data-original-width=\"697\" data-original-height=\"88\" data-original-format=\"image/png\" data-original-filesize=\"6755\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 660px; max-height: 109px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 16.520000000000003%;\"></div>
<div class=\"image-view\" data-width=\"660\" data-height=\"109\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2509688-17f11863fde19358.png\" data-original-width=\"660\" data-original-height=\"109\" data-original-format=\"image/png\" data-original-filesize=\"6745\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 696px; max-height: 93px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 13.36%;\"></div>
<div class=\"image-view\" data-width=\"696\" data-height=\"93\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2509688-d7b76984404e96c0.png\" data-original-width=\"696\" data-original-height=\"93\" data-original-format=\"image/png\" data-original-filesize=\"5202\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 695px; max-height: 59px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 8.49%;\"></div>
<div class=\"image-view\" data-width=\"695\" data-height=\"59\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2509688-30266ed5d22d7e2b.png\" data-original-width=\"695\" data-original-height=\"59\" data-original-format=\"image/png\" data-original-filesize=\"4475\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<h4>结果</h4>
<p>   有趣的内容到了，让我们看看深度学习技术实现的效果吧！</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 663px; max-height: 362px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 54.6%;\"></div>
<div class=\"image-view\" data-width=\"663\" data-height=\"362\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2509688-e5a830a41781e31e.png\" data-original-width=\"663\" data-original-height=\"362\" data-original-format=\"image/png\" data-original-filesize=\"24285\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 415px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 47.43%;\"></div>
<div class=\"image-view\" data-width=\"875\" data-height=\"415\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2509688-36ada32c92f73357.png\" data-original-width=\"875\" data-original-height=\"415\" data-original-format=\"image/png\" data-original-filesize=\"244191\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 415px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 47.43%;\"></div>
<div class=\"image-view\" data-width=\"875\" data-height=\"415\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2509688-8c7afca0a73591fe.png\" data-original-width=\"875\" data-original-height=\"415\" data-original-format=\"image/png\" data-original-filesize=\"228086\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<h4>结论</h4>
<p>   在这篇文章中，使用PyTorch工具从头创建了一个简单的自动图像着色器，没有太复杂的代码，只需要简单的准备好数据并设计好合理的模型即可得到令人令人兴奋的结果，此外，这仅仅只是起步，后续还有很多地方可以进行改进优化并进行推广。</p>
<p><a href=\"https://promotion.aliyun.com/ntms/act/ambassador/sharetouser.html?spm=a2c4e.11153940.blogcont596972.9.3b3f4901dN50c8&amp;userCode=j4nkrg1c&amp;utm_source=j4nkrg1c\" target=\"_blank\" rel=\"nofollow\">数十款阿里云产品限时折扣中，赶紧点击领劵开始云上实践吧！</a></p>
<p>本文作者：【方向】</p>
<p><a href=\"http://click.aliyun.com/m/51413/\" target=\"_blank\" rel=\"nofollow\">阅读原文</a></p>
<p>本文为云栖社区原创内容，未经允许不得转载。</p>
          </div>','1531183713'),
('325571','{39}{36}{779}{1300}{257}{1094771}{17}{51396}{975757}{1124225}{976193}{975728}{975773}{250}{1740}{975789}{4338}{975785}{975761}{36586}{1124228}{555}{4442}{165}{146960}{1468}{1427}{247}{2397}{975720}{976042}{62}{1846}{955}{2779}{21879}{2217}{101658}{17666}{1476}{1124232}{1137}{2269}{13023}{1124233}{463}{1124235}{13844}{975798}{182996}','】 所有图片都转化为灰度图,代码如下 import cv2 as cv img = cv.imread(image) img=cv.cvtColor(img,cv.COLOR_RGB2GRAY) 将数据集组织好后，放入./data文件夹下 网络训练 Pytorch 使用github上Mobilenet公布的源码：pytorch-mobilenet-master 启动训练代码： CUDA_VISIBLE_DEVICES=3 python main.py -a mobilenet --resume mobilenet_sgd.pth.tar --lr 0.01 .format(args.resume, checkpoint[\'epoch\'])) else: print(\"= no checkpoint found at \'{}\'\".','使用Mobilenet进行车辆违章行为的检测','<div class=\"show-content-free\">
            <p>学校大创项目做了关于车辆违章检测的模型，现在简单记录一下~~~<br>
项目主要的模块为<strong>车辆目标检测+车辆违章行为检测+车牌识别+微信小程序开发</strong></p>
<p>现在主要介绍车辆违章行为检测部分，微信小程序开发见我的另一篇文章<a href=\"https://www.jianshu.com/p/28cc6637ebac\" target=\"_blank\">Django+uwsgi+nginx微信小程序环境搭建</a></p>
<h3>选取网络</h3>
<p>在项目中违章行为识别的思想主要是分类问题，可以简化为<strong>二分类</strong>（违章+非违章），或者复杂一点的<strong>多分类</strong>（将违章的情况细分为压实线、占用自行车道、占用人行横道等）<br>
当然更好的方法是通过<strong>检测</strong>一些可能造成违章的标识，如禁止停车、自行车道标志、白色实线等，但考虑到复杂程度，我还是选择了分类【笑哭】<br>
<strong>最终选取了较简单的Mobilenet网络，其中心思想是深度可分卷积，所以速度很快，并非常适合分类问题。</strong><br>
</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 362px; max-height: 479px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 132.32%;\"></div>
<div class=\"image-view\" data-width=\"362\" data-height=\"479\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/7394071-829539698f7c3e0e.png\" data-original-width=\"362\" data-original-height=\"479\" data-original-format=\"image/png\" data-original-filesize=\"50549\"></div>
</div>
<div class=\"image-caption\">Mobilenet 深度可分卷积</div>
</div>
<p></p>
<h3>准备数据集</h3>
<p>由于涉及个人隐私等问题，与交管部门沟通无果，只好通过网络爬虫和自己拍摄来收集数据集。。。<br>
因为数量较少，所以在训练时使用了<strong>数据增强</strong><br>
数据集中违章与非违章的比例约为1：2<br>
训练集与数据集的比例约为10：1，没有设置验证集【数据实在是太少了呜呜呜...】<br>
所有图片都转化为灰度图,代码如下</p>
<pre><code>import cv2 as cv
img = cv.imread(image) 
img=cv.cvtColor(img,cv.COLOR_RGB2GRAY)
</code></pre>
<p>将数据集组织好后，放入./data文件夹下</p>
<h3>网络训练 Pytorch</h3>
<p>使用github上Mobilenet公布的源码：<a href=\"https://github.com/marvis/pytorch-mobilenet\" target=\"_blank\" rel=\"nofollow\">pytorch-mobilenet-master</a></p>
<h5>启动训练代码：</h5>
<pre><code>CUDA_VISIBLE_DEVICES=3 python main.py -a mobilenet --resume mobilenet_sgd.pth.tar --lr 0.01 ./data &gt; log.txt
</code></pre>
<h5>网络结构</h5>
<pre><code>class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()

        def conv_bn(inp, oup, stride):
            return nn.Sequential(
                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),#inp:input channel,oup:output channel
                nn.BatchNorm2d(oup),
                nn.ReLU(inplace=True)
            )

        def conv_dw(inp, oup, stride):
            return nn.Sequential(
                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),
                nn.BatchNorm2d(inp),
                nn.ReLU(inplace=True),
    
                nn.Conv2d(inp, oup, 1, 1, 0, bias=False),
                nn.BatchNorm2d(oup),
                nn.ReLU(inplace=True),
            )
        #哈哈哈哈在这里可见pytorch真是简单啊~~~
        self.model = nn.Sequential(
            conv_bn(  3,  32, 2), 
            conv_dw( 32,  64, 1),
            conv_dw( 64, 128, 2),
            conv_dw(128, 128, 1),
            conv_dw(128, 256, 2),
            conv_dw(256, 256, 1),
            conv_dw(256, 512, 2),
            conv_dw(512, 512, 1),
            conv_dw(512, 512, 1),
            conv_dw(512, 512, 1),
            conv_dw(512, 512, 1),
            conv_dw(512, 512, 1),
            conv_dw(512, 1024, 2),
            conv_dw(1024, 1024, 1),
            nn.AvgPool2d(7),
        )
        self.fc1 = nn.Linear(1024, 2)  #这里将输出改为2，因为是二分类
    def forward(self, x):
        x = self.model(x)
        x = x.view(-1, 1024)
        x = self.fc1(x)
        return x

</code></pre>
<h5>参数设置</h5>
<pre><code>parser = argparse.ArgumentParser(description=\'PyTorch ImageNet Training\')
#数据集存放的位置
parser.add_argument(\'data\', metavar=\'DIR\',
                    help=\'path to dataset\')
#使用的网络结构  -a mobilenet
parser.add_argument(\'--arch\', \'-a\', metavar=\'ARCH\', default=\'resnet18\',
                    choices=model_names,
                    help=\'model architecture: \' +
                        \' | \'.join(model_names) +
                        \' (default: resnet18)\')
parser.add_argument(\'-j\', \'--workers\', default=4, type=int, metavar=\'N\',
                    help=\'number of data loading workers (default: 4)\')
#训练的epoch总数
parser.add_argument(\'--epochs\', default=90, type=int, metavar=\'N\',
                    help=\'number of total epochs to run\')
#每次训练从第几个epoch开始
parser.add_argument(\'--start-epoch\', default=0, type=int, metavar=\'N\',
                    help=\'manual epoch number (useful on restarts)\')
#设置batch-size
parser.add_argument(\'-b\', \'--batch-size\', default=32, type=int,
                    metavar=\'N\', help=\'mini-batch size (default: 32)\')
#设置学习率
parser.add_argument(\'--lr\', \'--learning-rate\', default=0.1, type=float,
                    metavar=\'LR\', help=\'initial learning rate\')
#设置动量
parser.add_argument(\'--momentum\', default=0.9, type=float, metavar=\'M\',
                    help=\'momentum\')
parser.add_argument(\'--weight-decay\', \'--wd\', default=1e-4, type=float,
                    metavar=\'W\', help=\'weight decay (default: 1e-4)\')
parser.add_argument(\'--print-freq\', \'-p\', default=10, type=int,
                    metavar=\'N\', help=\'print frequency (default: 10)\')
#设置选用的预训练模型  项目中使用mobilenet提供的模型：mobilenet_sgd.pth.tar
parser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\',
                    help=\'path to latest checkpoint (default: none)\')
parser.add_argument(\'-e\', \'--evaluate\', dest=\'evaluate\', action=\'store_true\',
                    help=\'evaluate model on validation set\')
parser.add_argument(\'--pretrained\', dest=\'pretrained\', action=\'store_true\',
                    help=\'use pre-trained model\')

</code></pre>
<h5>加载预训练模型</h5>
<p><code>pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}</code><br>
注意只挑选共同存在的部分加载</p>
<pre><code># optionally resume from a checkpoint
    if args.resume:
        if os.path.isfile(args.resume):
            print(\"=&gt; loading checkpoint \'{}\'\".format(args.resume))
            checkpoint = torch.load(args.resume)
            args.start_epoch=0
            best_prec1 = checkpoint[\'best_prec1\']

            pretrained_dict=checkpoint[\'state_dict\']
            model_dict = model.state_dict()
            #注意这里！因为对网络结构进行了修改，所以这里加载resume时，只挑选共同存在的部分加载！！
            pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}
            model_dict.update(pretrained_dict)
            model.load_state_dict(model_dict)

            print(\"=&gt; loaded checkpoint \'{}\' (epoch {})\"
                  .format(args.resume, checkpoint[\'epoch\']))
        else:
            print(\"=&gt; no checkpoint found at \'{}\'\".format(args.resume))

    cudnn.benchmark = True

</code></pre>
<h5>数据集加载</h5>
<p>直接调用pytorch用来导入数据的API，附上<a href=\"http://pytorch.apachecn.org/cn/docs/0.3.0/\" target=\"_blank\" rel=\"nofollow\">Pytorch中文文档</a></p>
<pre><code>    traindir = os.path.join(args.data, \'train\')#训练集
    valdir = os.path.join(args.data, \'val\')#测试集
     
    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                     std=[0.229, 0.224, 0.225])

    #直接调用pytorch用来导入数据的API，很方便
    train_loader = torch.utils.data.DataLoader(
        datasets.ImageFolder(traindir, transforms.Compose([
            transforms.RandomResizedCrop(224),#resize 为224*224
            transforms.RandomHorizontalFlip(),#数据增强 随机翻转
            transforms.ToTensor(),
            normalize,#正则化   
        ])),
        batch_size=args.batch_size, shuffle=True,
        num_workers=args.workers, pin_memory=True)

    val_loader = torch.utils.data.DataLoader(
        datasets.ImageFolder(valdir, transforms.Compose([
            #transforms.Resize(256),
            transforms.CenterCrop(224),#选取中间部分的224*224
            transforms.ToTensor(),
            normalize,
        ])),
        batch_size=args.batch_size, shuffle=False,
        num_workers=args.workers, pin_memory=True)
</code></pre>
<h3>测试</h3>
<p>我自己重新书写了test文件,其中测试用图放在./data/figure下</p>
<h5>测试命令：<code>CUDA_VISIBLE_DEVICES=3 python test.py</code>
</h5>
<h5>代码分析</h5>
<pre><code>def test():
    model = Net()
    #加载测试使用的训练好的网络模型    
    checkpoint=torch.load(\'./checkpoint.pth.tar\')
    
    model = torch.nn.DataParallel(model).cuda()
    model.load_state_dict(checkpoint[\'state_dict\']) 

    # define loss function (criterion) and optimizer
    criterion = nn.CrossEntropyLoss().cuda()
           
    # Data loading code
    testdir = os.path.join(\'./data\', \'figure\')
    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                     std=[0.229, 0.224, 0.225])

    test_loader = torch.utils.data.DataLoader(
        datasets.ImageFolder(testdir, transforms.Compose([
            transforms.Resize(224), #同样进行resize到224*224      
            transforms.ToTensor(),
            normalize,
        ])),
        )

    validate(test_loader, model, criterion)

</code></pre>
<p>其中validate函数具体如下：</p>
<pre><code>def validate(test_loader, model, criterion):
    #Computes and stores the average and current value    
    batch_time = AverageMeter()
    losses = AverageMeter()

    # switch to evaluate mode
    model.eval()

    for i, (input,target) in enumerate(test_loader):
        target = target.cuda(async=True)
        input_var = torch.autograd.Variable(input, volatile=True)
        target_var = torch.autograd.Variable(target, volatile=True)
        
        # compute output 
        output = model(input_var)
        output=torch.nn.functional.softmax(output)
     
        #将output转化为numpy类型，以便得到分类结果
        outputb=output.data.cpu()
        outputb=outputb.numpy()
        if outputb[0][0]/0.4&gt;=outputb[0][1]/0.6:
            print(\'Not Violate\')
        else:
           print(\'Violate!\')
</code></pre>
<h3>Finetune一部分层</h3>
<p>分为四种情况，解决方法基于的原则就是:</p>
<p>NN中的低层特征是比较generic的，比如说线、边缘的信息，高层特征是Dataset Specific的，基于此，如果你的数据集和ImageNet差异比较大，这个时候你应该尽可能的少用pre-trained model的高层特征.</p>
<h5>1.数据集小(比如&lt;5000)，相似度高</h5>
<p>这是最常见的情况，可以仅重新训练最后一层(fc layer）</p>
<h5>2.数据集大(比如&gt;10000)，相似度高</h5>
<p>fine-tuning后几层，保持前面几层不变或者干脆直接使用pre-trained model作为初始化，fine-tuning整个网络</p>
<h5>3.数据集小，相似度低</h5>
<p>小数据集没有办法进行多层或者整个网络的fine-tuning，建议保持前几层不动，fine-tuning后几层(效果可能也不会很好)</p>
<h5>4.数据集大，相似度低</h5>
<p>虽然相似度低，但是数据集大，可以和2一样处理</p>
<p>从上面我们可以看出，数据集大有优势，否则最好是数据集和原始的相似度比较高；如果出现数据集小同时相似度低的情况，这个时候去fine-tuning后几层未必会有比较好的效果.</p>
<h5>代码演示（只finetune最后一层fc层）</h5>
<pre><code>#将除最后一层的参数，其它层的参数 requires_grad 设置为 False
    for param in list(model.parameters())[:-2]:
        param.requires_grad = False
    # define loss function (criterion) and optimizer
    criterion = nn.CrossEntropyLoss().cuda()
    #只优化最后的分类层
    optimizer = torch.optim.SGD(model.fc.parameters(), lr=0.005,
                                momentum=0.9,
                                weight_decay=1e-4)
</code></pre>
<p>以上就是车辆违章行为检测的主要内容。</p>

          </div>','1531183715'),
('325572','{222}{2768}{2786}{1124255}{74}{131}{2159}{17}{975714}{36}{975728}{975727}{428114}{300}{72}{442}{975717}{983810}{975755}{975910}{976193}{975892}{975703}{1124259}{791}{975721}{576}{2175}{1700}{1124261}{975757}{2591}{4196}{2152}{2790}{4197}{2729}{30423}{1124264}{20978}{975786}{955}{975761}{976063}{975788}{3778}{692}{761}{1192}{975738}','value * shp[3].value # tf.reshape(tensor, shape, name=None) 将tensor变换为参数shape的形式。 resh1 = tf.reshape(pool5, [-1, flattened_shape], name=\"resh1\") 第一个全连接层，是一个隐藏节点数为4096的全连接层，后面接一个dropout层，训练时保留率为0.5，预测时为1.','卷积神经网络VGG 论文细读 + Tensorflow实现','<div class=\"show-content-free\">
            <h2>一. 背景介绍</h2>
<h4>VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION</h4>
<p>是牛津大学计算机视觉实验室参加2014年<strong>ILSVRC</strong>（ImageNet Large Scale Visual Recognition Challenge)比赛的网络结构。解决ImageNet中的1000类图像分类和localization。<br>
</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 284px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 36.46%;\"></div>
<div class=\"image-view\" data-width=\"779\" data-height=\"284\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/3352522-f5f362075eab9717.png\" data-original-width=\"779\" data-original-height=\"284\" data-original-format=\"image/png\" data-original-filesize=\"309810\"></div>
</div>
<div class=\"image-caption\">分类、定位、检测、分割</div>
</div>
<br>
实验结果是VGGNet斩获了2014年ILSVRC<strong>分类第二，定位第一</strong>。（当年分类第一是GoogleNet，后续会介绍）<p></p>
<h4><a href=\"http://www.robots.ox.ac.uk/~vgg/\" target=\"_blank\" rel=\"nofollow\">Oxford Visual Geometry Group</a></h4>
<h4><a href=\"http://www.robots.ox.ac.uk/\" target=\"_blank\" rel=\"nofollow\">Robotics Research Group</a></h4>
<h4><a href=\"https://arxiv.org/abs/1409.1556\" target=\"_blank\" rel=\"nofollow\">Paper link</a></h4>
<h2>二. Abstract</h2>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 573px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 44.84%;\"></div>
<div class=\"image-view\" data-width=\"1278\" data-height=\"573\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/3352522-2976676d221c8591.png\" data-original-width=\"1278\" data-original-height=\"573\" data-original-format=\"image/png\" data-original-filesize=\"272721\"></div>
</div>
<div class=\"image-caption\">Abstract</div>
</div>
<h5>1.VGGNet 探索的是神经网络的深度(depth)与其性能之间的关系。</h5>
<p>VGG通过反复堆叠3×3的小型卷积核和2×2的最大池化层，VGG成功构建了16-19层的卷积神经网络。是当时在论文发表前最深的深度网络。实际上，VGG在探索深度对神经网络影响的同时，其实本身广度也是很深的。那么：</p>
<h4>神经网络的深度和广度对其本身的影响是什么呢？</h4>
<ul>
<li>卷积核的种类对应了网络的广度，卷积层数对应了网络的深度。这两者对网络的拟合都有影响。但是在现代深度学习中，大家普遍认为深度比广度的影响更加高。</li>
<li>宽度即卷积核的种类个数，在LeNet那篇文章里我们说了，<strong><em>权值共享</em></strong>(每个神经元对应一块局部区域，如果局部区域是10*10，那么就有100的权重参数，但如果我们把每个神经元的权重参数设置为一样，相当于每个神经元用的是同一个卷积核去卷积图像，最终两层间的连接只有 100 个参数 ！)可以大大减少我们的训练参数，但是由于使用了同一个卷积核，最终特征个数太少，效果也不会好，所以一般神经网络都会有多个卷积核，这里说明宽度的增加在一开始对网络的性能提升是有效的。但是，随着广度的增加，对网络整体的性能其实是开始趋于饱和，并且有下降趋势，因为过多的特征（一个卷积核对应发现一种特征）可能对带来噪声的影响。</li>
<li>深度即卷积层的个数，对网络的性能是极其重要的，ResNet已经表明越深的深度网络性能也就越好。深度网络自然集成了低、中、高层特征。多层特征可以通过网络的堆叠的数量（深度）来丰富其表达。挑战imagenet数据集的优秀网络都是采用较深的模型。网络的深度很重要，但是否能够简单的通过增加更多的网络层次学习更好的网络？这个问题的障碍就是臭名昭著的梯度消失(爆炸)问题，这从根本上阻碍了深度模型的收敛。</li>
<li>增加更多的卷积核可以发现更多的特征，但是特征是需要进行组合的，只有知道了特征之间的关系才能够更好的表达图片内容，而增加深度就是组合特征的过程。</li>
</ul>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 601px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 47.89%;\"></div>
<div class=\"image-view\" data-width=\"1255\" data-height=\"601\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/3352522-67a7bae7f48ff4bc.png\" data-original-width=\"1255\" data-original-height=\"601\" data-original-format=\"image/png\" data-original-filesize=\"597459\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<h5>2. VGG结构全部都采用较小的卷积核（3×3，部分1×1）</h5>
<p>在VGG出现之前的深度网络，比如ZFNet或Overfeat普遍都采用了7×7和11×11的卷积核。VGG通篇全部采用很小的卷积核。我们再回顾一下在深度学习中卷积核的感受野的作用。</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 385px; max-height: 307px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 79.74%;\"></div>
<div class=\"image-view\" data-width=\"385\" data-height=\"307\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/3352522-f12720f0da1f878e.png\" data-original-width=\"385\" data-original-height=\"307\" data-original-format=\"image/png\" data-original-filesize=\"139345\"></div>
</div>
<div class=\"image-caption\">卷积核的感受野</div>
</div>
<h5>如何选择卷积核的大小？越大越好还是越小越好？</h5>
<p>答案是<strong>小而深</strong>，单独较小的卷积核也是不好的，只有堆叠很多小的卷积核，模型的性能才会提升。</p>
<ul>
<li>如上图所示，CNN的卷积核对应一个感受野，这使得每一个神经元不需要对全局图像做感受，每个神经元只感受局部的图像区域，然后在更高层，将这些感受不同局部的神经元综合起来就可以得到全局信息。这样做的一个好处就是可以减少大量训练的参数。</li>
<li>
<p>VGG经常出现多个完全一样的3×3的卷积核堆叠在一起的情况，这些多个小型卷积核堆叠的设计其实是非常有效的。如下图所示，两个3×3的卷积层串联相当于1个5×5的卷积层，即一个像素会和周围5×5的像素产生关联，可以说感受野是5×5。同时，3个串联的3×3卷积层串联的效果相当于一个7×7的卷积层。除此之外，3个串联的3×3的卷积层拥有比一个7×7更少的参数量，只有后者的 (3×3×3) / (7×7) = 55%。最重要的是3个3×3的卷积层拥有比一个7×7的卷积层更多的非线性变换（前者可以使用三次ReLu激活，而后者只有一次）。</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 438px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 53.74%;\"></div>
<div class=\"image-view\" data-width=\"815\" data-height=\"438\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/3352522-95871f31121e2641.png\" data-original-width=\"815\" data-original-height=\"438\" data-original-format=\"image/png\" data-original-filesize=\"203895\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
</li>
</ul>
<h5>3.VGG获得了2014年ILSVRC<strong>分类第二，定位第一</strong>。（当年分类第一是GoogleNet，后续会介绍）</h5>
<h2>三. Architecture</h2>
<p></p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 439px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 62.849999999999994%;\"></div>
<div class=\"image-view\" data-width=\"1241\" data-height=\"780\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/3352522-033d9629bd04007b.png\" data-original-width=\"1241\" data-original-height=\"780\" data-original-format=\"image/png\" data-original-filesize=\"468769\"></div>
</div>
<div class=\"image-caption\">architecture</div>
</div>
<br>
<strong>1. vgg模型的输入是固定的224×224的彩色RGB通道图像。</strong><br>
<strong>2. 输入做的唯一一个数据预处理就是各自减去 RGB 3个通道的均值</strong><br>
<strong>3. 使用的是非常小的3×3的卷积核。</strong><br>
<strong>4. 其中一个结构采用了一些1×1的卷积核。</strong><p></p>
<h4>1×1的卷积核到底有什么作用呢？</h4>
<ul>
<li>1×1的卷积核和正常的滤波器完全是一样的，只不过它不再感受一个局部区域，不考虑像素与像素之间的关系。1×1的卷积本身就是不同feature channel的线性叠加。1×1的卷积最早出现在<a href=\"https://arxiv.org/abs/1312.4400\" target=\"_blank\" rel=\"nofollow\">Network in Network</a>这篇文章中，在Google的inception结构中也采用了大量1×1的卷积。</li>
<li>NIN论文中解释1×1的卷积实现了多个feature map的结合，从而整合了不同通道间的信息。（个人认为这个作用并不是特点，因为其它大小的卷积核也可以实现）</li>
<li>
<p>1×1的卷积可以实现通道数量的升维和降维。并且是低成本的特征变换（计算量比3×3小很多）。是一个性价比很高的聚合操作。怎么理解1×1是性价比很高的升降通道数的操作呢？<br>
（以google inception为例）</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 426px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 49.13%;\"></div>
<div class=\"image-view\" data-width=\"867\" data-height=\"426\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/3352522-61e40de69fe1aef2.png\" data-original-width=\"867\" data-original-height=\"426\" data-original-format=\"image/png\" data-original-filesize=\"42821\"></div>
</div>
<div class=\"image-caption\">原始</div>
</div>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 442px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 52.370000000000005%;\"></div>
<div class=\"image-view\" data-width=\"844\" data-height=\"442\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/3352522-91fd68acfa15537a.png\" data-original-width=\"844\" data-original-height=\"442\" data-original-format=\"image/png\" data-original-filesize=\"46163\"></div>
</div>
<div class=\"image-caption\">新</div>
</div>
</li>
</ul>
<p>原始结构：<br>
参数：(1×1×192×64) + (3×3×192×128) + (5×5×192×32) = 153600<br>
最终输出的feature map：64+128+32+192 = 416</p>
<p>加入不同channel的1×1卷积后：<br>
参数：1×1×192×64+（1×1×192×96+3×3×96×128）+（1×1×192×16+5×5×16×32）=15872<br>
最终输出的feature map： 64+128+32+32=256</p>
<p>所以加入1×1的卷积后，在降低大量运算的前提下，降低了维度。</p>
<p><strong>5. 卷积步长是一个像素</strong><br>
<strong>6.采用最大池化层</strong><br>
<strong>7. 不是所有卷积层后面都接一个池化层。（和之前的网络有区别，是反复堆叠几个3×3的卷积）</strong><br>
<strong>8. 最大池化是2×2，步长为2.</strong><br>
<strong>9. 最后接了3个全连接层</strong><br>
<strong>10. 前两个全连接都是4096，最后一个根据imagenet1000类定为1000个输出</strong><br>
<strong>11. 分类层是softmax</strong><br>
<strong>12. 所有隐层都进行了ReLU激活。</strong><br>
<strong>13. 只有一个地方使用了LRN，并且实验表明LRN没有任何用处。</strong></p>
<h2>四. ConvNet Configurations</h2>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 700px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 100.0%;\"></div>
<div class=\"image-view\" data-width=\"830\" data-height=\"830\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/3352522-1a60b820389a1ae5.png\" data-original-width=\"830\" data-original-height=\"830\" data-original-format=\"image/png\" data-original-filesize=\"196082\"></div>
</div>
<div class=\"image-caption\">ConvNet Configurations</div>
</div>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 671px; max-height: 109px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 16.24%;\"></div>
<div class=\"image-view\" data-width=\"671\" data-height=\"109\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/3352522-7bd8f7b59799ec83.png\" data-original-width=\"671\" data-original-height=\"109\" data-original-format=\"image/png\" data-original-filesize=\"28144\"></div>
</div>
<div class=\"image-caption\">参数数量</div>
</div>
<ul>
<li>VGG全部使用了3×3的卷积核和2×2的池化核，通过不断加深网络结构来提升性能。上图为VGG各个级别的网络结构图。</li>
<li>VGG各种级别的结构都采用了5段卷积，每一段有一个或多个卷积层。同时每一段的尾部都接着一个最大池化层来缩小图片尺寸。每一段内的卷积核数量一致，越靠后的卷积核数量越多 64-128-256-512-512。经常出现多个完全一样的卷积层堆叠在一起的情况。</li>
<li>A-LRN结构使用了LRN，结果表明并没有什么用处。</li>
<li>C 结构比B多了几个1×1的卷积。在VGG里，1×1的卷积意义主要是线性变换，输入输出通道数量并没有变化。没有发生降维。所以作者认为1×1没有3×3好，大一些的卷积核可以学到更大的空间特征。</li>
<li>A-E 每一级网络逐渐变深，但是参数并没有变多很多。这是因为参数量主要消耗在最后3个全连接层，卷积虽然深但是参数消耗并不多。但是训练耗时的仍然是卷积，因其计算量大。</li>
<li>D E就是我们经常说的VGG-16和VGG-19</li>
</ul>
<h2>五. Training</h2>
<p></p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 546px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 78.09%;\"></div>
<div class=\"image-view\" data-width=\"1123\" data-height=\"877\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/3352522-395d80f0e8c02760.png\" data-original-width=\"1123\" data-original-height=\"877\" data-original-format=\"image/png\" data-original-filesize=\"467806\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
这个部分是VGG当时是怎么训练的具体过程，有很多值得借鉴的地方。<br>
<strong>1. 使用mini-batch的梯度下降法，并且是带动量的。batch_size设置为256，动量是0.9。</strong><br>
<strong>2. 前两个全连接使用了dropout，值是0.5, 用来缓解过拟合。</strong><br>
<strong>3.  学习率初始设置为0.01，衰减系数为10，每当验证集上准确率不再变好时，会降低学习率。学习率一共被衰减3次。总共训练了74个epoch，370k个iteration。</strong><p></p>
<h4>VGG的参数初始化方式是怎么样的？</h4>
<ul>
<li>上图中间红框部分作者介绍了VGG训练时参数的初始化方式，这个部分比较有意思。作者认为这么深的网络（论文发表前最深）训练收敛是很困难的，必须借助有效的参数初始化方式。</li>
<li>作者先训练上面网络结构中的A结构，A收敛之后呢，将A的网络权重保存下来，再复用A网络的权重来初始化后面几个简单模型。</li>
<li>复用A的网络权重，只是前四个卷积层，以及后三层全连接层，其它的都是随机初始化。</li>
<li>随机初始化，均值是0，方差是0.01。bias是0.</li>
</ul>
<h2>六. Image Size</h2>
<p>在训练和测试阶段，VGG都采用了Multi-scale的方式。</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 545px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 49.91%;\"></div>
<div class=\"image-view\" data-width=\"1092\" data-height=\"545\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/3352522-a2d8c129da423b5d.png\" data-original-width=\"1092\" data-original-height=\"545\" data-original-format=\"image/png\" data-original-filesize=\"298274\"></div>
</div>
<div class=\"image-caption\">training</div>
</div>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 329px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 30.209999999999997%;\"></div>
<div class=\"image-view\" data-width=\"1089\" data-height=\"329\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/3352522-d92b70e51121e4bc.png\" data-original-width=\"1089\" data-original-height=\"329\" data-original-format=\"image/png\" data-original-filesize=\"156361\"></div>
</div>
<div class=\"image-caption\">testing</div>
</div>
<h2>VGG的Multi-Scale方法</h2>
<ul>
<li>VGG在训练阶段使用了Multi-Scale的方法做数据增强，将原始图片缩放到不同的尺寸S，然后再随机裁剪224×224的图片，这样能增加很多数据量，对于防止模型过拟合有很不错的效果。</li>
<li>实验中，作者令S在[256, 512]这个区间，使用Multi-Scale获得了多个版本的数据，并将多个版本的数据合在一起训练。</li>
<li>在测试时，也采用了Multi-Scale的方法，将图像scale到一个尺寸Q，并将图片输入卷积网络计算，然后再最后一个卷积层使用滑窗的方式进行分类预测，将不同窗口的分类结果平均，再将不同尺寸Q的结果平均，得到最后的结果。这样可以提高数据的利用率和预测准确率。</li>
<li>
<p>下图是VGG各种不同scale的训练结果，融合了Multi-Scale的D和E是最好的。</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 382px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 37.05%;\"></div>
<div class=\"image-view\" data-width=\"1031\" data-height=\"382\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/3352522-a463b9f02eba418d.png\" data-original-width=\"1031\" data-original-height=\"382\" data-original-format=\"image/png\" data-original-filesize=\"120571\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
</li>
</ul>
<h2>七. Tensorflow实现简单的VGG-16的结构</h2>
<p>本部分整理自《Tensorflow实战》</p>
<h4>1. 实现卷积操作函数</h4>
<p>VGG包含很多卷积，函数conv_op创建卷积层，并把本层参数存入参数列表。这样可以方便后面VGG结构中多次使用卷积操作。</p>
<p>输入参数</p>
<ul>
<li>
<strong>input_op:</strong> 输入tensor，是一个4D的tensor，可以理解为image batch。shape=[batch, in_height, in_width, in_channels]，即：[训练时一个batch的图片数量, 图片高度, 图片宽度, 图像通道数]</li>
<li>
<strong>kh</strong>：卷积核的高</li>
<li>
<strong>kw：</strong>卷积核的宽</li>
<li>
<strong>n_out：</strong>输出通道数；这三个参数恰好定义了一个卷积核的参数，卷积核kernel的参数即为: shape=[filter_height, filter_width, in_channels, out_channels]，即：[卷积核的高度，卷积核的宽度，图像通道数，卷积核个数]</li>
<li>
<strong>dh</strong>：卷积的步长高</li>
<li>
<strong>dw</strong>：卷积的步长宽。 进行卷积操作时，我们除了需要输入的tensor和卷积核参数外，还需要定义卷积的步长strides，strides卷积时在每一维上的步长，strides[0]=strides[3]=1。</li>
<li>
<strong>p</strong>：参数列表。将卷积核和bias的参数写入p，以便后面使用</li>
</ul>
<p>输出：<br>
经过卷积，和激活函数的tensor。[batch_size, new_h, new_w, n_out]</p>
<pre><code>def conv_op(input_op, name, kh, kw, n_out, dh, dw, p):
    n_in = input_op.get_shape()[-1].value
    with tf.name_scope(name) as scope:
        kernel = tf.get_variable(scope+\'w\', shape=[kh, kw, n_in, n_out], dtype = tf.float32,
                                initializer = tf.contrib.layers.xavier_initializer_conv2d())
        conv = tf.nn.conv2d(input_op, kernel, (1, dh, dw, 1), padding = \'SAME\')
        bias_init_val = tf.constant(0.0, shape = [n_out], dtype = tf.float32)
        biases = tf.variable(bias_init_val, trainable = True, name = \'b\')
        z = tf.nn.bias_add(conv, biases)
        activation = tf.nn.relu(z, name = scope)
        p += [kernel, biases]
        return activation
</code></pre>
<ul>
<li>整体过程非常简单，建议熟背这一段代码。首先获得输入的tensor，即image batch，并且定义name，卷积核的大小，卷积的步长(输入参数)。</li>
<li>使用get_shape()获得输入tensor的输入通道数。</li>
<li>name_scope将scope内生成的variable自动命名为name/xxx，用于区分不同卷积层的组件</li>
<li>get_variable()定义卷积核的参数，注意shape和初始化方式</li>
<li>conv2d对输入tensor，使用刚刚的卷积核进行卷积操作，注意此处定义strides和padding</li>
<li>constant定义bias，注意shape等于输出通道数。一个卷积核对应一个输出通道，对应一个bias。tf.variable再将其转换成可训练的参数。</li>
<li>进行relu激活</li>
</ul>
<h4>2. 实现池化操作函数</h4>
<p>输入参数</p>
<ul>
<li>
<strong>input_op:</strong> 输入tensor</li>
<li>
<strong>kh</strong>：池化的高</li>
<li>
<strong>kw：</strong>池化的宽</li>
<li>
<strong>dh</strong>：池化的步长高</li>
<li>
<strong>dw</strong>：池化的步长宽。</li>
</ul>
<pre><code>def maxpool_op(input_op, name, kh, kw, dh, dw):
    return tf.nn.max_pool(input_op,
                          ksize=[1, kh, kw, 1],
                          strides=[1, dh, dw, 1],
                          padding=\'SAME\',
                          name=name)
</code></pre>
<p>这部分代码也很容易理解，nn.max_pool可以直接使用，需要定义池化的大小ksize，步长strides，以及边界填充方式padding。此部分没有需要训练的参数。</p>
<h4>3. 定义全连接操作函数</h4>
<p>输入参数</p>
<ul>
<li>
<strong>input_op:</strong> 输入的tensor</li>
<li>
<strong>n_out</strong>: 输出向量长度。 全连接只需要这两个参数</li>
</ul>
<pre><code>def fc_op(input_op, name, n_out, p):
    n_in = input_op.get_shape()[-1].value
    with tf.name_scope(name) as scope:
        kernel = tf.get_variable(scope+\"w\", shape=[n_in, n_out], dtype=tf.float32,
                                 initializer=tf.contrib.layers.xavier_initializer())
        biases = tf.Variable(tf.constant(0.1, shape=[n_out], dtype=tf.float32), name=\'b\')n
        activation = tf.nn.relu_layer(input_op, kernel, biases, name= scope)
        p += [kernel, biases]
        return activation

</code></pre>
<p>此部分代码也很简单，全连接层需要训练参数，并且比卷积层更多(卷积层是局部连接)，同样获得输入图片tensor的通道数(向量长度)，同样获得输入图片tensor的通道数，注意每个训练参数都需要给定初始化值或初始化方式。bias利用constant函数初始化为较小的值0.1,而不是0， 再做relu非线性变。</p>
<h4>4. 根据论文结构创建VGG16网络</h4>
<p>input_op是输入的图像tensor shape=[batch, in_height, in_width, in_channels]<br>
keep_prob是控制dropout比率的一个placeholder</p>
<pre><code>def inference_op(input_op,keep_prob):
    # 初始化参数p列表
    p = []
</code></pre>
<p>VGG16包含6个部分，前面5段卷积，最后一段全连接,<br>
每段卷积包含多个卷积层和pooling层.</p>
<p>下面是第一段卷积，包含2个卷积层和一个pooling层,<br>
利用前面定义好的函数conv_op,mpool_op 创建这些层</p>
<pre><code># 第一段卷积的第一个卷积层 卷积核3*3，共64个卷积核（输出通道数），步长1*1
# input_op：224*224*3 输出尺寸224*224*64
conv1_1 = conv_op(input_op, name=\"conv1_1\", kh=3, kw=3, n_out=64, dh=1,
                      dw=1, p=p)

# 第一段卷积的第2个卷积层 卷积核3*3，共64个卷积核（输出通道数），步长1*1
# input_op：224*224*64 输出尺寸224*224*64
conv1_2 = conv_op(conv1_1, name=\"conv1_2\", kh=3, kw=3, n_out=64, dh=1,
                      dw=1, p=p)

# 第一段卷积的pooling层，核2*2，步长2*2
# input_op：224*224*64 输出尺寸112*112*64
pool1 = mpool_op(conv1_2, name=\"pool1\", kh=2, kw=2, dh=2, dw=2)
</code></pre>
<p>下面是第2段卷积，包含2个卷积层和一个pooling层</p>
<pre><code># 第2段卷积的第一个卷积层 卷积核3*3，共128个卷积核（输出通道数），步长1*1
# input_op：112*112*64 输出尺寸112*112*128
conv2_1 = conv_op(pool1, name=\"conv2_1\", kh=3, kw=3, n_out=128, dh=1,
                      dw=1, p=p)

# input_op：112*112*128 输出尺寸112*112*128
conv2_2 = conv_op(conv2_1, name=\"conv2_2\", kh=3, kw=3, n_out=128, dh=1,
                      dw=1, p=p)

# input_op：112*112*128 输出尺寸56*56*128
pool2 = mpool_op(conv2_2, name=\"pool2\", kh=2, kw=2, dh=2, dw=2)
</code></pre>
<p>下面是第3段卷积，包含3个卷积层和一个pooling层</p>
<pre><code># 第3段卷积的第一个卷积层 卷积核3*3，共256个卷积核（输出通道数），步长1*1
# input_op：56*56*128 输出尺寸56*56*256
conv3_1 = conv_op(pool2, name=\"conv3_1\", kh=3, kw=3, n_out=256, dh=1,
                      dw=1, p=p)

# input_op：56*56*256 输出尺寸56*56*256
conv3_2 = conv_op(conv3_1, name=\"conv3_2\", kh=3, kw=3, n_out=256, dh=1,
                      dw=1, p=p)

# input_op：56*56*256 输出尺寸56*56*256
conv3_3 = conv_op(conv3_2, name=\"conv3_3\", kh=3, kw=3, n_out=256, dh=1,
                      dw=1, p=p)

# input_op：56*56*256 输出尺寸28*28*256
pool3 = mpool_op(conv3_3, name=\"pool3\", kh=2, kw=2, dh=2, dw=2)
</code></pre>
<p>下面是第4段卷积，包含3个卷积层和一个pooling层</p>
<pre><code># 第3段卷积的第一个卷积层 卷积核3*3，共512个卷积核（输出通道数），步长1*1
# input_op：28*28*256 输出尺寸28*28*512
conv4_1 = conv_op(pool3, name=\"conv4_1\", kh=3, kw=3, n_out=512, dh=1,
                      dw=1, p=p)

# input_op：28*28*512 输出尺寸28*28*512
conv4_2 = conv_op(conv4_1, name=\"conv4_2\", kh=3, kw=3, n_out=512, dh=1,
                      dw=1, p=p)

# input_op：28*28*512 输出尺寸28*28*512
conv4_3 = conv_op(conv4_2, name=\"conv4_3\", kh=3, kw=3, n_out=512, dh=1,
                      dw=1, p=p)

# input_op：28*28*512 输出尺寸14*14*512
pool4 = mpool_op(conv4_3, name=\"pool4\", kh=2, kw=2, dh=2, dw=2)
</code></pre>
<p>前面4段卷积发现，VGG16每段卷积都是把图像面积变为1/4，但是通道数翻倍, 因此图像tensor的总尺寸缩小一半。<br>
下面是第5段卷积，包含3个卷积层和一个pooling层</p>
<pre><code># 第5段卷积的第一个卷积层 卷积核3*3，共512个卷积核（输出通道数），步长1*1
# input_op：14*14*512 输出尺寸14*14*512
conv5_1 = conv_op(pool4, name=\"conv5_1\", kh=3, kw=3, n_out=512, dh=1,
                      dw=1, p=p)

# input_op：14*14*512 输出尺寸14*14*512
conv5_2 = conv_op(conv5_1, name=\"conv5_2\", kh=3, kw=3, n_out=512, dh=1,
                      dw=1, p=p)

# input_op：14*14*512 输出尺寸14*14*512
conv5_3 = conv_op(conv5_2, name=\"conv5_3\", kh=3, kw=3, n_out=512, dh=1,
                      dw=1, p=p)

# input_op：28*28*512 输出尺寸7*7*512
pool5 = mpool_op(conv5_3, name=\"pool5\", kh=2, kw=2, dh=2, dw=2)
</code></pre>
<p>下面要经过全连接，需将第五段卷积网络的结果扁平化, reshape将每张图片变为7<em>7</em>512=25088的一维向量</p>
<pre><code>shp = pool5.get_shape()
flattened_shape = shp[1].value * shp[2].value * shp[3].value
# tf.reshape(tensor, shape, name=None) 将tensor变换为参数shape的形式。
resh1 = tf.reshape(pool5, [-1, flattened_shape], name=\"resh1\")
</code></pre>
<p>第一个全连接层，是一个隐藏节点数为4096的全连接层，后面接一个dropout层，训练时保留率为0.5，预测时为1.0</p>
<pre><code>fc6 = fc_op(resh1, name=\"fc6\", n_out=4096, p=p)
fc6_drop = tf.nn.dropout(fc6, keep_prob, name=\"fc6_drop\")
</code></pre>
<p>第2个全连接层，是一个隐藏节点数为4096的全连接层，后面接一个dropout层，训练时保留率为0.5，预测时为1.0</p>
<pre><code>fc7 = fc_op(fc6_drop, name=\"fc7\", n_out=4096, p=p)
fc7_drop = tf.nn.dropout(fc7, keep_prob, name=\"fc7_drop\")
</code></pre>
<p>最后是一个1000个输出节点的全连接层，利用softmax输出分类概率，argmax输出概率最大的类别。</p>
<pre><code>fc8 = fc_op(fc7_drop, name=\"fc8\", n_out=1000, p=p)
softmax = tf.nn.softmax(fc8)
predictions = tf.argmax(softmax, 1)
return predictions, softmax, fc8, p
</code></pre>
<blockquote>
<p><strong>个人原创作品，转载需征求本人同意</strong></p>
</blockquote>

          </div>','1531183718'),
('325573','{222}{1124280}{2786}{3403}{6468}{1212}{17}{975714}{975892}{976103}{2159}{975727}{2790}{1124283}{131}{3213}{127369}{6500}{976193}{2175}{3312}{2360}{1352}{2591}{975951}{975904}{975788}{442}{975728}{975786}{1328}{975757}{23231}{74}{975980}{977660}{3289}{1124286}{406}{1452}{761}{2715}{975839}{9332}{1672}{1700}{975721}{975779}{763}{975748}','3）加到最终的分类结果中。 个人原创作品，转载需征求本人同意','Google Inception Net论文细读','<div class=\"show-content-free\">
            <h2>Going deeper with convolutions</h2>
<blockquote>
<p>GoogleNet首次出现在ILSVRC 2014比赛中（和VGG同年），获得了当时比赛的第一名。使用了Inception的结构，当时比赛的版本叫做Inception V1。inception结构现在已经更新了4个版本。Going deeper with convolutions这篇论文就是指的Inception V1版本。</p>
</blockquote>
<p><a href=\"https://arxiv.org/pdf/1409.4842.pdf\" target=\"_blank\" rel=\"nofollow\">论文地址</a></p>
<h2>一. Abstract</h2>
<p></p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 642px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 46.32%;\"></div>
<div class=\"image-view\" data-width=\"1386\" data-height=\"642\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/3352522-318f8e398c2bcac0.png\" data-original-width=\"1386\" data-original-height=\"642\" data-original-format=\"image/png\" data-original-filesize=\"276886\"></div>
</div>
<div class=\"image-caption\">abstract</div>
</div>
<br>
<strong>1. 该深度网络的代号为“inception”，在ImageNet大规模视觉识别挑战赛2014上，在分类和检测上都获得了好的结果。</strong><br>
<strong>2. 控制了计算量和参数量的同时，获得了很好的分类性能。500万的参数量只有AlexNet的1/12（6000万）。</strong><p></p>
<h2>Inception V1(或者说深度网络)为什么以降低参数量为目的？</h2>
<ul>
<li>参数越多，计算压力更大，需要的计算资源越多。</li>
<li>参数越多，模型越大，越容易过拟合。（鼓励简单模型）</li>
<li>参数越多，模型越大，就需要更多的数据来学习，但是高质量的训练数据很宝贵。</li>
</ul>
<h2>Inception V1如何降低参数量的？</h2>
<ul>
<li>用全局平均池化层代替了全连接（VGG中全连接层的参数占据了90%的参数量）</li>
<li>大量1×1的卷积核的使用</li>
</ul>
<p><strong>3. Inception Net整体结构受Hebbian原理的启发，并且充满了multi-scale的思想。</strong></p>
<h2>二. Motivation and High Level Considerations</h2>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 457px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 27.73%;\"></div>
<div class=\"image-view\" data-width=\"1648\" data-height=\"457\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/3352522-944398c86d1414c2.png\" data-original-width=\"1648\" data-original-height=\"457\" data-original-format=\"image/png\" data-original-filesize=\"270264\"></div>
</div>
<div class=\"image-caption\">Motivation and High Level Considerations</div>
</div>
<h2>Inception Net设计的思考是什么？(好的深度网络有哪些设计原则)</h2>
<ul>
<li><p>逐层构造网络：如果数据集的概率分布能够被一个神经网络所表达，那么构造这个网络的最佳方法是逐层构筑网络，即将上一层高度相关的节点连接在一起。几乎所有效果好的深度网络都具有这一点，不管AlexNet VGG堆叠多个卷积，googleNet堆叠多个inception模块，还是ResNet堆叠多个resblock。</p></li>
<li><p>稀疏的结构：人脑的神经元连接就是稀疏的，因此大型神经网络的合理连接方式也应该是稀疏的。稀疏的结构对于大型神经网络至关重要，可以减轻计算量并减少过拟合。 卷积操作（局部连接，权值共享）本身就是一种稀疏的结构，相比于全连接网络结构是很稀疏的。</p></li>
<li><p>符合Hebbian原理: Cells that fire together, wire together. 一起发射的神经元会连在一起。 相关性高的节点应该被连接而在一起。</p></li>
</ul>
<p>inception中 1×1的卷积恰好可以融合三者。我们一层可能会有多个卷积核，在同一个位置但在不同通道的卷积核输出结果相关性极高。一个1×1的卷积核可以很自然的把这些相关性很高，在同一个空间位置，但不同通道的特征结合起来。而其它尺寸的卷积核（3×3，5×5）可以保证特征的多样性，因此也可以适量使用。于是，这就完成了inception module下图的设计初衷：4个分支：</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 501px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 31.209999999999997%;\"></div>
<div class=\"image-view\" data-width=\"1605\" data-height=\"501\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/3352522-9b8de6178b60f24a.png\" data-original-width=\"1605\" data-original-height=\"501\" data-original-format=\"image/png\" data-original-filesize=\"105100\"></div>
</div>
<div class=\"image-caption\">inception module</div>
</div>
<h2>New Version比Old version是如何减少参数量的？</h2>
<ul>
<li>1×1的卷积核和正常的滤波器完全是一样的，只不过它不再感受一个局部区域，不考虑像素与像素之间的关系。1×1的卷积本身就是不同feature channel的线性叠加。1×1的卷积最早出现在<a href=\"https://arxiv.org/abs/1312.4400\" target=\"_blank\" rel=\"nofollow\">Network in Network</a>这篇文章中，在Google的inception结构中也采用了大量1×1的卷积。</li>
<li>NIN论文中解释1×1的卷积实现了多个feature map的结合，从而整合了不同通道间的信息。（个人认为这个作用并不是特点，因为其它大小的卷积核也可以实现）</li>
<li>
<p>1×1的卷积可以实现通道数量的升维和降维。并且是低成本的特征变换（计算量比3×3小很多）。是一个性价比很高的聚合操作。怎么理解1×1是性价比很高的升降通道数的操作呢？<br>
（以google inception为例）</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 426px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 49.13%;\"></div>
<div class=\"image-view\" data-width=\"867\" data-height=\"426\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/3352522-61e40de69fe1aef2.png\" data-original-width=\"867\" data-original-height=\"426\" data-original-format=\"image/png\" data-original-filesize=\"42821\"></div>
</div>
<div class=\"image-caption\">原始</div>
</div>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 442px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 52.370000000000005%;\"></div>
<div class=\"image-view\" data-width=\"844\" data-height=\"442\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/3352522-91fd68acfa15537a.png\" data-original-width=\"844\" data-original-height=\"442\" data-original-format=\"image/png\" data-original-filesize=\"46163\"></div>
</div>
<div class=\"image-caption\">新</div>
</div>
</li>
</ul>
<p>原始结构：<br>
参数：(1×1×192×64) + (3×3×192×128) + (5×5×192×32) = 153600<br>
最终输出的feature map：64+128+32+192 = 416</p>
<p>加入不同channel的1×1卷积后：<br>
参数：1×1×192×64+（1×1×192×96+3×3×96×128）+（1×1×192×16+5×5×16×32）=15872<br>
最终输出的feature map： 64+128+32+32=256</p>
<p>所以加入1×1的卷积后，在降低大量运算的前提下，降低了维度。<br>
降低维度也是inception module一个非常明智的举措。</p>
<h2>三. GoogleNet</h2>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 216px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 13.19%;\"></div>
<div class=\"image-view\" data-width=\"1638\" data-height=\"216\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/3352522-7cdd10a4799663af.png\" data-original-width=\"1638\" data-original-height=\"216\" data-original-format=\"image/png\" data-original-filesize=\"88698\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<ul>
<li>在inception module中，通常1×1的卷积比例（输出通道占比）最高，3×3和5×5的卷积稍低。</li>
<li>在整个网络中，会有多个堆叠的inception module，希望靠后的inception module可以捕捉更高阶的抽象特征，因此靠后的inception module中，大的卷积应该占比变多。<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 396px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 56.71000000000001%;\"></div>
<div class=\"image-view\" data-width=\"1527\" data-height=\"866\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/3352522-d5397247923fe73f.png\" data-original-width=\"1527\" data-original-height=\"866\" data-original-format=\"image/png\" data-original-filesize=\"294174\"></div>
</div>
<div class=\"image-caption\">google net</div>
</div>
<br>
GoogleNet有22层深，比同年的VGG19还深。包含了9个inception module，下面是具体的结构。我试图将表格和结构图结合起来，解释整个的网络结构。<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 2919px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 417.06000000000006%;\"></div>
<div class=\"image-view\" data-width=\"3213\" data-height=\"13400\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/3352522-d8bb3762fa8ac827.png\" data-original-width=\"3213\" data-original-height=\"13400\" data-original-format=\"image/png\" data-original-filesize=\"6992795\"></div>
</div>
<div class=\"image-caption\">googlenet</div>
</div>
<br>
<strong>DepthConcat：</strong>聚合操作，在输出通道这个维度上聚合（一个inception module每个分支通道数可能不一样，但是feature map大小应该是一样的。strides=1，padding=same）</li>
</ul>
<h4>inception 3a</h4>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 284px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 39.660000000000004%;\"></div>
<div class=\"image-view\" data-width=\"716\" data-height=\"284\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/3352522-e789b5125fc58e09.png\" data-original-width=\"716\" data-original-height=\"284\" data-original-format=\"image/png\" data-original-filesize=\"78805\"></div>
</div>
<div class=\"image-caption\">inception 3a</div>
</div>
<br>
<p>输入：28×28×192<br>
输出：由于每个分支strides=1，padding=same，所以只是通道数在变化，feature map大小不变。最终输出 28×28×256（只增加了少量通道数）<br>
其它的inception module也是这种形式，可自己推算。</p>

<h4>辅助分类器</h4>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 216px; max-height: 557px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 257.87%;\"></div>
<div class=\"image-view\" data-width=\"216\" data-height=\"557\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/3352522-05d34de3f41ce95e.png\" data-original-width=\"216\" data-original-height=\"557\" data-original-format=\"image/png\" data-original-filesize=\"39135\"></div>
</div>
<div class=\"image-caption\">classifier 1</div>
</div>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 207px; max-height: 533px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 257.49%;\"></div>
<div class=\"image-view\" data-width=\"207\" data-height=\"533\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/3352522-59b590bb4d0715b1.png\" data-original-width=\"207\" data-original-height=\"533\" data-original-format=\"image/png\" data-original-filesize=\"37544\"></div>
</div>
<div class=\"image-caption\">classifier 2</div>
</div>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 243px; max-height: 429px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 176.54000000000002%;\"></div>
<div class=\"image-view\" data-width=\"243\" data-height=\"429\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/3352522-8396b836d963044b.png\" data-original-width=\"243\" data-original-height=\"429\" data-original-format=\"image/png\" data-original-filesize=\"34789\"></div>
</div>
<div class=\"image-caption\">classifier 3</div>
</div>
<br>
<p>Google net除了最后一层输出进行分类外，其中间节点的分类效果也很好。于是，Googlenet也会将中间的某一层的输出用于分类，并按一个较小的权重（0.3）加到最终的分类结果中。</p>

<blockquote>
<p><strong>个人原创作品，转载需征求本人同意</strong></p>
</blockquote>

          </div>','1531183719'),
('325574','{1876}{72}{4681}{975714}{17}{290765}{62}{975728}{975915}{36}{24}{1124296}{975796}{1901}{4011}{3918}{1945}{976193}{975761}{975779}{36549}{975776}{687703}{1065820}{1635}{975744}{1785}{976136}{1605}{295}{975951}{3734}{4641}{1124297}{3047}{975903}{975741}{558}{1121}{296}{975835}{417}{1350}{1212}{397954}{975813}{976042}{975717}{1684}{1118578}','θ_h)$。 网络的外部输入是glimpse特征向量$g_t$。 操作：在每一步中，代理执行两个操作：它决定如何通过传感器控件$l_t$部署其传感器，以及可能影响环境状态的环境操作。环境行动的性质取决于任务。在这项工作中，位置动作是从位置网络$f_l（h_t;62％。 然而，具有4次瞥见的RAM性能比卷积网络略好，并且在6次和8次瞥见中进一步胜过它，达到1.','《视觉注意循环神经网络》论文阅读','<div class=\"show-content-free\">
            <p><a href=\"https://blog.csdn.net/u011239443/article/details/80474983\" target=\"_blank\" rel=\"nofollow\">https://blog.csdn.net/u011239443/article/details/80474983</a><br>
论文地址：<a href=\"http://pdfs.semanticscholar.org/09a5/03095db2d68b439e48d67481399198ed0e5b.pdf\" target=\"_blank\" rel=\"nofollow\">http://pdfs.semanticscholar.org/09a5/03095db2d68b439e48d67481399198ed0e5b.pdf</a></p>
<h1>摘要</h1>
<p>将卷积神经网络应用于大型图像的计算量很大，因为计算量与图像像素数成线性关系。我们提出了一种新颖的循环神经网络模型，可以从图像或视频中提取信息，方法是自适应地选择一系列区域或位置，并仅以高分辨率处理选定区域。与卷积神经网络一样，所提出的模型具有内置的平移不变性程度，但其执行的计算量可以独立于输入图像大小进行控制。虽然模型是不可区分的，但可以使用强化学习方法来学习，以学习特定于任务的策略。我们在几个图像分类任务上评估我们的模型，其中它显着优于混乱图像上的卷积神经网络基线以及动态视觉控制问题，其中它学习跟踪简单对象而没有明确的训练信号。</p>
<h1>1 介绍</h1>
<p>基于神经网络的架构最近在极具挑战性的图像分类和物体检测数据集方面取得了巨大的成功。但是，他们出色的识别准确性在训练和测试时间都会带来高昂的计算成本。尽管输入图像被降采样以减少计算，但目前通常使用的大型卷积神经网络需要数天的时间才能在多个GPU上训练。在对象检测处理的情况下，当在单个GPU上运行时，测试时间的单个图像需要几秒钟，因为这些方法有效地遵循来自计算机视觉文献的经典滑动窗口范例，其中分类器被训练用于检测紧密裁剪中的对象边界框在不同位置和尺度上独立应用于数千个测试图像的候选窗口。尽管可以共享一些计算，但这些模型的主要计算费用来自于整个输入图像的卷积滤波映射，因此它们的计算复杂度至少在像素数量上是线性的。</p>
<p>人类感知的一个重要特性是不倾向于一次处理整个场景。 相反，人类有选择地将注意力集中在视觉空间的某些部分上，以获取需要的信息，并随时间将不同视角的信息相结合，以建立场景的内部表示，指导未来的眼球运动和决策制定。 由于需要处理更少的“像素”，因此将场景中的部分计算资源集中在一起可节省“带宽”。 但它也大大降低了任务的复杂性，因为感兴趣的对象可以放置在固定的中心，固定区域之外的视觉环境（“杂乱”）的不相关特征自然被忽略。</p>
<p>根据其基本作用，在神经科学和认知科学文献中广泛研究了人眼运动的指导。 虽然低层次场景的属性和自下而上的过程扮演着重要的角色，但人们注视的位置也显示出强烈的特定任务。在本文中，我们从这些结果中获得灵感，并开发了一个新颖的基于注意力的任务驱动的神经网络视觉处理‘框架。 我们的模型将视觉场景的基于注意力的处理视为控制问题，并且通用性足以应用于静态图像，视频或作为与动态视觉环境交互的代理的感知模块。</p>
<p>该模型是一个循环神经网络（RNN），它按顺序处理输入，一次一个地处理图像（或视频帧）内的不同位置，并递增地组合来自这些注视的信息以建立场景的动态内部表示，或环境。基于过去的信息和任务的需求，模型不是一次处理整个图像甚至是边界框，而是在每一步中选择下一个要注意的位置。我们的模型中的参数数量和它执行的计算量可以独立于输入图像的大小来控制，而卷积网络的计算需与图像像素的数量线性地成比例。我们描述了一个端到端的优化程序，该程序允许模型直接针对给定的任务进行训练，并最大限度地提高可能取决于模型做出的整个决策序列的性能测量。该过程使用反向传播来训练神经网络组件和策略梯度以解决由于控制问题导致的非差异性。</p>
<p>我们表明，我们的模型可以有效的学习特定于任务的策略，如多图像分类任务以及动态视觉控制问题。 我们的结果还表明，基于关注的模型可能比卷积神经网络更好地处理杂波和大输入图像。</p>
<h1>2 前期工作</h1>
<p>计算机视觉文献中计算限制受到了很多关注。 例如，对于对象检测，已经做了很多工作来降低广泛的滑动窗口范例的成本，主要着眼于减少评估完整分类器的窗口的数量，例如， 通过分类器级联，通过分类器输出上的分支和边界方法从考虑中去除图像区域，或者通过提出可能包含对象的候选窗口。 尽管使用这些方法可以获得实质性的加速，并且其中一些可以与CNN分类器相结合或用作CNN分类器的附加，但它们仍然牢牢扎根于用于对象检测的窗口分类器设计，并且只利用过去的信息来通知未来 以非常有限的方式处理图像。</p>
<p>显着性检测器是计算机视觉领域具有悠久历史，受人类感知强烈驱动的第二类方法。 这些方法优先处理潜在感兴趣（“显着”）图像区域的处理，该图像区域通常基于局部低级别特征对比度的某种度量来识别。 显着性检测器确实捕捉人眼运动的一些属性，但它们通常不会将信息整合到注视点上，它们的显着性计算大多是硬连线的，并且它们仅基于低级图像属性，通常忽略其他因素，例如 场景和任务需求的语义内容。</p>
<p>计算机视觉文献和其他文献中的一些作品已经将视觉作为一个顺序决策任务，就像我们在这里所做的那样。 在那里，和我们的工作一样，关于图像的信息是按顺序收集的，下一次参加的决定取决于以前的图像。</p>
<p>我们的工作也许与其他在深度学习框架中实施注意力处理的尝试最相似。 然而，我们的公式使用RNN来整合视觉信息，并决定如何行动，但是，我们的学习过程允许对顺序决策过程进行端到端的优化，而不是依赖贪婪的行为选择。 我们进一步展示了如何使用相同的通用体系结构在静止图像中进行高效的对象识别，并以任务驱动的方式与动态可视化环境进行交互。</p>
<h1>3 循环注意力模型</h1>
<p>在本文中，我们将关注问题视为目标导向代理与视觉环境交互的顺序决策过程。 在每个时间点，代理只通过一个带宽限制的传感器观察环境，即它不会完全感知环境。 它可能仅在本地区域或窄频带中提取信息。 但是，代理可以主动控制如何部署其传感器资源（例如，选择传感器位置）。 代理还可以通过执行操作来影响环境的真实状态。 由于只能部分观察环境，因此代理需要随时间整合信息，以确定如何采取行动以及如何最有效地部署传感器。 在每一步中，代理人都会收到一个标量奖励（这取决于代理人已执行并可能推迟的行动），代理人的目标是最大化这些奖励的总和。</p>
<p>该公式包含了各种任务，如静态图像中的对象检测和控制问题，如从屏幕上可见的图像流中播放电脑游戏。 对于游戏，环境状态将是游戏引擎的真实状态，并且代理的传感器将在屏幕上显示的视频帧上运行。 （请注意，对于大多数游戏，单个框架不会完全指定游戏状态）。 这里的环境行动将对应于操纵杆控制，并且奖励将反映得分。 对于静态图像中的对象检测，环境状态将是固定的，并与图像的真实内容相对应。 环境行动将与分类决定相对应（只有在固定数量的注视之后才能执行），并且奖励将反映决策是否正确。</p>
<h2>3.1 模型</h2>
<p>如图1所示，Agent是围绕循环神经网络构建的。</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 654px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 43.37%;\"></div>
<div class=\"image-view\" data-width=\"1508\" data-height=\"654\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1621805-20a16a00e7443932.png\" data-original-width=\"1508\" data-original-height=\"654\" data-original-format=\"image/png\" data-original-filesize=\"149285\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<blockquote>
<p>图1：A）<strong>Glimpse传感器</strong>：给定Glimpse和输入图像的坐标，传感器提取以$l_{t-1}$为中心的包含多个分辨率块的视网膜样表示$ρ（x_t，l_{t-1}）$。<br>
B）<strong>Glimpse网络</strong>：给定位置（$l_{t-1}$）和输入图像（$x_t$），使用Glimpse传感器来提取视网膜表示$ρ（x_t，l_{t-1}）$。 然后，视网膜表示和Glimpse位置被映射到使用分别由$θ<em>{g}^{0}$和$θ</em>{g}<sup>{1}$参数化的独立线性层的隐藏空间，使用整流单元，接着是另一个线性层$θ_g</sup>2$以组合来自两个分量的信息。 Glimpse网络$f_g(.;{θ_g<sup>0,θ_g</sup>1,θ_g^2})$定义了用于产生Glimpse表示$g_t$的注意力网络的可训练带宽限制传感器。<br>
C）<strong>模型架构</strong>：总体而言，该模型是一个RNN。 模型$f_h (.; θ<em>h )$的核心网络将Glimpse表示$g_t$作为输入，并与上一时间步$h</em>{t-1}$的内部表示相结合，产生模型$h_t$的新内部状态。 位置网络$f_l（;θ_l）$和动作网络$f_a（;θ_a）$分别使用模型的内部状态$h_t$来产生下一个要注意的位置和动作/分类。 这个基本的RNN迭代有可变数量的步骤重复。</p>
</blockquote>
<p>在每个时间步骤中，它处理传感器数据，随时间集成信息，并选择如何操作以及如何在下一步骤部署传感器：</p>
<p><strong>传感器</strong>：在每个步骤t，代理以图像形式接收（部分）环境观察。 该代理不能完全访问该图像，而是可以通过其带宽受限传感器ρ从$x_t$中提取信息，例如， 通过将传感器聚焦在感兴趣的某个区域或频带上。</p>
<p>在本文中，我们假设带宽限制传感器从图像$x_t$中提取位置$l_{t-1}$周围的视网膜样表示$ρ（x_t，l_{t-1}）$。 它以高分辨率对$l$周围的区域进行编码，但是对于距离$l$更远的像素使用逐渐降低的分辨率，导致维度比原始图像$x$低得多的向量。 我们将把这个低分辨率表示看作是glimpse。 glimpse传感器用于我们称之为glimpse网络$f_g$的内部，以产生glimpse特征向量$gt = f_g（x_t，l_{t-1};θ_g）$，其中$θg= {θ_g<sup>0，θ_g</sup>1，θ_g^2}$（图1B）。</p>
<p><strong>内部状态</strong>：代理维护一个内部状态，汇总从过去观察历史中提取的信息; 它对代理人的环境知识进行编码，并有助于决定如何采取行动以及在何处部署传感器。 这个内部状态由循环神经网络的隐含单元$h_t$组成，并且随时间由核心网络更新：$h_t = f_h(h_{t1}, g_t; θ_h)$。 网络的外部输入是glimpse特征向量$g_t$。</p>
<p><strong>操作</strong>：在每一步中，代理执行两个操作：它决定如何通过传感器控件$l_t$部署其传感器，以及可能影响环境状态的环境操作。环境行动的性质取决于任务。在这项工作中，位置动作是从位置网络$f_l（h_t;θ_l）$在时间t参数化的分布随机选择的：$l_tp（·| f_l（h_t;θ_l））$。在$p（·| f_a（h_t;θ_a））$处的第二个网络输出的条件下，环境行为同样得到。为了进行分类，它使用softmax输出进行制定，对于动态环境，其确切表述取决于为特定环境定义的动作集（例如，操纵杆运动，电机控制等）。最后，我们的模型还可以增加一个额外的动作，决定何时停止glimpse。例如，这可以用来学习一个成本敏感的分类器，通过给代理人每次glimpse给予一个负面的回报，迫使它代之以做出正确的分类和更多的glimpse的代价。</p>
<p><strong>奖励</strong>：在执行一个行动之后，行动者接收一个对环境$x_{t + 1}$和奖励信号$r_{t + 1}$的新视觉观察。 代理的目标是最大化通常非常稀疏和延迟的奖励信号的总和：$R = \\sum ^T_{t= 1} r_t$。 在对象识别的情况下，例如，如果在T步骤之后对象被正确分类，则$r_T = 1$，否则为0。</p>
<p>上述设置是RL社区中已知的作为部分可观察马尔可夫决策过程（POMDP）的特例。 环境的真实状态（可以是静态的或动态的）是不可见的。 在这个视图中，代理需要学习一个（随机）策略$π（（l_t，a_t）| s_{1:t};θ）$，参数θ表示在每个步骤t映射过去与环境交互的历史$s_{1:t} = x_1，l_1，a_1，... x_{t-1}，l_{t-1}，a_{t-1}$，$x_t$分配给当前时间步的动作分配，但不受传感器的限制。 在我们的例子中，策略π由上面概括的RNN定义，并且历史$s_t$在隐含单位$h_t$中被汇总。 我们将在第4节中描述上述组件的具体选择。</p>
<h2>3.2 训练</h2>
<p>我们的代理参数由glimpse网络，核心网络（图1C）和操作网络$θ=  {θ_g，θ_h，θ_a}$ 给予，并且我们学习这些以最大化当代理人与环境交互时可以期望的总奖励。</p>
<p>更正式地说，代理人的政策可能结合环境的动态（例如玩游戏），引发对可能的交互序列$s_{1:N}$的分配，并且我们的目标是在这种分配下最大化奖励:$J(θ) = E_{p(s_1:T ;θ)}[\\sum ^T_{t= 1} r_t]= E_{p(s_1:T ;θ)}[R]$，其中$p(s_1:T ;θ)$取决于策略。</p>
<p>正确地最大化$J$不是简单的事，因为它涉及对高维相互作用序列的期望，这可能反过来涉及未知的环境动态。 然而，将问题看作是POMDP，可以让我们从RL文献中找到技术来承担：正如Williams所示，梯度的样本近似值由：</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 128px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 9.17%;\"></div>
<div class=\"image-view\" data-width=\"1396\" data-height=\"128\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1621805-c90f7b356db23d57.png\" data-original-width=\"1396\" data-original-height=\"128\" data-original-format=\"image/png\" data-original-filesize=\"27977\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
<p>其中$s_i$是通过针对$i = 1...M$事件运行当前代理$π_θ$获得的交互序列。</p>

<blockquote>
<p>根据情景，考虑折扣奖励总和可能更为合适，在远期未来获得的奖励贡献较少：$R = \\sum ^T_{t= 1} γ^{t1}r_t$。这种情况，我们有$T → ∞$</p>
</blockquote>
<p>学习规则（1）也被称为REINFORCE规则，它涉及以当前策略运行代理以获取交互序列$s_{1:T}$的样本，然后调整智能体的参数θ，使得导致高累积奖励的选择行为增加，而产生低回报的行动减少。</p>
<p>等式（1）需要我们计算：$<em>θ logπ(u<sup>i_t|s</sup>i</em>{1:t};θ)$。但是，这只是RNN的梯度，它定义了我们的代理在时间步骤$t$评估并可以通过标准反向传播计算。</p>
<p>方差减少：方程（1）为我们提供了无偏估计的梯度，但它可能具有高方差。 因此通常考虑梯度估计：<br>
</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 148px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 13.43%;\"></div>
<div class=\"image-view\" data-width=\"1102\" data-height=\"148\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1621805-bf4e881580aaf698.png\" data-original-width=\"1102\" data-original-height=\"148\" data-original-format=\"image/png\" data-original-filesize=\"23199\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
其中，$R<sup>i_t$是在执行行动$u_t</sup>i$之后获得的累积奖励，$b_t$是基线，可以基于$s^i_{1:t}$ (如通过$h^i_t$) ,而不基于行动$u^i_t$本身。<p></p>
<p>这个估计值等于（1）的预期值，但可能有较低的方差。 选择$b_t =E_π[R_t] $是很自然的，这种形式的基线被称为强化学习文献中的价值函数。 由此产生的算法增加了一个行动的对数概率，其后跟随着一个大于预期的累积奖励，并且如果获得的累积奖励较小则降低概率。 我们使用这种类型的基线并通过减少$R_t^i$和$b_t$之间的平方误差来学习它。</p>
<p>$使用混合监督损失$：上述算法允许我们在“最佳”行为未知时训练代理，并且学习信号仅通过奖励提供。 举例来说，我们可能并不知道哪一系列的注视提供了关于未知图像的大部分信息，但是在一集结束时的总奖励将给我们指示尝试的序列是好还是坏。</p>
<p>但是，在某些情况下，我们确实知道要采取的正确操作：例如，在对象检测任务中，代理必须输出对象的标签作为最终操作。 对于训练图像，该标签将是已知的，并且我们可以直接优化策略以在观察序列结束时输出与训练图像相关的正确标签。这可以像监督学习中常见的那样通过给定来自图像的观察结果最大化真实标签的条件概率来实现，即通过最大化$logπ(a<sup>_T|s_{1:T};θ)$，其中$a</sup>_T$对应于与观察$s_1:T$所获得的图像相关的实况标签。 我们遵循这种分类问题的方法，我们优化交叉熵损失以训练操作网络$f_a$并通过核心和glimpse网络反向传播梯度。 位置网络$f_l$总是通过REINFORCE进行训练。</p>
<h1>4 实验</h1>
<p>我们在几个图像分类任务以及一个简单的游戏中评估了我们的方法。 我们首先描述所有实验中共同的设计选择：</p>
<p><strong>视网膜和位置编码</strong>：视网膜编码$ρ（x，l）$提取以位置$l$为中心的$k$个正方形贴片，第一个贴片尺寸为$g_w×g_w$像素，并且每个连续贴片具有前一个宽度的两倍。 然后将$k$个补丁全部调整为$g_w×g_w$并连接。Glimpse位置$l$被编码为实值$（x，y）$坐标，其中$（0,0）$是图像$x$的中心，$（-1，-1）$是$x$的左上角。</p>
<p><strong>Glimpse网络</strong>: Glimpse网络$f_g (x, l)$有两个全连接层。$Linear(x)$表示向量x的线性变换，$Linear(x) = W x+b $，$W$为权重矩阵，$b$为偏置。$Rect(x) = max(x, 0)$为整流器的非线性。glimpse网络的输出g被定义为$g=Rect(Linear(h_g)+Linear(h_l))$，其中$h_g =Rect(Linear(ρ(x,l))) $，$h_l = Rect(Linear(l))$。$h_g$和$h_l$的维度为128，而本文中训练的所有注意模型的维度为256。</p>
<p><strong>位置网络</strong>：位置$l$的策略由具有固定方差的双分量高斯定义。 位置网络在时间$t$输出位置策略的均值并且被定义为$ f_l(h) = Linear(h)$，其中$h$是核心网络/ RNN的状态。</p>
<p><strong>核心网络</strong>：对于核心$f_h$后面的分类实验，整流器单元网络定义为$h_t = f_h(h_{t1}) = Rect(Linear(h_{t1}) + Linear(g_t))$。在动态环境下进行的实验使用了LSTM单元的核心</p>
<h2>4.1 图像分类</h2>
<p>在以下分类实验中使用的注意力网络仅在最后一个时间步$t = N$做出分类决定。 动作网络$f_a$只是一个线性的softmax分类器，定义为$f_a（h）= exp（Linear（h））/ Z$，其中$Z$是一个归一化常数。 RNN状态向量$h$具有维度256。所有方法均使用随机梯度下降进行训练，其中尺寸为20，动量为0.9。 在训练过程中，我们将学习率从初始值线性地退化到0。 使用随机搜索来选择超参数，例如初始学习率和位置策略的方差。 如果代理商分类正确，则最后一步的奖励为1，否则为0。 所有其他时间步的奖励为0。</p>
<p><strong>居中数字</strong>：我们首先测试了我们的训练方法通过使用它训练MNIST数字数据集上最多7次瞥见RAM模型的成功glimpse策略的能力。 这个实验的“视网膜”只是一个8×8的贴片，它只能容纳一个数字的一部分，因此该实验还测试了RAM结合多次glimpse信息的能力。 我们还训练了具有两个隐藏层的标准前馈和卷积神经网络作为基线。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 446px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 30.259999999999998%;\"></div>
<div class=\"image-view\" data-width=\"1474\" data-height=\"446\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1621805-492a31a2c3719a52.png\" data-original-width=\"1474\" data-original-height=\"446\" data-original-format=\"image/png\" data-original-filesize=\"198950\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<blockquote>
<p>表1：在MNIST和翻译的MNIST数据集上的分类结果。 FC表示具有两层整流器单元的全连接网络。 卷积网络具有一个8步10×10个步长为5的滤波器，其后是每层之后的具有256个单元整流器的完全连接层。 注意力模型的实例标有glimpse的数量，视网膜中的片数量和视网膜的大小。</p>
</blockquote>
<p>表1a列出了测试装置上不同模型的误差率。 我们发现RAM的性能通常会随着更多的glimpse而改善，并且它最终将胜过在全28×28居中数字上训练的基准模型。 这表明该模型可以成功地学习结合来自多个glimpse的信息。</p>
<p><strong>非居中数字</strong>：我们考虑的第二个问题是对非居中数字进行分类。 我们创建了一个名为Translated MNIST的新任务，通过将MNIST数字放置在较大空白补丁的随机位置生成数据。 有效训练集大小为50000（MNIST训练集的大小）乘以可能的位置数。图2a包含60乘60转换MNIST任务的测试用例的随机样本。表1b显示了用60乘60图片在Translated MNIST任务上训练的几个不同模型的结果。<br>
</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 248px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 16.25%;\"></div>
<div class=\"image-view\" data-width=\"1526\" data-height=\"248\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1621805-49dbf5c1d9277fbd.png\" data-original-width=\"1526\" data-original-height=\"248\" data-original-format=\"image/png\" data-original-filesize=\"45963\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p></p>
<blockquote>
<p>图2：转换和杂乱转换MNIST任务的测试用例示例。</p>
</blockquote>
<p>除了RAM和两个完全连接的网络之外，我们还训练了一个网络，其中一个卷积层由16个10×10的滤波器组成，步长为5，然后是整流器非线性，然后是256个整流器单元的完全连接层。 卷积网络，RAM网络和较小的全连接模型都具有大致相同的参数数量。 由于卷积网络内置了一定程度的平移不变性，因此与完全连接的网络相比，它的误码率显着降低到1.62％。 然而，具有4次瞥见的RAM性能比卷积网络略好，并且在6次和8次瞥见中进一步胜过它，达到1.2％的误差。 这是可能的因为注意模型可以将其视网膜集中在数字上并因此学习转化不变策略。 该实验还表明，当对象不居中时，注意模型能够成功地搜索大图像中的对象。</p>
<p><strong>凌乱的非中心位数</strong>：分类真实世界图像最具挑战性的方面之一是存在广泛的杂波。 以全分辨率操作整个图像的系统特别容易混乱，并且必须学会不变。 注意机制的一个可能的优点是它可以通过关注图像的相关部分并且忽略不相关部分而使得在存在杂乱的情况下学习更容易。 我们用一些我们称之为Cluttered Translated MNIST的新任务进行了几次实验来验证这个假设。 该任务的数据是通过首先将MNIST数字放置在较大空白图像的随机位置，然后将来自其他随机MNIST数字的随机8乘8个子图像添加到图像的随机位置来生成的。 目标是分类图像中的完整数字。 图2b显示了60乘60混杂翻译MNIST任务的测试用例的随机样本。<br>
</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 394px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 25.419999999999998%;\"></div>
<div class=\"image-view\" data-width=\"1550\" data-height=\"394\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1621805-301c3a6a7d27229e.png\" data-original-width=\"1550\" data-original-height=\"394\" data-original-format=\"image/png\" data-original-filesize=\"190774\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p></p>
<blockquote>
<p>表2：Cluttered Translated MNIST数据集上的分类。 FC表示具有两层整流器单元的全连接网络。卷积网络有一层8×10×10的滤波器，步长为5，其次是在60×60的情况下有256个单元的完全连接层，在100×100的情况下有86个单元，每层之后有整流器。关注模型的实例标有视线数量，视网膜大小和视网膜片数量。除大型全连接网络外，所有型号的参数数量大致相同。</p>
</blockquote>
<p>表2a显示了我们用60块Cluttered Translated MNIST对4块杂波进行训练的模型的分类结果。 杂波的存在使得任务变得更加困难，但是关注模型的性能受到的影响要小于其他模型的性能。 具有4次glimpse的RAM达到4.96％的误差，其大幅超过完全连接的模型并且卷积神经网络超过3％，并且用6次和8次glimpse训练的RAM实现更低的误差。 由于RAM在杂波存在的情况下对卷积网络的相对误差提高较大，这些结果表明，基于注意力模型在处理杂波时可能比卷积网络更好，因为它们可以通过不看它而简单地忽略它。 学习策略的两个样本如图3所示，更多的样本包含在补充材料中。</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 332px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 30.630000000000003%;\"></div>
<div class=\"image-view\" data-width=\"1084\" data-height=\"332\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1621805-70b957b69dad20f4.png\" data-original-width=\"1084\" data-original-height=\"332\" data-original-format=\"image/png\" data-original-filesize=\"74185\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<blockquote>
<p>图3：在60×60混杂转换的MNIST任务上学习策略的示例。 第1列：带有glimpse路径的输入图像以绿色覆盖。 第2-7列：网络选择的六个glimpse。 每个图像的中心显示全分辨率的glimpse，外部低分辨率区域通过将低分辨率glimpse升级为完整图像大小而获得。glimpse路径清楚地表明，学习策略避免了在输入空间的空白或嘈杂部分进行计算，并直接探索感兴趣对象周围的区域。</p>
</blockquote>
<p>第一列显示覆盖了glimpse路径的原始数据点。第一次glimpse的位置用实心圆标记，并且最后glimpse的位置用空圆圈标记。 路径上的中间点用实线直线描绘。 右侧的每个连续图像都显示网络看到的glimpse。 可以看出，学习策略可以可靠地发现和探索感兴趣的对象，同时避免混乱。 最后，表2a还包括8-glimpse RAM模型的结果，其随机均匀地选择glimpse位置。 学习glimpse策略的RAM模型即使只有一半的glimpse，也可以实现低得多的错误率。</p>
<p>为了进一步检验这个假设，我们还对100×100 Cluttered Translated MNIST和8块杂波进行了实验。 表2b列出了我们比较的模型所实现的测试误差。 结果显示了卷积网络上RAM的类似改进。必须指出的是，我们的模型的整体容量和计算量并没有从60×60图像变为100×100，而连接到线性层的卷积网络的隐藏层随着输入中像素的数量线性增长。</p>
<h2>4.2 动态环境</h2>
<p>循环注意力模型的一个吸引人的特性是它可以像静态图像任务一样容易地应用于具有视觉输入的视频或交互式问题。 我们测试了我们的方法在动态视觉环境中学习控制策略的能力，同时通过训练它来玩简单的游戏，通过带宽有限的视网膜感知环境。 该游戏在24×24像素的二进制像素屏幕上播放，涉及两个对象：一个像素表示一个从屏幕顶部落下的球，同时弹出屏幕两侧，以及一个位于屏幕底部的双像素桨，该屏幕由控制器控制以捕捉球。 当下降像素到达屏幕底部时，如果桨与球交迭，则代理获得1奖励，否则奖励0。 游戏然后从头开始重新开始。</p>
<p>我们训练了循环注意力模型，只使用最终奖励作为输入来玩“Catch”游戏。 该网络在三个尺度上有一个6×6的视网膜作为其输入，这意味着该代理必须在6×6的最高分辨率区域捕捉球，以便知道其精确位置。 除了两个位置动作之外，关注模型还有三个游戏动作（左，右，并且什么都不做），并且动作网络$f_a$使用线性softmax来模拟游戏动作的分布。 我们使用了256个LSTM单元的核心网络。</p>
<p>我们进行随机搜索以找到合适的超参数并为每个代理训练2000万帧。 最佳代理的视频大约有85％的时间捕获球，可以从<a href=\"http://www.cs.toronto.edu/vmnih/docs/retention.mov\" target=\"_blank\" rel=\"nofollow\">http://www.cs.toronto.edu/vmnih/docs/retention.mov</a>下载。 该视频显示，循环注意力模型通过追踪屏幕底部附近的球来学习玩游戏。 由于代理人没有以任何方式被告知追踪球并且仅因为获得奖励而获得奖励，这一结果表明该模型有能力学习有效的特定任务的关注策略。</p>
<h1>5 讨论</h1>
<p>本文介绍了一种新颖的视觉注意模型，该模型被设计为一个单一的循环神经网络，它以一个glimpse窗口为输入，利用网络的内部状态来选择下一个要关注的位置，以及在动态的环境。尽管模型不可区分，但是所提出的统一架构是从像素输入到使用策略梯度方法的操作端到端地进行的。该模型有几个吸引人的属性。首先，可以独立于输入图像的大小来控制参数的数量和RAM执行的计算量。其次，该模型可以通过将视网膜对准相关区域来忽略图像中的杂乱。我们的实验显示RAM在杂乱的对象分类任务上明显胜过卷积体系结构，其参数数量可比。此外，我们的方法的灵活性允许一些有趣的扩展。例如，网络可以增加另一个动作，允许它在任何时间点终止并做出最终的分类决定。我们的初步实验表明，这可以让网络学会停止glimpse，一旦它有足够的信息做出自信的分类。网络也可以被允许控制视网膜对图像进行采样的尺度，从而允许其在固定尺寸的视网膜中适应不同尺寸的对象。在这两种情况下，额外的动作都可以简单地添加到动作网络$f_a$中，并使用我们描述的策略渐变过程进行训练。鉴于RAM所取得的令人鼓舞的成果，将该模型应用于大规模目标识别和视频分类是未来工作的一个自然方向。</p>
<br>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 540px; max-height: 959px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 137.04000000000002%;\"></div>
<div class=\"image-view\" data-width=\"540\" data-height=\"740\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1621805-e7dd694099f756b7\" data-original-width=\"540\" data-original-height=\"740\" data-original-format=\"\" data-original-filesize=\"40562\"></div>
</div>
<div class=\"image-caption\"></div>
</div>

          </div>','1531183725'),
('325575','{394211}{1124331}{1124332}{260}{1077}{761}{14699}{17}{975714}{1124333}{976103}{442}{39}{74}{975728}{1277}{2592}{1124334}{975727}{975717}{257}{3302}{975899}{1124335}{1751}{1968}{36}{62}{2336}{1124336}{975911}{975843}{701}{975951}{3307}{300}{1436}{9464}{4448}{1190}{1325}{975761}{975729}{884}{975786}{975788}{975755}{976063}{975779}{1687}','view(features.size(0), -1) out = self.classifier(out) return out 选择不同网络参数，就可以实现不同深度的DenseNet，这里实现DenseNet-121网络，而且Pytorch提供了预训练好的网络参数： def densenet121(pretrained=False, **kwargs): \"\"\"DenseNet121\"\"\" model = DenseNet(num_init_features=64, growth_rate=32, block_config=(6, 12, 24, 16), **kwargs) if pretrained: # \'.\':norm|relu|conv))\\.','DenseNet：比ResNet更优的CNN模型','<div class=\"show-content-free\">
            <h1><strong>前  言</strong></h1>
<p>在计算机视觉领域，卷积神经网络（CNN）已经成为最主流的方法，比如最近的GoogLenet，VGG-19，Incepetion等模型。CNN史上的一个里程碑事件是ResNet模型的出现，ResNet可以训练出更深的CNN模型，从而实现更高的准确度。ResNet模型的核心是通过建立前面层与后面层之间的“短路连接”（shortcuts，skip connection），这有助于训练过程中梯度的反向传播，从而能训练出更深的CNN网络。今天我们要介绍的是DenseNet模型，它的基本思路与ResNet一致，但是它建立的是前面所有层与后面层的密集连接（dense connection），它的名称也是由此而来。DenseNet的另一大特色是通过特征在channel上的连接来实现特征重用（feature reuse）。这些特点让DenseNet在参数和计算成本更少的情形下实现比ResNet更优的性能，DenseNet也因此斩获CVPR 2017的最佳论文奖。本篇文章首先介绍DenseNet的原理以及网路架构，然后讲解DenseNet在Pytorch上的实现。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 674px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 68.43%;\"></div>
<div class=\"image-view\" data-width=\"985\" data-height=\"674\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-2acf9f631f285466\" data-original-width=\"985\" data-original-height=\"674\" data-original-format=\"image/jpeg\" data-original-filesize=\"63868\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<h1><strong>设计理念</strong></h1>
<p>相比ResNet，DenseNet提出了一个更激进的密集连接机制：即互相连接所有的层，具体来说就是每个层都会接受其前面所有层作为其额外的输入。图1为ResNet网络的连接机制，作为对比，图2为DenseNet的密集连接机制。可以看到，ResNet是每个层与前面的某层（一般是2~3层）短路连接在一起，连接方式是通过元素级相加。而在DenseNet中，每个层都会与前面所有层在channel维度上连接（concat）在一起（这里各个层的特征图大小是相同的，后面会有说明），并作为下一层的输入。对于一个L层的网络，DenseNet共包含个L*(L+1)/2 连接，相比ResNet，这是一种密集连接。而且DenseNet是直接concat来自不同层的特征图，这可以实现特征重用，提升效率，这一特点是DenseNet与ResNet最主要的区别。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 365px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 34.599999999999994%;\"></div>
<div class=\"image-view\" data-width=\"1055\" data-height=\"365\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-7b1b6ef5b02ec02b\" data-original-width=\"1055\" data-original-height=\"365\" data-original-format=\"image/jpeg\" data-original-filesize=\"41154\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>图1 ResNet网络的短路连接机制（其中+代表的是元素级相加操作）</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 498px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 46.67%;\"></div>
<div class=\"image-view\" data-width=\"1067\" data-height=\"498\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-e405e43be476ffb2\" data-original-width=\"1067\" data-original-height=\"498\" data-original-format=\"image/jpeg\" data-original-filesize=\"59668\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>图2 DenseNet网络的密集连接机制（其中c代表的是channel级连接操作）</p>
<p>如果用公式表示的话，传统的网络在L层的输出为：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 308px; max-height: 104px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 33.77%;\"></div>
<div class=\"image-view\" data-width=\"308\" data-height=\"104\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-38c4977cb30e3253\" data-original-width=\"308\" data-original-height=\"104\" data-original-format=\"image/png\" data-original-filesize=\"2355\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>而对于ResNet，增加了来自上一层输入的identity函数：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 418px; max-height: 96px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 22.97%;\"></div>
<div class=\"image-view\" data-width=\"418\" data-height=\"96\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-2439fff4ece4cd0f\" data-original-width=\"418\" data-original-height=\"96\" data-original-format=\"image/png\" data-original-filesize=\"3149\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>在DenseNet中，会连接前面所有层作为输入：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 524px; max-height: 82px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 15.65%;\"></div>
<div class=\"image-view\" data-width=\"524\" data-height=\"82\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-1b57a2f086bca81b\" data-original-width=\"524\" data-original-height=\"82\" data-original-format=\"image/png\" data-original-filesize=\"3637\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p></p>
<p></p>
其中，上面的<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 104px; max-height: 62px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 59.62%;\"></div>
<div class=\"image-view\" data-width=\"104\" data-height=\"62\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-5cc02ff26d1de49d\" data-original-width=\"104\" data-original-height=\"62\" data-original-format=\"image/png\" data-original-filesize=\"1395\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>代表是非线性转化函数（non-liear transformation），它是一个组合操作，其可能包括一系列的BN(Batch Normalization)，ReLU，Pooling及Conv操作。注意这里L层与L-1层之间可能实际上包含多个卷积层。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 243px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 22.5%;\"></div>
<div class=\"image-view\" data-width=\"1080\" data-height=\"243\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-5691346a30980b98\" data-original-width=\"1080\" data-original-height=\"243\" data-original-format=\"image/jpeg\" data-original-filesize=\"32254\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>图3 DenseNet的前向过程</p>
<p>CNN网络一般要经过Pooling或者stride&gt;1的Conv来降低特征图的大小，而DenseNet的密集连接方式需要特征图大小保持一致。为了解决这个问题，DenseNet网络中使用DenseBlock+Transition的结构，其中DenseBlock是包含很多层的模块，每个层的特征图大小相同，层与层之间采用密集连接方式。而Transition模块是连接两个相邻的DenseBlock，并且通过Pooling使特征图大小降低。图4给出了DenseNet的网路结构，它共包含4个DenseBlock，各个DenseBlock之间通过Transition连接在一起。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 320px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 29.630000000000003%;\"></div>
<div class=\"image-view\" data-width=\"1080\" data-height=\"320\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-92ab7e73b34a603f\" data-original-width=\"1080\" data-original-height=\"320\" data-original-format=\"image/jpeg\" data-original-filesize=\"38024\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<h1><strong>网络结构</strong></h1>
<p>如前所示，DenseNet的网络结构主要由DenseBlock和Transition组成，如图5所示。下面具体介绍网络的具体实现细节。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 319px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 29.54%;\"></div>
<div class=\"image-view\" data-width=\"1080\" data-height=\"319\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-5fcab3a571d8ff2c\" data-original-width=\"1080\" data-original-height=\"319\" data-original-format=\"image/jpeg\" data-original-filesize=\"45918\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>图6 DenseNet的网络结构</p>
<p></p>
<p></p>
在DenseBlock中，各个层的特征图大小一致，可以在channel维度上连接。DenseBlock中的非线性组合函数<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 81px; max-height: 49px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 60.49%;\"></div>
<div class=\"image-view\" data-width=\"81\" data-height=\"49\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-acc5c1fce3973bf0\" data-original-width=\"81\" data-original-height=\"49\" data-original-format=\"image/jpeg\" data-original-filesize=\"1470\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p></p>
<p></p>
采用的BN+ReLU+3x3 Conv的结构，如图6所示。另外值得注意的一点是，与ResNet不同，所有DenseBlock中各个层卷积之后均输出个k特征图，即得到的特征图的channel数为k，或者说采用k个卷积核。k在DenseNet称为growth rate，这是一个超参数。一般情况下使用较小的k（比如12），就可以得到较佳的性能。假定输入层的特征图的channel数为<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 44px; max-height: 52px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 118.17999999999999%;\"></div>
<div class=\"image-view\" data-width=\"44\" data-height=\"52\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-da15e2e28f89bb69\" data-original-width=\"44\" data-original-height=\"52\" data-original-format=\"image/png\" data-original-filesize=\"728\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p></p>
<p></p>
，那么L层输入的channel数为<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 238px; max-height: 48px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 20.169999999999998%;\"></div>
<div class=\"image-view\" data-width=\"238\" data-height=\"48\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-cadb5872e070a489\" data-original-width=\"238\" data-original-height=\"48\" data-original-format=\"image/jpeg\" data-original-filesize=\"2701\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>，因此随着层数增加，尽管k设定得较小，DenseBlock的输入会非常多，不过这是由于特征重用所造成的，每个层仅有k个特征是自己独有的。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 634px; max-height: 350px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 55.21%;\"></div>
<div class=\"image-view\" data-width=\"634\" data-height=\"350\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-7cda618549ebc5b5\" data-original-width=\"634\" data-original-height=\"350\" data-original-format=\"image/jpeg\" data-original-filesize=\"25815\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>图6 DenseBlock中的非线性转换结构</p>
<p>由于后面层的输入会非常大，DenseBlock内部可以采用bottleneck层来减少计算量，主要是原有的结构中增加1x1 Conv，如图7所示，即BN+ReLU+1x1 Conv+BN+ReLU+3x3 Conv，称为DenseNet-B结构。其中1x1 Conv得到4k个特征图它起到的作用是降低特征数量，从而提升计算效率。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 364px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 33.7%;\"></div>
<div class=\"image-view\" data-width=\"1080\" data-height=\"364\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-609a8aeba8114ec4\" data-original-width=\"1080\" data-original-height=\"364\" data-original-format=\"image/jpeg\" data-original-filesize=\"37661\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>图7 使用bottleneck层的DenseBlock结构</p>
<p></p>
<p></p>
对于Transition层，它主要是连接两个相邻的DenseBlock，并且降低特征图大小。Transition层包括一个1x1的卷积和2x2的AvgPooling，结构为BN+ReLU+1x1 Conv+2x2 AvgPooling。另外，Transition层可以起到压缩模型的作用。假定Transition的上接DenseBlock得到的特征图channels数为m，Transition层可以产生<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 96px; max-height: 61px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 63.54%;\"></div>
<div class=\"image-view\" data-width=\"96\" data-height=\"61\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-981e053d3181a370\" data-original-width=\"96\" data-original-height=\"61\" data-original-format=\"image/jpeg\" data-original-filesize=\"1737\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p></p>
<p></p>
个特征（通过卷积层），其中<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 174px; max-height: 54px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 31.03%;\"></div>
<div class=\"image-view\" data-width=\"174\" data-height=\"54\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-e75052eda5ec6b01\" data-original-width=\"174\" data-original-height=\"54\" data-original-format=\"image/png\" data-original-filesize=\"1626\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p></p>
<p></p>
是压缩系数（compression rate）。当<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 106px; max-height: 52px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 49.059999999999995%;\"></div>
<div class=\"image-view\" data-width=\"106\" data-height=\"52\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-2b77e4fba45bdfd1\" data-original-width=\"106\" data-original-height=\"52\" data-original-format=\"image/png\" data-original-filesize=\"846\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p></p>
<p></p>
时，特征个数经过Transition层没有变化，即无压缩，而当压缩系数小于1时，这种结构称为DenseNet-C，文中使用<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 142px; max-height: 48px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 33.800000000000004%;\"></div>
<div class=\"image-view\" data-width=\"142\" data-height=\"48\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-2333cdfd022ef1a2\" data-original-width=\"142\" data-original-height=\"48\" data-original-format=\"image/png\" data-original-filesize=\"1281\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>。对于使用bottleneck层的DenseBlock结构和压缩系数小于1的Transition组合结构称为DenseNet-BC。</p>
<p>DenseNet共在三个图像分类数据集（CIFAR，SVHN和ImageNet）上进行测试。对于前两个数据集，其输入图片大小为32<em>32，所使用的DenseNet在进入第一个DenseBlock之前，首先进行进行一次3x3卷积（stride=1），卷积核数为16（对于DenseNet-BC为2K）。DenseNet共包含三个DenseBlock，各个模块的特征图大小分别为32</em>32，16<em>16和8</em>8，每个DenseBlock里面的层数相同。最后的DenseBlock之后是一个global AvgPooling层，然后送入一个softmax分类器。注意，在DenseNet中，所有的3x3卷积均采用padding=1的方式以保证特征图大小维持不变。对于基本的DenseNet，使用如下三种网络配置：{L = 40,k = 12},{L = 100,k = 12},{L = 40,k = 24}。而对于DenseNet-BC结构，使用如下三种网络配置{L = 100, k = 12},{L = 250,k = 24},{L = 190,k = 40}。这里的L指的是网络总层数（网络深度），一般情况下，我们只把带有训练参数的层算入其中，而像Pooling这样的无参数层不纳入统计中，此外BN层尽管包含参数但是也不单独统计，而是可以计入它所附属的卷积层。对于普通的{L = 0,K = 12}网络，除去第一个卷积层、2个Transition中卷积层以及最后的Linear层，共剩余36层，均分到三个DenseBlock可知每个DenseBlock包含12层。其它的网络配置同样可以算出各个DenseBlock所含层数。</p>
<p>对于ImageNet数据集，图片输入大小为224*224，网络结构采用包含4个DenseBlock的DenseNet-BC，其首先是一个stride=2的7x7卷积层（卷积核数为2K），然后是一个stride=2的3x3 MaxPooling层，后面才进入DenseBlock。ImageNet数据集所采用的网络配置如表1所示：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 525px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 48.61%;\"></div>
<div class=\"image-view\" data-width=\"1080\" data-height=\"525\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-551b95004703b80f\" data-original-width=\"1080\" data-original-height=\"525\" data-original-format=\"image/jpeg\" data-original-filesize=\"101356\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p><strong>实验结果与讨论</strong></p>
<p>这里给出DenseNet在CIFAR-100和ImageNet数据集上与ResNet的对比结果，如图8和9所示。从图8中可以看到，只有0.8M的DenseNet-100性能已经超越ResNet-1001，并且后者参数大小为10.2M。而从图9中可以看出，同等参数大小时，DenseNet也优于ResNet网络。其它实验结果见原论文。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 503px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 46.88%;\"></div>
<div class=\"image-view\" data-width=\"1073\" data-height=\"503\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-f25780a405a8faa9\" data-original-width=\"1073\" data-original-height=\"503\" data-original-format=\"image/jpeg\" data-original-filesize=\"60798\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>图8 在CIFAR-100数据集上ResNet vs DenseNet</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 504px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 51.53%;\"></div>
<div class=\"image-view\" data-width=\"978\" data-height=\"504\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-d634cd979a11a7a4\" data-original-width=\"978\" data-original-height=\"504\" data-original-format=\"image/jpeg\" data-original-filesize=\"58261\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>图9 在ImageNet数据集上ResNet vs DenseNet</p>
<p>综合来看，DenseNet的优势主要体现在以下几个方面：</p>
<ul>
<li><p>由于密集连接方式，DenseNet提升了梯度的反向传播，使得网络更容易训练。由于每层可以直达最后的误差信号，实现了隐式的“deep supervision”；超链接：<a href=\"https://arxiv.org/abs/1409.5185\" target=\"_blank\" rel=\"nofollow\">https://arxiv.org/abs/1409.5185</a></p></li>
<li><p>参数更小且计算更高效，这有点违反直觉，由于DenseNet是通过concat特征来实现短路连接，实现了特征重用，并且采用较小的growth rate，每个层所独有的特征图是比较小的；</p></li>
<li><p>由于特征复用，最后的分类器使用了低级特征。</p></li>
</ul>
<p>要注意的一点是，如果实现方式不当的话，DenseNet可能耗费很多GPU显存，一种高效的实现如图10所示，更多细节可以见这篇论文Memory-Efficient Implementation of DenseNets，超链接：<a href=\"https://arxiv.org/abs/1707.06990\" target=\"_blank\" rel=\"nofollow\">https://arxiv.org/abs/1707.06990</a>。不过我们下面使用Pytorch框架可以自动实现这种优化。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 522px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 53.05%;\"></div>
<div class=\"image-view\" data-width=\"984\" data-height=\"522\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-b120c75003dd7f12\" data-original-width=\"984\" data-original-height=\"522\" data-original-format=\"image/jpeg\" data-original-filesize=\"75076\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>图10 DenseNet的更高效实现方式</p>
<h1><strong>使用Pytorch实现Denseet</strong></h1>
<p>这里我们采用<strong>Pytorch</strong>框架(<a href=\"https://pytorch.org/\" target=\"_blank\" rel=\"nofollow\">https://pytorch.org/</a>)来实现DenseNet，目前它已经支持Windows系统。对于DenseNet，Pytorch在<strong>torchvision.models</strong>模块(<a href=\"https://github.com/pytorch/vision/tree/master/torchvision/models\" target=\"_blank\" rel=\"nofollow\">https://github.com/pytorch/vision/tree/master/torchvision/models</a>)里给出了官方实现，这个DenseNet版本是用于ImageNet数据集的DenseNet-BC模型，下面简单介绍实现过程。</p>
<p>首先实现DenseBlock中的内部结构，这里是BN+ReLU+1x1 Conv+BN+ReLU+3x3 Conv结构，最后也加入dropout层以用于训练过程。</p>
<pre><code>class _DenseLayer(nn.Sequential):
    \"\"\"Basic unit of DenseBlock (using bottleneck layer) \"\"\"
    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):
        super(_DenseLayer, self).__init__()
        self.add_module(\"norm1\", nn.BatchNorm2d(num_input_features))
        self.add_module(\"relu1\", nn.ReLU(inplace=True))
        self.add_module(\"conv1\", nn.Conv2d(num_input_features, bn_size*growth_rate,
                                           kernel_size=1, stride=1, bias=False))
        self.add_module(\"norm2\", nn.BatchNorm2d(bn_size*growth_rate))
        self.add_module(\"relu2\", nn.ReLU(inplace=True))
        self.add_module(\"conv2\", nn.Conv2d(bn_size*growth_rate, growth_rate,
                                           kernel_size=3, stride=1, padding=1, bias=False))
        self.drop_rate = drop_rate

    def forward(self, x):
        new_features = super(_DenseLayer, self).forward(x)
        if self.drop_rate &gt; 0:
            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)
        return torch.cat([x, new_features], 1)
</code></pre>
<p>据此，实现DenseBlock模块，内部是密集连接方式（输入特征数线性增长）：</p>
<pre><code>class _DenseBlock(nn.Sequential):
    \"\"\"DenseBlock\"\"\"
    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate):
        super(_DenseBlock, self).__init__()
        for i in range(num_layers):
            layer = _DenseLayer(num_input_features+i*growth_rate, growth_rate, bn_size,
                                drop_rate)
            self.add_module(\"denselayer%d\" % (i+1,), layer)
</code></pre>
<p>此外，我们实现Transition层，它主要是一个卷积层和一个池化层：</p>
<pre><code>class _Transition(nn.Sequential):
    \"\"\"Transition layer between two adjacent DenseBlock\"\"\"
    def __init__(self, num_input_feature, num_output_features):
        super(_Transition, self).__init__()
        self.add_module(\"norm\", nn.BatchNorm2d(num_input_feature))
        self.add_module(\"relu\", nn.ReLU(inplace=True))
        self.add_module(\"conv\", nn.Conv2d(num_input_feature, num_output_features,
                                          kernel_size=1, stride=1, bias=False))
        self.add_module(\"pool\", nn.AvgPool2d(2, stride=2))
</code></pre>
<p>最后我们实现DenseNet网络：</p>
<pre><code>class DenseNet(nn.Module):
    \"DenseNet-BC model\"
    def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16), num_init_features=64,
                 bn_size=4, compression_rate=0.5, drop_rate=0, num_classes=1000):
        \"\"\"
        :param growth_rate: (int) number of filters used in DenseLayer, `k` in the paper
        :param block_config: (list of 4 ints) number of layers in each DenseBlock
        :param num_init_features: (int) number of filters in the first Conv2d
        :param bn_size: (int) the factor using in the bottleneck layer
        :param compression_rate: (float) the compression rate used in Transition Layer
        :param drop_rate: (float) the drop rate after each DenseLayer
        :param num_classes: (int) number of classes for classification
        \"\"\"
        super(DenseNet, self).__init__()
        # first Conv2d
        self.features = nn.Sequential(OrderedDict([
            (\"conv0\", nn.Conv2d(3, num_init_features, kernel_size=7, stride=2, padding=3, bias=False)),
            (\"norm0\", nn.BatchNorm2d(num_init_features)),
            (\"relu0\", nn.ReLU(inplace=True)),
            (\"pool0\", nn.MaxPool2d(3, stride=2, padding=1))
        ]))

        # DenseBlock
        num_features = num_init_features
        for i, num_layers in enumerate(block_config):
            block = _DenseBlock(num_layers, num_features, bn_size, growth_rate, drop_rate)
            self.features.add_module(\"denseblock%d\" % (i + 1), block)
            num_features += num_layers*growth_rate
            if i != len(block_config) - 1:
                transition = _Transition(num_features, int(num_features*compression_rate))
                self.features.add_module(\"transition%d\" % (i + 1), transition)
                num_features = int(num_features * compression_rate)

        # final bn+ReLU
        self.features.add_module(\"norm5\", nn.BatchNorm2d(num_features))
        self.features.add_module(\"relu5\", nn.ReLU(inplace=True))

        # classification layer
        self.classifier = nn.Linear(num_features, num_classes)

        # params initialization
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.bias, 0)
                nn.init.constant_(m.weight, 1)
            elif isinstance(m, nn.Linear):
                nn.init.constant_(m.bias, 0)

    def forward(self, x):
        features = self.features(x)
        out = F.avg_pool2d(features, 7, stride=1).view(features.size(0), -1)
        out = self.classifier(out)
        return out
</code></pre>
<p>选择不同网络参数，就可以实现不同深度的DenseNet，这里实现DenseNet-121网络，而且Pytorch提供了预训练好的网络参数：</p>
<pre><code>def densenet121(pretrained=False, **kwargs):
    \"\"\"DenseNet121\"\"\"
    model = DenseNet(num_init_features=64, growth_rate=32, block_config=(6, 12, 24, 16),
                     **kwargs)

    if pretrained:
        # \'.\'s are no longer allowed in module names, but pervious _DenseLayer
        # has keys \'norm.1\', \'relu.1\', \'conv.1\', \'norm.2\', \'relu.2\', \'conv.2\'.
        # They are also in the checkpoints in model_urls. This pattern is used
        # to find such keys.
        pattern = re.compile(
            r\'^(.*denselayer\\d+\\.(?:norm|relu|conv))\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$\')
        state_dict = model_zoo.load_url(model_urls[\'densenet121\'])
        for key in list(state_dict.keys()):
            res = pattern.match(key)
            if res:
                new_key = res.group(1) + res.group(2)
                state_dict[new_key] = state_dict[key]
                del state_dict[key]
        model.load_state_dict(state_dict)
    return model
</code></pre>
<p>下面，我们使用预训练好的网络对图片进行测试，这里给出top-5预测值：</p>
<pre><code>densenet = densenet121(pretrained=True)
densenet.eval()

img = Image.open(\"./images/cat.jpg\")

trans_ops = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])

images = trans_ops(img).view(-1, 3, 224, 224)
outputs = densenet(images)

_, predictions = outputs.topk(5, dim=1)

labels = list(map(lambda s: s.strip(), open(\"./data/imagenet/synset_words.txt\").readlines()))
for idx in predictions.numpy()[0]:
    print(\"Predicted labels:\", labels[idx])
</code></pre>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 480px; max-height: 360px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 75.0%;\"></div>
<div class=\"image-view\" data-width=\"480\" data-height=\"360\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-8cede1454c850b11\" data-original-width=\"480\" data-original-height=\"360\" data-original-format=\"image/jpeg\" data-original-filesize=\"30183\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>给出的预测结果为：</p>
<pre><code>Predicted labels: n02123159 tiger cat
Predicted labels: n02123045 tabby, tabby cat
Predicted labels: n02127052 lynx, catamount
Predicted labels: n02124075 Egyptian cat
Predicted labels: n02119789 kit fox, Vulpes macrotis
</code></pre>
<h1><strong>小结</strong></h1>
<p>这篇文章详细介绍了DenseNet的设计理念以及网络结构，并给出了如何使用Pytorch来实现。值得注意的是，DenseNet在ResNet基础上前进了一步，相比ResNet具有一定的优势，但是其却并没有像ResNet那么出名（吃显存问题？深度不能太大？）。期待未来有更好的网络模型出现吧！</p>
<p>****参考文献****</p>
<p>1.DenseNet-CVPR-Slides: <a href=\"http://www.cs.cornell.edu/~gaohuang/papers/DenseNet-CVPR-Slides.pdf\" target=\"_blank\" rel=\"nofollow\">http://www.cs.cornell.edu/~gaohuang/papers/DenseNet-CVPR-Slides.pdf</a></p>
<p>2.Densely Connected Convolutional Networks :<a href=\"https://arxiv.org/abs/1608.06993\" target=\"_blank\" rel=\"nofollow\">https://arxiv.org/abs/1608.06993</a></p>
<hr>
<blockquote>
<p>个人技术博客：<a href=\"https://blog.csdn.net/u013709270\" target=\"_blank\" rel=\"nofollow\">https://blog.csdn.net/u013709270</a><br>
微信：TonyJeemy520      加个人微信拉你进机器学习、深度学习交流群，请备注 : 来自简书<br>
QQ交流群：651616387   请备注 : 来自简书<br>
微信公众号：机器学习算法工程师   ----二维码见下图</p>
</blockquote>
<hr>
<p><strong>扫码关注微信号：机器学习算法工程师，更多干货分享， 或加个人微信，拉你进机器学习、深度学习交流群</strong><br>
</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 430px; max-height: 430px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 100.0%;\"></div>
<div class=\"image-view\" data-width=\"430\" data-height=\"430\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-0bdf004ac80c0197\" data-original-width=\"430\" data-original-height=\"430\" data-original-format=\"image/jpeg\" data-original-filesize=\"32906\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p></p>

          </div>','1531183728'),
('325576','{1124339}{87411}{503}{1858}{90738}{8986}{36}{975835}{976042}{384862}{5699}{62}{829}{975708}{569}{975779}{4046}{84}{6386}{1188}{576}{517}{978260}{975915}{74326}{408}{2743}{2341}{562}{40303}{1901}{3087}{975714}{975842}{975811}{3220}{2224}{975741}{5291}{658}{975748}{975911}{4701}{4054}{976557}{2372}{63}{9081}','5- 深度学习之神经网络核心原理与算法-正则化 正则化 正则化是机器学习中一种常见的概念。正则化不仅在深度学习中有，在传统的机器学习中也有。 单纯从名字上不好理解。但是其实它的意义还是比较简单的。 泛化能力 机器学习中，通常通过大量样本放入模型中训练然后得到待定的系数。 而不论是哪种模型我们都希望这种模型在精确的前提下尽可能的简洁。这里我们所','5- 深度学习之神经网络核心原理与算法-正则化','<div class=\"show-content-free\">
            <h2>正则化</h2>
<p>正则化是机器学习中一种常见的概念。正则化不仅在深度学习中有，在传统的机器学习中也有。</p>
<p>单纯从名字上不好理解。但是其实它的意义还是比较简单的。</p>
<h3>泛化能力</h3>
<p>机器学习中，通常通过大量样本放入模型中训练然后得到待定的系数。</p>
<p>而不论是哪种模型我们都希望这种模型在精确的前提下尽可能的简洁。这里我们所说的精确不止是在测试集上精确就够了。</p>
<p>泛化能力好: 模型在测试集以及其他验证集上也要表现的同样好。</p>
<p>日常: 对于观察到的各种认知对象来说，描述共性的东西越抽象，越简洁，其泛化性也就越好。</p>
<p>相反，越是精确描述个体的东西，通常“个性化”的特点就非常明显，越具体，越复杂，泛化性就越差。</p>
<h3>泛化能力举例</h3>
<p>我们描述一个物体是方的。通常指这是一个四边形，两两平行，并两两垂直。</p>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180331/KEL7GKCAk9.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180331/KEL7GKCAk9.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<p>忽略了颜色，大小等诸多特征。参数变多了，有了约束性，泛化性变低。</p>
<p>描述的更为具体，参数更多，泛化性就会是这三个词中最低的一个。</p>
<p>正则化的过程就是来为我们找到更为简洁的描述方式的量化过程。</p>
<h3>L1正则化</h3>
<p>对于损失函数的改造。</p>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180331/34JFkaFfG7.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180331/34JFkaFfG7.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<p>这就是改造完毕，带有正则化项的损失函数。我们之前接触到的损失函数只有C0这一部分。没有后面的:</p>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180331/cej0HbBJa9.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180331/cej0HbBJa9.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<p>前面的损失函数值C0，我们称之为经验风险.</p>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180331/LGIleIebH4.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180331/LGIleIebH4.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<p>后面的表达式加入了正则项的，叫做结构风险</p>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180331/bGCKkJ2iJ6.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180331/bGCKkJ2iJ6.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<blockquote>
<p>结构风险就是我们刚刚提到的风险，我们希望这种描述能够简洁来保证泛化性的良好。</p>
</blockquote>
<p>这个正则项含义是把整个模型中所有的权重w绝对值加起来之后除以样本的总数量n。</p>
<p>这里n上面的分子 拉姆达 不是我们在机器学习中所提到的学习率，而是一个权重，称之为正则化系数或惩罚系数。</p>
<p>表示对这部分有多重视，如果你很重视结构风险，很不希望结构风险太大。我们就加大 拉姆达 的值，迫使损失函数向着权值减小的方向快速移动。</p>
<p>换句话说就是w的值越大，整个因子的值就越大。也就是我们说的越不简洁。</p>
<p>这里我们说的正则化因子其实叫做L1正则化项。</p>
<h3>L2正则化项</h3>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180331/8LIHG03ja2.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180331/8LIHG03ja2.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<p>只不过将绝对值变成了w的平方，将n变成了2n</p>
<h3>L1正则项的损失函数导数</h3>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180331/bcAhhEmK3h.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180331/bcAhhEmK3h.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<p>我们求偏c/ 偏w和以前不一样了。</p>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180331/Kb31hAimC2.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180331/Kb31hAimC2.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<p>我们反向更新的时候，也和以前不一样了。</p>
<p>sgn函数表示取w的符号。大于0表示正1，小于0表示负1</p>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180331/JdHiIcKcKL.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180331/JdHiIcKcKL.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<p>整个导数，除了经验风险对w贡献的部分，还有后面结构风险对于w求导贡献的部分。</p>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180331/JJkG1f5jm5.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180331/JJkG1f5jm5.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<h3>L2正则项的导数</h3>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180331/a78L3im9ik.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180331/a78L3im9ik.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<h3>可视化正则化的实现过程。</h3>
<p>假设在一个模型中只有两个维度，w1和w2作为待定的系数，最终的理想解在圆心或者说抛物线的最低点。</p>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180331/Dc985cl38l.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180331/Dc985cl38l.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<p>这里在第一象限只是我们画出来在第一象限。由于w1和w2在初始化时可能在空白处的<br>
任何地方，那么在训练的过程中，w1和w2就会逐步从从初始化的位置，向圆心靠拢。</p>
<p>圆心就是我们的最优解，在训练过程中w1和w2会从上下左右任何可能的方向向圆心靠拢。</p>
<p>因为w1和w2在初始化的时候可以在任何的位置。圆心周围的这一圈蓝色的线，代表损失函数的等高线。也就是w1 w2组成的坐标点在这一圈上的任意位置产生的损失值是相同大小。</p>
<p>随机初始化，因此w1和w2可能出现在圈上的任意位置。显然离坐标系原点(0,0)更远的点(w1,w2)会产生更大的结构风险。因为离坐标系原点更远的点w1 w2坐标的值就会更大。</p>
<p>这里我们再看下黄色圆圈和黄色正方形所围成的面积，就分别代表L1和L2正则化公式所产生的损失值。</p>
<p>左边是L2的，右边是L1的。边缘的圆圈线和直线分别表示他们各自的损失函数值的等高线。</p>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180331/hAkdl0g7ii.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180331/hAkdl0g7ii.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<p>看左边的公式就可以知道L2围成一个圆形，L1围成一个正方形。这里加入正则化项之后损失就会由两部分组成，一个是上面的这个蓝色圈圈，一个是下面黄色的部分。</p>
<p>那么在训练时，上面的部分会约束w向着圆心收敛，下面这一部分会约束w1向着原点收敛。</p>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180331/bj2iA8j67K.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180331/bj2iA8j67K.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<p>最后的解会兼顾这两部分，也就是图中的w星这点。</p>
<h2>L2正则项的导数</h2>
<p>如何在代码中添加正则化项。我们准备在代码里添加L2正则化项。</p>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180403/hCeEKic3Gd.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180403/hCeEKic3Gd.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<p>L2正则化项主要是改变了W的更新公式。这里出现了一个变量 拉姆达。</p>
<p>这个变量是我们人工指定的。</p>
<p>首先SGD方法中添加一个lmbda变量,默认值0.0</p>
<pre><code>    def SGD(self, training_data, epochs, mini_batch_size, eta,
            lmbda = 0.0,
            test_data=None):
</code></pre>
<p>找到w的更新公式。将lmbda传进去。</p>
<pre><code>            # 训练mini_batch
            for mini_batch in mini_batches:
                self.update_mini_batch(mini_batch, eta, lmbda, n)
</code></pre>
<p>同时我们也要传入训练集的总个数n，接着我们要处理update_mini_batch方法。</p>
<pre><code class=\"python\"> # 更新mini_batch
    def update_mini_batch(self, mini_batch, eta, lmbda, n):
</code></pre>
<p>为w的更新方程添加后面那一项。</p>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180403/LGIfFD7bd3.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180403/LGIfFD7bd3.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<pre><code class=\"python\">        # 更新权重和偏置 Wn+1 = wn - eta * nw
        self.weights = [(1 - eta*(lmbda/n))*w - (eta / len(mini_batch)) * nw
                        for w, nw in zip(self.weights, nabla_w)]
        self.biases = [b - (eta / len(mini_batch)) * nb
                       for b, nb in zip(self.biases, nabla_b)]
</code></pre>
<p>提取公因式。</p>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180403/EG073fIHcG.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180403/EG073fIHcG.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>

          </div>','1531183729'),
('325577','{1672}{1098600}{186833}{30}{1124343}{975915}{442}{975714}{975703}{1350}{975757}{341}{975727}{62}{46114}{110906}{3328}{24}{189573}{198307}{103195}{1124345}{1124347}{3592}{327}{744}{975892}{1411}{775}{976193}{2425}{975761}{4605}{27351}{1031}{975919}{1094}{975741}{36}{868}{975951}{1352}{977450}{728}{1051098}{975841}{1237}{17}{1124349}','id=SJU4ayYgl Modeling Relational Data with Graph Convolutional Networks https://arxiv.org/abs/1703.06103 Inductive Representation Learning on Large Graphs https://arxiv.org/abs/1706.02216 欢迎持续关注我们的技术专栏，可以关注微信公众号并在后台回复“论文”就可以获取三篇论文啦～ 还可以添加技术助理微信“geetest1024”微信，一起交流进步！','浅析图卷积神经网络','<div class=\"show-content-free\">
            <p> 技术专栏</p>
<p><b>本文作者：刘忠雨</b></p>
<p><b>由萝卜兔编辑整理</b></p>
<p>今天想和大家分享的是图卷积神经网络。随着人工智能发展，很多人都听说过机器学习、深度学习、卷积神经网络这些概念。但图卷积神经网络，却不多人提起。那什么是图卷积神经网络呢？简单的来说就是其研究的对象是图数据（Graph），研究的模型是卷积神经网络。</p>
<p><b>为什么有图卷积神经网络</b></p>
<p>自2012年以来，深度学习在计算机视觉以及自然语言处理两个领域取得了巨大的成功。和传统方法相比，它好在哪里呢？</p>
<p>假设有一张图，要做分类，传统方法需要手动提取一些特征，比如纹理啊，颜色啊，或者一些更高级的特征。然后再把这些特征放到像随机森林等分类器，给到一个输出标签，告诉它是哪个类别。而深度学习是输入一张图，经过神经网络，直接输出一个标签。特征提取和分类一步到位，避免了手工提取特征或者人工规则，从原始数据中自动化地去提取特征，是一种端到端（end-to-end）的学习。相较于传统的方法，深度学习能够学习到更高效的特征与模式。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 405px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 56.25%;\"></div>
<div class=\"image-view\" data-width=\"720\" data-height=\"405\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/7803390-28fb68f5c439c31f\" data-original-width=\"720\" data-original-height=\"405\" data-original-format=\"image/jpeg\" data-original-filesize=\"30102\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>卷积神经网络很好，但是它研究的对象还是限制在Euclidean domains的数据。什么是Euclidean data？ Euclidean data最显著的特征就是有规则的空间结构，比如图片是规则的正方形栅格，比如语音是规则的一维序列。而这些数据结构能够用一维、二维的矩阵表示，卷积神经网络处理起来很高效。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 185px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 17.130000000000003%;\"></div>
<div class=\"image-view\" data-width=\"1080\" data-height=\"185\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/7803390-87548b344f5bfd02\" data-original-width=\"1080\" data-original-height=\"185\" data-original-format=\"image/jpeg\" data-original-filesize=\"21940\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>但是，我们的现实生活中有很多数据并不具备规则的空间结构，称为Non Euclidean data。比如推荐系统、电子交易、计算几何、脑信号、分子结构等抽象出的图谱。这些图谱结构每个节点连接都不尽相同，有的节点有三个连接，有的节点有两个连接，是不规则的数据结构。</p>
<p>下面结合两个典型的业务场景来说明什么是图：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 650px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 60.19%;\"></div>
<div class=\"image-view\" data-width=\"1080\" data-height=\"650\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/7803390-b51c639a3953016e\" data-original-width=\"1080\" data-original-height=\"650\" data-original-format=\"image/jpeg\" data-original-filesize=\"33451\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>社交网络非常适合用图数据来表达</p>
<p>上面的图谱刻画社交网络中各个节点以及它们之间的关系，用户A、用户B、帖子都是节点，用户与用户之间的关系是关注，用户与帖子之间的关系可能是发布或者转发。通过这样一个图谱，可以分析用户对什么人、什么事感兴趣，进一步实现推荐机制。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 650px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 60.19%;\"></div>
<div class=\"image-view\" data-width=\"1080\" data-height=\"650\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/7803390-b05d732deceb6222\" data-original-width=\"1080\" data-original-height=\"650\" data-original-format=\"image/jpeg\" data-original-filesize=\"37072\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>电商场景中的图谱</p>
<p>在电商中，我们首先可以想到的关键节点就是，用户、交易和商品。用户关联的节点比如会有注册地址、收获地址等；交易会关联到商品、收货地址、交易IP等、商品会关联类目等。这些节点之间的关系，比如用户除了可以通过交易购买商品，还可以对商品进行评分。这样的图数据我们可以用来做两件事情，一是推荐、二是反欺诈。</p>
<p>通过上面两个例子，可以很明显的感受到，图有两个基本的特性：</p>
<p><b>一是每个节点都有自己的特征信息。</b>比如针对上图，我们建立一个风控规则，要看这个用户的注册地址、IP地址、交易的收货地址是否一样，如果这些特征信息不匹配，那么系统就会判定这个用户就存在一定的欺诈风险。这是对图节点特征信息的应用。</p>
<p><b>二是图谱中的每个节点还具有结构信息。</b>如果某段时间某个IP节点连接的交易节点非常多，也就是说从某个IP节点延伸出来的边非常多，那么风控系统会判定这个IP地址存在风险。这是对图节点结构信息的应用。</p>
<p>总的来说，在图数据里面，我们要同时考虑到节点的特征信息以及结构信息，如果靠手工规则来提取，必将失去很多隐蔽和复杂的模式，那么有没有一种方法能自动化地同时学到图的特征信息与结构信息呢？——图卷积神经网络</p>
<p><b>什么是图卷积神经网络</b></p>
<p>图卷积神经网络（Graph Convolutional Network）是一种能对图数据进行深度学习的方法。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 526px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 75.28%;\"></div>
<div class=\"image-view\" data-width=\"1080\" data-height=\"813\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/7803390-78a449b2158e2f54\" data-original-width=\"1080\" data-original-height=\"813\" data-original-format=\"image/jpeg\" data-original-filesize=\"17370\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p><b>图卷积算子：</b></p>
<p><b><br></b></p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 344px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 31.85%;\"></div>
<div class=\"image-view\" data-width=\"1080\" data-height=\"344\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/7803390-8e7feec1250af1a8.png\" data-original-width=\"1080\" data-original-height=\"344\" data-original-format=\"image/png\" data-original-filesize=\"77132\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>上面给出的是图卷积算子的计算公式，设中心节点为i；</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 548px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 50.739999999999995%;\"></div>
<div class=\"image-view\" data-width=\"1080\" data-height=\"548\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/7803390-225c633234629291\" data-original-width=\"1080\" data-original-height=\"548\" data-original-format=\"image/jpeg\" data-original-filesize=\"65324\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p><b>如何理解图卷积算法？我们看动图分三步去理解（注意不同颜色代表不同的权重）：</b></p>
<p>第一步：发射（send）每一个节点将自身的特征信息经过变换后发送给邻居节点。这一步是在对节点的特征信息进行抽取变换。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 560px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 80.0%;\"></div>
<div class=\"image-view\" data-width=\"1000\" data-height=\"800\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/7803390-08e00a8fc4041f9d\" data-original-width=\"1000\" data-original-height=\"800\" data-original-format=\"image/gif\" data-original-filesize=\"38361\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>第二步：接收（receive）每个节点将邻居节点的特征信息聚集起来。这一步是在对节点的局部结构信息进行融合。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 560px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 80.0%;\"></div>
<div class=\"image-view\" data-width=\"1000\" data-height=\"800\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/7803390-983aa31f57260e1a\" data-original-width=\"1000\" data-original-height=\"800\" data-original-format=\"image/gif\" data-original-filesize=\"38866\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>第三步：变换（transform）把前面的信息聚集之后做非线性变换，增加模型的表达能力。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 560px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 80.0%;\"></div>
<div class=\"image-view\" data-width=\"1000\" data-height=\"800\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/7803390-538212c7e8060829\" data-original-width=\"1000\" data-original-height=\"800\" data-original-format=\"image/gif\" data-original-filesize=\"67825\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p></p>
<p><b>图卷积神经网络具有卷积神经网络的以下性质：</b></p>
<p>1、局部参数共享，算子是适用于每个节点（圆圈代表算子），处处共享。</p>
<p>2、感受域正比于层数，最开始的时候，每个节点包含了直接邻居的信息，再计算第二层时就能把邻居的邻居的信息包含进来，这样参与运算的信息就更多更充分。层数越多，感受域就更广，参与运算的信息就更多。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 640px; max-height: 512px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 80.0%;\"></div>
<div class=\"image-view\" data-width=\"640\" data-height=\"512\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/7803390-d917891b5cc6b895\" data-original-width=\"640\" data-original-height=\"512\" data-original-format=\"image/gif\" data-original-filesize=\"784256\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>我们来看GCN这个模型框架，输入是一张图，经过一层一层计算变换，最后输出一张图。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 604px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 55.93%;\"></div>
<div class=\"image-view\" data-width=\"1080\" data-height=\"604\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/7803390-3848a1aac1ad57dc\" data-original-width=\"1080\" data-original-height=\"604\" data-original-format=\"image/jpeg\" data-original-filesize=\"46935\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p><b>GCN模型同样具备深度学习的三种性质：</b></p>
<p>1、层级结构（特征一层一层抽取，一层比一层更抽象，更高级）；</p>
<p>2、非线性变换 （增加模型的表达能力）；</p>
<p>3、端对端训练（不需要再去定义任何规则，只需要给图的节点一个标记，让模型自己学习，融合特征信息和结构信息。）</p>
<p><b>GCN四个特征：</b></p>
<p>1、GCN 是对卷积神经网络在 graph domain 上的自然推广。</p>
<p>2、它能同时对节点特征信息与结构信息进行端对端学习，是目前对图数据学习任务的最佳选择。</p>
<p>3、图卷积适用性极广，适用于任意拓扑结构的节点与图。</p>
<p>4、在节点分类与边预测等任务上，在公开数据集上效果要远远优于其他方法。</p>
<p><b>我们怎么用图卷积神经网络</b></p>
<p>下面分享一个我们在实际应用场景中的实验：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 560px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 80.0%;\"></div>
<div class=\"image-view\" data-width=\"1080\" data-height=\"864\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/7803390-62d1247802f67f68\" data-original-width=\"1080\" data-original-height=\"864\" data-original-format=\"image/jpeg\" data-original-filesize=\"23687\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>实验输入是一个验证数据构成的图数据，节点是验证事件以及事件相关的属性节点。如IP，DeviceID，UA等节点。（我们总计用了30天的验证数据，每两个小时的数据构成一张图，共360张图。）</p>
<p>实验输出是对事件节点进行人机分类，正常或者异常。</p>
<p><b>实验细节</b></p>
<p><b>网络结构：</b></p>
<p>GCN(128)-&gt;GCN(64)-&gt;GCN(64)-&gt;Linear(2)</p>
<p><b>训练:</b>Adam优化器, lr=0.001</p>
<p><b>参照基准:</b>以只能学习特征信息的GBDT做为基准, grid_search 搜索超参数，GBDT是目前最流行的浅层分类器。</p>
<p>我们用第一天的数据做训练，持续30天预测结果如下：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 677px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 62.69%;\"></div>
<div class=\"image-view\" data-width=\"1080\" data-height=\"677\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/7803390-de5f79e24d43f38a\" data-original-width=\"1080\" data-original-height=\"677\" data-original-format=\"image/jpeg\" data-original-filesize=\"35007\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>GCN模型的准确率衰减比较小，而GBDT的衰减很严重。可见，GCN模型的人机判别效果要好，鲁棒性好。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 615px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 87.87%;\"></div>
<div class=\"image-view\" data-width=\"1080\" data-height=\"949\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/7803390-33f40295782b18b6\" data-original-width=\"1080\" data-original-height=\"949\" data-original-format=\"image/jpeg\" data-original-filesize=\"113744\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>7d评估效果可视化，（用第一天的数据训练模型，第七天观察其预测效果及最后一层输出的tsne可视化结果)。上图可以看出，GCN在第七天时对样本判别的分界面仍很明显，但是GBDT对样本判别的分界面已经很模糊类了。综上，GCN学到的结构信息在人机判别中不仅效果很好，也具有更好的鲁棒性。</p>
<p><b>写在最后</b></p>
<p>由于时间有限，很多问题浅尝辄止，关于GCN还有很多有趣的东西。我们将开设专栏《Graph Learning》，作者会分享给大家更加全面的图学习算法。</p>
<p>一直以来，我们都认为自己是一家技术驱动的公司，也说自己是AI公司。AI公司并不是那么高大上，实际上有时候很难，因为很多技术还不成熟，没有现成的可以用。在做公司业务的时候，会遇上一些现实难题，会踩到很多坑，当然也会收获一些感悟和经验。我们想，这些是企业创造的另外一种价值，应当好好利用。</p>
<p>“你有一种思想，我有一种思想，互相交换后，我们都拥有两种思想”这便是分享的意义。因此，后期我们还会整理一系列的干货分享专栏，以期把在实际应用中总结、学习、创造的知识分享给大家。当然，也非常欢迎有人可以一起探讨、交流、进步，这是我们做这件事情最期待的回馈。</p>
<p><b>《Graph Learning》专栏大纲</b></p>
<p>第一章       图及其应用场景</p>
<p>第二章       图的传播算法</p>
<p>第三章       社群检测以及高密子图</p>
<p>第四章       异构信息网络</p>
<p>第五章       图表示学习</p>
<p>第六章       图卷积神经网络</p>
<p>总共六章内容，预计25-30篇幅，感兴趣的小伙伴欢迎持续关注哦～</p>
<p>论文推荐</p>
<p>Semi-Supervised Classification with Graph Convolutional Networks</p>
<p>https://openreview.net/pdf?id=SJU4ayYgl</p>
<p>Modeling Relational Data with Graph Convolutional Networks</p>
<p>https://arxiv.org/abs/1703.06103</p>
<p>Inductive Representation Learning on Large Graphs</p>
<p>https://arxiv.org/abs/1706.02216</p>
<p>欢迎持续关注我们的技术专栏，可以<b>关注微信公众号</b>并在后台回复“<b>论文</b>”就可以获取三篇论文啦～</p>
<p>还可以添加技术助理微信“<b>geetest1024</b>”微信，一起交流进步！</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 658px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 94.13%;\"></div>
<div class=\"image-view\" data-width=\"750\" data-height=\"706\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/7803390-0dff97cf9e40cc03\" data-original-width=\"750\" data-original-height=\"706\" data-original-format=\"image/jpeg\" data-original-filesize=\"44879\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
          </div>','1531183731'),
('325578','{309263}{363492}{975757}{1123833}{39}{36}{1173}{187552}{1753}{6632}{32255}{15346}{1123834}{975741}{6499}{1069}{296928}{1123835}{1072}{401474}{405500}{1737}{17}{976042}{975911}{975761}{975728}{1277}{465}{89243}{1123836}{1751}{634}{360074}{102769}{1123837}{503}{326652}{62}{976193}{7369}{408}{1314}{884}{74}{975835}{133662}{9193}{268197}{5131}','5记为Positive，小于0.','Evaluate the Malignancy of Pulmonary Nodules Using','<div class=\"show-content-free\">
            <h1>Evaluate the Malignancy of Pulmonary Nodules Using the 3D Deep Leaky Noisy-or Network 论文阅读</h1>
<hr>
<p>原文：<a href=\"https://arxiv.org/pdf/1711.08324v1.pdf\" target=\"_blank\" rel=\"nofollow\">Evaluate the Malignancy of Pulmonary Nodules Using the 3D Deep Leaky Noisy-or Network</a></p>
<p>博文参考：<a href=\"https://blog.csdn.net/qq_25624231\" target=\"_blank\" rel=\"nofollow\">Doublle Tree的博客</a>中<a href=\"https://blog.csdn.net/qq_25624231/article/details/79632072\" target=\"_blank\" rel=\"nofollow\">Evaluate the Malignancy of Pulmonary Nodules Using the 3D Deep Leaky Noisy-or Network 论文阅读</a>一文。</p>
<p>注：本文为2017年Kaggle举办的数据科学竞赛中，第一名获奖团队的相关论文，若需查看代码可访问<a href=\"https://github.com/lfz/DSB2017\" target=\"_blank\" rel=\"nofollow\">Github</a>。</p>
<hr>
<h2>简介</h2>
<p>根据CT图像的肺癌自动诊断系统包含以下步骤：</p>
<ol>
<li>检测所有可疑病变；</li>
<li>评估整个肺部的恶性程度。</li>
</ol>
<p>但目前大多数的研究主要集中于第一步，以及通过肺结节诊断肺癌存在较高的假阳性率。因此，肺癌的诊断需要对每个可疑结节进行细致分析，再联合所有结节信息进行定性诊断。针对上述问题，本文提出了一个三维深度神经网络（3D deep neural network）用于解决这些问题。该网络由两部分组成：</p>
<ol>
<li>用于结节检测的3D region proposal network；</li>
<li>基于置信检测（the detection confidence）选出top-5结节并评估其癌症可能性，最后将此概率与Leaky noisy-or模型相结合评估患者患癌的可能性。</li>
</ol>
<p>其中，上述两个模型均采用修改后的<a href=\"https://arxiv.org/abs/1505.04597\" target=\"_blank\" rel=\"nofollow\">U-net模型</a>，并使用数据增强操作避免过拟合问题。</p>
<h2>数据集和预处理</h2>
<h3>数据集</h3>
<p>训练集由LUNA16数据集（the Lung Nodule Analysis 2016）和NDSB3（Data Science Bowl 2017）数据集两部分组成。其中，LUNA16数据集含有888个病例，标记了1186个肺结节；在NDSB3数据集中，1397个病例用于训练，198个病例用于验证，506个病例用于测试，且人工标注了训练集中754个结节和验证集中78个结节。</p>
<p>对于LUNA16数据集，其存在许多较小的注释结节，且临床经验认为直径6mm以下的肺结节无危险。但在NDSB3数据集中，存在较多的大直径结节且结节多与主支气管相连。因此，针对两个数据集的差异，需去除LUNA16数据集中直径6mm的结节，同时对NDSB3数据集进行人工标注。</p>
<blockquote>
<p>此处说明了<a href=\"https://blog.csdn.net/u013058162/article/details/79987363\" target=\"_blank\" rel=\"nofollow\">Julian de Wit的解决方案</a>中，直接设置结节直径为6mm的原因，以及为何需对NDSB3数据集进行人工标注。</p>
</blockquote>
<div class=\"image-package\">
<img src=\"http://static.zybuluo.com/Rookie-FCB/54icqr0md7zqxe0ogubkmm50/image_1cdh21aa1402v6unub1jcj1pvh9.png\" data-original-src=\"http://static.zybuluo.com/Rookie-FCB/54icqr0md7zqxe0ogubkmm50/image_1cdh21aa1402v6unub1jcj1pvh9.png\"><div class=\"image-caption\"></div>
</div>
<p>上图为结节分布情况图。其中，图a为DSB（NDSB3）与LUNA（LUNA16）数据集中结节直径分布情况；图b为DSB数据集中患癌患者与健康人群的最大结节直径分布情况。</p>
<h3>预处理</h3>
<p>首先将所有的原始数据转变为HU值，如下图a所示，再进行如下步骤：</p>
<div class=\"image-package\">
<img src=\"http://static.zybuluo.com/Rookie-FCB/48xv962cib9wvwdjtoemg4i5/Screenshot%20from%202018-05-15%2013:41:24.png\" data-original-src=\"http://static.zybuluo.com/Rookie-FCB/48xv962cib9wvwdjtoemg4i5/Screenshot%20from%202018-05-15%2013:41:24.png\"><div class=\"image-caption\"></div>
</div>
<ol>
<li><p>掩膜提取：在2D切片上，首先使用标准差为1的高斯滤波和阈值为-600的处理得到肺部以及周围较暗部分的掩膜，如上图b所示，然后进行连通性分析去除小于30mm<sup>2</sup>的connected component和离心率大于0.99的部分（some high-luminance radial imaging noise），再计算得到二值的3D矩阵中所有的3D connected component，且仅保留非边缘部分（用于去除肺部周围较暗的部分）以及体积在0.68~7.5L之间的部分，结果如上图c所示；</p></li>
<li>
<p>凸包与扩张：若结节与肺的外壁相连，则其将不会出现在上述提取的掩膜中。因此，对于这种情况，首先将肺部分为左右两个部分，即左肺与右肺，如上图d所示。然后分别对左右肺进行凸包处理，并向外扩张10像素，如上图f所示。但对于一些2D切片而言，肺部的底部类似与月牙形，如下图所示。若对于该类型进行凸包处理后，面积大于初始的1.5倍，则放弃凸包，从而避免引入过多的其他组织；</p>
<br>
<div class=\"image-package\">
<img src=\"http://static.zybuluo.com/Rookie-FCB/dbz8ktogy0hwsxs9jnv4zeb9/image_1cdh5tfqkhgo1r941k60sts18l312.png\" data-original-src=\"http://static.zybuluo.com/Rookie-FCB/dbz8ktogy0hwsxs9jnv4zeb9/image_1cdh5tfqkhgo1r941k60sts18l312.png\"><div class=\"image-caption\"></div>
</div>
</li>
<li><p>灰度标准化：将HU值（[-1200, 600]）线性变换至0~255内的灰度值，且掩膜以外的像素灰度值均设为170，以及扩张区域内的像素灰度值高于210则也设为170。</p></li>
</ol>
<h2>用于结节检测的3D卷积神经网络</h2>
<p>该网络是基于U-net的3D版<a href=\"https://hal.inria.fr/hal-01349107v2/document\" target=\"_blank\" rel=\"nofollow\">RPN（Region Proposal Network）模型</a>。</p>
<h3>输入数据</h3>
<p>受限于显存，输入数据大小为128×128×128×1（Height×Length×Width×Channel），并随机选择两种patch：一种为70%的输入数据至少包含一个结节；另一种为30%的输入数据不含结节。其中，patch超出图像部分用灰度值为170填充。</p>
<p>为了避免过拟合问题，数据采用数据增强方法。</p>
<blockquote>
<p>从输入数据大小可看出，本文作者采用的显卡为专业卡，其显存大。鉴于此，可根据实际情况将输入数据大小调整为64或者32。</p>
</blockquote>
<h3>网络结构</h3>
<p>网络由前馈路径和反馈路径组成，如下图图a所示。</p>
<div class=\"image-package\">
<img src=\"http://static.zybuluo.com/Rookie-FCB/2pex11z7ghnx2gmd8bw4k5mg/image_1ce37f9qh1aabrhk1qp174g19v49.png\" data-original-src=\"http://static.zybuluo.com/Rookie-FCB/2pex11z7ghnx2gmd8bw4k5mg/image_1ce37f9qh1aabrhk1qp174g19v49.png\"><div class=\"image-caption\"></div>
</div>
<h4>前馈路径</h4>
<p>以两层卷积核为2×2×2的卷积（channel为24）开始，且padding为1；其后为4个残差块，其中每个残差块由3个残差单元组成（如上图图b所示），而每个残差单元由卷积、Batch Norm、ReLU激活函数、卷积和Batch Norm组成，且卷积核大小均为3×3×3。除此之外，每个残差块均有一个最大池化层，大小为2×2×2，步长为2。</p>
<h4>反馈路径</h4>
<p>反馈路径由两层反卷积（装置卷积）层和两个融合单元构成。最后，由卷积核均为1×1×1且channel为64和15的两层卷积层将数据大小转换为32×32×32×15。</p>
<h5>反卷积层</h5>
<p>卷积核大小为2，步长为2。</p>
<blockquote>
<p>注意该部分的代码实现部分，原始U-net网络设置为不可学习。</p>
</blockquote>
<h5>融合单元</h5>
<p>每个融合单元（如上图图c所示）均由一个前馈blob和反馈blob组成，其结果作为残差块的输入。</p>
<blockquote>
<p>值得注意的一点，本文作者在此处引入了位置信息，作为额外的输入数据。</p>
</blockquote>
<hr>
<p><strong>位置信息</strong></p>
<p>proposal的位置信息可能影响是否为结节和是否为恶性的判断，因而引入位置信息。</p>
<p>具体方法：对于每个patch，计算其相对位置坐标，并将其大小转换为32×32×32×3。</p>
<p>其中，位置坐标对应归一化后的X，Y和Z轴（每个轴的取值范围为-1~1，对应于肺的两端）。</p>
<hr>
<h4>输出层</h4>
<p>输出数据为4D的tensor，32×32×32×3×5，其中3表示anchor个数，5表示回归量（即概率，三维坐标和bounding box直径大小）。</p>
<p>其中，对于概率这一参数的激活函数采用sigmoid函数，其余不使用任何激活函数。</p>
<h3>损失函数</h3>
<p>真值标签为(G<sub>x</sub>, G<sub>y</sub>, G<sub>z</sub>, G<sub>r</sub>)，每个anchor记为(A<sub>x</sub>, A<sub>y</sub>, A<sub>z</sub>, A<sub>r</sub>)，IoU（Intersection over Unit）大于0.5记为Positive，小于0.02记为False，其他在训练过程中忽略。</p>
<p>分类损失为：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 530px; max-height: 69px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 13.020000000000001%;\"></div>
<div class=\"image-view\" data-width=\"530\" data-height=\"69\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/5983416-6f8a96e7c0a455cd.png\" data-original-width=\"530\" data-original-height=\"69\" data-original-format=\"image/png\" data-original-filesize=\"10810\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>其中，p为anchor box的真值标签。</p>
<p>bounding box回归标签为：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 280px; max-height: 375px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 133.93%;\"></div>
<div class=\"image-view\" data-width=\"280\" data-height=\"375\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/5983416-ff827c2f0ce747f9.png\" data-original-width=\"280\" data-original-height=\"375\" data-original-format=\"image/png\" data-original-filesize=\"25734\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>回归总损失为：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 434px; max-height: 115px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 26.5%;\"></div>
<div class=\"image-view\" data-width=\"434\" data-height=\"115\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/5983416-32c696a3ff8238e4.png\" data-original-width=\"434\" data-original-height=\"115\" data-original-format=\"image/png\" data-original-filesize=\"11332\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>其中，S为smoothed L1-norm函数：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 592px; max-height: 127px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 21.45%;\"></div>
<div class=\"image-view\" data-width=\"592\" data-height=\"127\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/5983416-c940eb7b255a7515.png\" data-original-width=\"592\" data-original-height=\"127\" data-original-format=\"image/png\" data-original-filesize=\"17592\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>对于每个anchor box的损失函数为：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 290px; max-height: 92px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 31.72%;\"></div>
<div class=\"image-view\" data-width=\"290\" data-height=\"92\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/5983416-9d8d6a35038c9a07.png\" data-original-width=\"290\" data-original-height=\"92\" data-original-format=\"image/png\" data-original-filesize=\"5173\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>最后，整体的anchor box的损失函数为anchor box的损失值取平均。</p>
<h3>正反例数据</h3>
<h4>正例数据</h4>
<p>对于大结节而言，网络会生成较多的positive anchor box，因此为了降低训练数据之间的相关性，随机挑选其中一个。</p>
<p>由于结节直径大小分布不均，而NDSB3数据集多为大结节，因而对大于30mm和40mm的结节，采样频率分别是其他结节的2倍和6倍。</p>
<blockquote>
<p>此处对于NDSB3竞赛得分有利，实际是否可行有待商榷。</p>
</blockquote>
<h4>反例数据</h4>
<p>对于一些易误诊为结节的反例数据，通过使用hard negative mining方法解决。</p>
<p>具体方法为：</p>
<ol>
<li>将不同的patch输入至网络得到不同置信度的输出映射；</li>
<li>随机选择N个反例数据构成候选池；</li>
<li>侯选池中的数据以置信度值大小排序，且选出top-n的数据作为反例数据。</li>
</ol>
<p>未选中的数据忽略且不参与损失计算。</p>
<blockquote>
<p>此处可借鉴该方法，尽可能降低假阳性率，以及加速模型训练。</p>
</blockquote>
<h3>图像分割（测试过程）</h3>
<p>输入数据大小为208×208×208×1，overlap为32像素。</p>
<p>输出数据为{x<sub>i</sub>, y<sub>i</sub>, z<sub>i</sub>, r<sub>i</sub>, p<sub>i</sub>}，其中x<sub>i</sub>, y<sub>i</sub>, z<sub>i</sub>表示proposal中心坐标，r<sub>i</sub>表示其半径大小，p<sub>i</sub>表示其置信度。</p>
<p>输出数据且使用非极大值抑制操作来去除overlaping proposal。</p>
<h2>肿瘤分类</h2>
<p>由于受限于训练样本数，因而复用结节检测器阶段的N-net网络。</p>
<p>输入数据为结节的proposal，大小均为96×96×96×1，其仅使用了结节中心点的信息。在分类器训练阶段，随机挑选proposal，且其选中的概率与proposal的置信度成正比；在测试阶段，只挑选top-5的proposal。</p>
<p>经卷积核为24×24×24×128的最后一个卷积层得到输出结果；随后提取每个proposal中心处2×2×2的体素，并将其通过最大池化操作后得到128维的特征，如下图图a所示。</p>
<p>对比四种预测肿瘤类别的方法（Feature combining method，MaxP method，Noisy-or method和Leaky Noisy-or method），挑选出Leaky Noisy-or方法作为最终的分类方法，如下图图b所示。</p>
<div class=\"image-package\">
<img src=\"http://static.zybuluo.com/Rookie-FCB/q3kusjz3mp2c71qd1yl7bwdv/image_1cegc5gkk1pl114vv1lribh14rk9.png\" data-original-src=\"http://static.zybuluo.com/Rookie-FCB/q3kusjz3mp2c71qd1yl7bwdv/image_1cegc5gkk1pl114vv1lribh14rk9.png\"><div class=\"image-caption\"></div>
</div>
<hr>
<p><strong>Leaky Noisy-or Method</strong></p>
<p>引入一个假想结节，其患癌概率为P<sub>d</sub>，P<sub>d</sub>的值在模型训练阶段学习获得。</p>
<p>将特征输入至两层相同的Perceptron得到分类概率P：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 450px; max-height: 104px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 23.11%;\"></div>
<div class=\"image-view\" data-width=\"450\" data-height=\"104\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/5983416-65dbf7bf3c1df4b6.png\" data-original-width=\"450\" data-original-height=\"104\" data-original-format=\"image/png\" data-original-filesize=\"7779\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>其中，P<sub>i</sub>表示第i个结节癌变的概率。</p>
<hr>
<h3>训练过程</h3>
<p>损失函数为交叉熵函数。为了避免过拟合采用了数据增强和正则化操作。</p>
<p>训练的步骤：</p>
<ol>
<li>transfer检测器训练参数后，再训练分类器；</li>
<li>采用gradient clipping方法训练分类器，随后存储BN（Batch Normalization）参数；</li>
<li>用存储的BN参数和gradient clipping方法交替训练检测器和分类器。</li>
</ol>
<p>注：BN在训练阶段和测试阶段所计算的方法有所差异。因复用N-net网络，分类器和检测器交替训练，因而需对BN的参数做特殊处理。</p>
<p><strong><a href=\"https://101804240000891.bqy.mobi\" target=\"_blank\" rel=\"nofollow\">版权印版权标识</a></strong></p>

          </div>','1531183732'),
('325579','{62}{4391}{1483}{131}{975728}{36}{13951}{39}{192}{2119}{975741}{408}{17}{975721}{2214}{975773}{733}{975761}{1869}{406}{975708}{2193}{6101}{2000}{2557}{975707}{975951}{111}{74}{975714}{247}{12}{975757}{40}{599452}{975785}{1124352}{620}{125609}{975877}{6812}{2118}{975915}{976063}{975727}{975786}{517}{158}{448}{975891}','format(accuracy, n)) 增加两个函数，total_cost和accuracy(传入一个参数convert) 当然也可以看一下在测试集上的表现是什么样的 if test_data: cost = self.total_cost(test_data, lmbda, convert=True) print(\"Cost on test data: {}\".','7- 深度学习之神经网络核心原理与算法-模型的保存与加载','<div class=\"show-content-free\">
            <h2>模型的保存与加载</h2>
<ul>
<li>网络训练完毕你需要保存，以便在产品上使用。(手写识别模型要识别新的图片)</li>
<li>保存网络的结构，权重，偏置，使用的损失函数。</li>
<li>使用别人的模型或者对已有模型进行微调。</li>
</ul>
<p>训练中断之后从该轮模型参数继续往后进行训练。不需要重新开始。</p>
<h3>添加模型保存的相关代码</h3>
<p>在network类中添加保存模型的方法。需要一个参数filename，保存到哪里</p>
<pre><code class=\"python\"> # 保存模型
    def save(self, filename):
        data = {\"sizes\": self.sizes,
                \"weights\": [w.tolist() for w in self.weights],
                \"biases\": [b.tolist() for b in self.biases],
                \"cost\": str(self.cost.__name__)
        }
        f = open(filename, \"w\")
        json.dump(data, f)
        f.close()
</code></pre>
<p>size是一个列表，定义了一共有多少层，每层有多少个神经元。保存模型权重。</p>
<p>w是numpy的array类型，调用它的tolist方法，把它转换成python的列表类型。</p>
<p>这里保存的是cost的类名字。(CrossEntropyCost)，json的dump方法可以将字典保存为字符串。</p>
<h3>加载文件</h3>
<pre><code class=\"python\"># 加载模型
def load(filename):
    f = open(filename, \"r\")
    data = json.load(f)
    f.close()
    cost = getattr(sys.modules[__name__], data[\"cost\"])
    net = Network(data[\"sizes\"], cost=cost)
    net.weights = [np.array(w) for w in data[\"weights\"]]
    net.biases = [np.array(b) for b in data[\"biases\"]]
    return net
</code></pre>
<p>json的load方法将字符串还原为我们的字典。</p>
<p>需要使用一个python的内置函数<code>getattr</code>，首先需要传入sys包。<br>
使用里面的modules方法，去获取当前的模型名字，然后使用data里面的cost，去把我们的cost对应的字符串取出来。</p>
<p>这里的意思是说，如果我，我们的这个文件在python里面是别人的另外的文件引入的，那么这个name的名字就是我们脚本的文件名。然后在我们的脚本里面去找到以CrossEntropyCost为名字的class对象。这样就可以使用它了。</p>
<p>实例化一个network。将权重偏置，网络结构进行填充初始化，然后返回这个network。</p>
<h2>应用案例—-进阶版本的前馈神经网络代码的手写数字识别</h2>
<p>为了提高神经网络的学习速度，添加了参数初始化，添加了L2正则化项，添加了交叉熵cost。使用增加的这部分代码再来做一遍手写数字识别。来看一下准确率有没有提高</p>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180403/6cdJcAL14f.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180403/6cdJcAL14f.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<p>我们首先多添加一些调试信息</p>
<p>每一轮结束之后，打印一下当前运行到了第几轮</p>
<pre><code class=\"python\">            print(\"Epoch %s training complete\" % j)
</code></pre>
<p>打印出当前的cost值在训练数据上的表现</p>
<pre><code class=\"python\">            cost = self.total_cost(training_data, lmbda)
            print(\"Cost on training data: {}\".format(cost))
</code></pre>
<p>打印一下网络预测的准确率在训练集上的表现是什么样的。</p>
<pre><code class=\"python\">            accuracy = self.accuracy(training_data, convert=True)
            print(\"Accuracy on training data: {} / {}\".format(accuracy, n))
</code></pre>
<p>增加两个函数，total_cost和accuracy(传入一个参数convert)</p>
<p>当然也可以看一下在测试集上的表现是什么样的</p>
<pre><code class=\"python\">            if test_data:
                cost = self.total_cost(test_data, lmbda, convert=True)
                print(\"Cost on test data: {}\".format(cost))
                accuracy = self.accuracy(test_data)
                print(\"Accuracy on test data: {} / {}\".format(accuracy, len(test_data)))
</code></pre>
<p>测试数据在模型上的准确率是多少？</p>
<p>计算在训练集上的cost和准确率。计算在测试集上的cost和测试集上的准确率。</p>
<h3>实现两个函数 total_cost 和 accuracy</h3>
<p>之前我们已经有一个函数去计算准确率，evaluate改为accuracy</p>
<pre><code class=\"python\">    def accuracy(self, data, convert=False):
        if convert:
            results = [(np.argmax(self.feedforward(x)), np.argmax(y))
                       for (x, y) in data]
        else:
            # 预测结果[0,1,2,3...]中最大的。然后再把真实值保存下来成为一对。 
            results = [(np.argmax(self.feedforward(x)), y)
                        for (x, y) in data]
        return sum(int(x == y) for (x, y) in results)
</code></pre>
<p>默认的convert为flase。通过判断convert来判断我们做什么事情？</p>
<p>为false就表示是测试数据集，我们就跟之前一样。</p>
<p>如果是训练数据集赋值是为真的。这个y有点变化。因为在训练集中的这个y不是一个实数，而是一个十维的向量。如果哪一维是真实的数据就会赋值成1.其他维全部为0[onehot]</p>
<p>定义一个计算损失的函数,兼容测试集和训练集两种，通过convert来区别</p>
<pre><code class=\"python\"> def total_cost(self, data, lmbda, convert=False):
        cost = 0.0
        for x, y in data:
            a = self.feedforward(x)
            if convert: y = vectorized_result(y)
            cost += self.cost.fn(a, y)/len(data)
        cost += 0.5*(lmbda/len(data))*sum(
            np.linalg.norm(w)**2 for w in self.weights)
        return cost
</code></pre>
<p>如果它是真，就表示它是测试数据集。因为测试数据集这个y是一个实数。<br>
我们要把它改变成一个onehot编码之后的数。</p>
<p>通过vectorized_result方法进行onehot编码</p>
<pre><code>def vectorized_result(j):
    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the j\'th position
    and zeroes elsewhere.  This is used to convert a digit (0...9)
    into a corresponding desired output from the neural network.

    \"\"\"
    e = np.zeros((10, 1))
    e[j] = 1.0
    return e
</code></pre>
<pre><code class=\"python\">if __name__ == \'__main__\':
    import mnist_loader

    traning_data, validation_data, test_data = mnist_loader.load_data_wrapper()

    # net = Network([784, 30, 10])
    # net.SGD(traning_data, 30, 10, 0.5, test_data=test_data)

    net = Network([784, 60, 10])
    net.SGD(traning_data, 30, 10, 0.5, 5.0, test_data=test_data)
</code></pre>
<h3>训练结果</h3>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180403/Bl97lA05f5.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180403/Bl97lA05f5.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180403/hcG6jC5i6K.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180403/hcG6jC5i6K.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>

          </div>','1531183733'),
('325580','{17}{3471}{975728}{1055}{975935}{975714}{975915}{503}{4921}{1124359}{3829}{3956}{4391}{452}{408}{975910}{975741}{36}{8344}{158}{975786}{232}{975761}{975820}{975951}{74}{202806}{576}{1124352}{3087}{854}{975740}{829}{1672}{5683}{976063}{946}{1099}{975790}{986}{975717}{175859}{247}{1193}{1189}{3477}{1124}{17108}','delta(zs[-1], activations[-1], y) 使用self.cost函数来替换掉我们之前写死的二次损失函数。 mark','6- 深度学习之神经网络核心原理与算法-学习率','<div class=\"show-content-free\">
            <h2>学习率</h2>
<p>学习率 一塔 就是每次挪动中的步长。一塔通常来说给一个比较小的值会更好一些。</p>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180403/2gHC6KLeJl.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180403/2gHC6KLeJl.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<p>步子太大会导致迈过谷底。</p>
<h3>其他超参数</h3>
<p>而由于偏导数方向的改变。你再次挪动又会向着谷底的方向挪动。</p>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180403/7c634kjiad.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180403/7c634kjiad.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<p>只是由于步子还是很大，还是会迈过谷底。这样就会像上图一样来回折返。</p>
<p>到底设置为多少，是要根据自己的项目进行判断的。但是小一点的值总是要好一点的。<br>
收敛的比较慢，但是可以使loss的值下到谷底。</p>
<h3>Dropout(克服过拟合)</h3>
<p>每次训练随机丢弃一些神经元，就相当于整个网络结构发生变化</p>
<p>减少过拟合风险</p>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180403/I50K4Kg9LC.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180403/I50K4Kg9LC.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<p>在某些层上临时关闭一些节点，让他们不输入也不输出。原则上选择关闭哪些节点都是随机性的。</p>
<p>在分类阶段，将所有的节点都置于有效的状态</p>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180403/KHFeK5IEK1.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180403/KHFeK5IEK1.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<p>就可以把训练中得到的子网络并联起来使用。</p>
<h2>交叉熵( Cross Entropy)</h2>
<p>学习慢</p>
<p>有时候我们使用相同的学习率，初始化不同的w和b，开始学习的变化率会很慢。</p>
<p>网络在开始学习的时候，整个loss下降的很慢。</p>
<p>举例: 使用梯度下降法来计算w和b, 来看看它是如何学习的</p>
<blockquote>
<p>w=0.6 b=0.9 x=1.0 y=0.82 学习率=0.15</p>
</blockquote>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180403/j1I8IfC1em.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180403/j1I8IfC1em.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<p>简化后的网络。</p>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180403/1F8mH2Ae4c.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180403/1F8mH2Ae4c.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<p>我们的w和b在不断的变化，我们的cost在不断的下降。</p>
<h3>举例: 一个不好的初始化</h3>
<p>w=2.0 b=2.0 x=1.0 y=0.2 学习率=0.15</p>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180403/63K9H8IHcg.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180403/63K9H8IHcg.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<p>可以看到网络的训练开始学习的很慢。w和b的变化很慢。</p>
<p>复杂的神经网络学习很慢。</p>
<p>原因: 其实就是因为偏导很小造成的。</p>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180403/2b9k15DJhJ.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180403/2b9k15DJhJ.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180403/aAdkblEc7c.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180403/aAdkblEc7c.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<p>也就是图中某一点的斜率几乎水平了。</p>
<p>为什么偏导很小，导致学习很慢。</p>
<p>回顾一下之前的网络更新方程。</p>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180403/3F4JLJg59G.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180403/3F4JLJg59G.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<p>最后一层的偏loss/偏b 等于 预测出来的结果减去网络的标签。点乘于sigmoid的导数。</p>
<p>这里的y0 yi都是一个定值。sigmoid在Z 小于-4 或大于4.水平。斜率为0.</p>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180403/gk43iDKB9F.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180403/gk43iDKB9F.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<p>想要让网络学习快一点，也就是偏导大一些。我们就要增大sigmoid函数的导数值。</p>
<h3>定义</h3>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180403/8FbffcKCFI.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180403/8FbffcKCFI.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<p>之前我们都是使用二次cost来定义我们网络的损失函数。这里我们可以使用交叉熵来定义我们网络的损失函数。</p>
<p>我们可以重新推导一遍偏loss偏w和偏loss偏b的值。</p>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180403/chimIabkEF.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180403/chimIabkEF.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<p>如果使用交叉熵函数，而不使用二次损失函数。</p>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180403/Gi1gDDIa4A.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180403/Gi1gDDIa4A.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<p>可以看到最终的公式里就没有sigmoid的导数这一项了。同样的方法我们也可以推导出偏loss/偏b。(避免使用sigmoid的导数)</p>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180403/AIgFD5Ec4i.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180403/AIgFD5Ec4i.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<p>这里的x,n,y都是定制。sigmoid(z)是网络的预测结果。如果网络的预测结果和真实结果接近的话，整个网络的loss值就会减小。</p>
<p>如果偏差比较大的话，loss的值也会因为偏导较大而减小。</p>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180403/mc86KIc4Bh.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180403/mc86KIc4Bh.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<blockquote>
<p>可以看到情况1中loss一直随着训练轮数增加而下降。</p>
</blockquote>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180403/eBCdjehEeA.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180403/eBCdjehEeA.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<blockquote>
<p>情况2不再出现学习很慢的情况。</p>
</blockquote>
<h2>交叉熵编码实现</h2>
<p>如何在代码里面添加交叉熵(Cross Entropy)</p>
<p>Network类的初始化时我们可以定义一个损失函数。</p>
<pre><code class=\"python\">    def __init__(self, sizes, cost=CrossEntropyCost):
        # 损失函数
        self.cost = cost
</code></pre>
<p>增加一个cost参数。</p>
<p>定义一个二次cost的类</p>
<pre><code class=\"python\">class QuadraticCost(object):
    @staticmethod
    def fn(a, y):
       return 0.5 * np.linalg.norm(a -y) ** 2

    @staticmethod
    def delta(z, a, y):
        return (a - y) * sigmoid_prime(z)
</code></pre>
<p>通过staticmethod装饰器，可以直接通过类名.方法名调用(不要实例化:QuadraticCost.fn)</p>
<p>fn里面a是网络预测结果，y是真实的标签。我们返回二次cost函数。</p>
<p>1/2 乘以 (预测值-真实值)的二范数 的平方。</p>
<h3>np.linalg.norm</h3>
<p><a href=\"https://blog.csdn.net/lanchunhui/article/details/51004387\" target=\"_blank\" rel=\"nofollow\">https://blog.csdn.net/lanchunhui/article/details/51004387</a></p>
<p>再定义另一个delta方法,输入参数为z，预测值a，真实值y</p>
<p>返回(误差) 乘以 sigmoid(z)</p>
<p>这时候再定义交叉熵的类。</p>
<pre><code>class CrossEntropyCost(object):
    \'\'\'
    &gt;&gt;&gt;import numpy as np
    &gt;&gt;&gt; a = np.array([[np.nan,np.inf],\\
    ...               [-np.nan,-np.inf]])
    &gt;&gt;&gt; a
    array([[  nan,   inf],
           [  nan,  -inf]])
    &gt;&gt;&gt; np.nan_to_num(a)
    array([[ 0.00000000e+000,  1.79769313e+308],
           [ 0.00000000e+000, -1.79769313e+308]])
    \'\'\'
    @staticmethod
    def fn(a, y):
        return np.sum(np.nan_to_num(-y * np.log(a) - (1 - y) * np.log(1 - a)))

    @staticmethod
    def delta(z, a, y):
        return (a - y)
</code></pre>
<p>交叉熵的方程:</p>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180403/G75Ee32DJe.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180403/G75Ee32DJe.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<p>因为在计算出来的数中可能存在无限大和nan值。所以我们通过nan_to_num方法将其进行处理。</p>
<p>网络初始化时，我们可以默认使用CrossEntropyCost这个类</p>
<p>接着我们要将反向更新的代码进行修改:</p>
<pre><code>        # 反向更新了
        # 计算最后一层的误差
        delta = (self.cost).delta(zs[-1], activations[-1], y)
</code></pre>
<p>使用self.cost函数来替换掉我们之前写死的二次损失函数。</p>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180403/g59B6JLghE.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180403/g59B6JLghE.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>

          </div>','1531183734'),
('325581','{4681}{3472}{1059716}{1112082}{975717}{1124366}{1124367}{975714}{975903}{975892}{975951}{975728}{17}{45248}{1706}{1124370}{1124372}{1124374}{327}{761}{408}{1350}{600}{975835}{553}{1193}{3416}{975877}{630}{975727}{3538}{775}{975761}{975721}{569}{36}{6159}{975876}{2915}{975757}{158}{789}{40}{490}{1124378}{2098}{4690}{255}{975768}{975729}','循环神经网络（RNN） 你曾想过文本预测算法是怎么工作的吗？语音识别软件如何辨别我们的声音？在图像分类方面，卷积神经网络像一个神秘的黑盒。而这些事情，都可以用循环神经网络（RNN）实现。循环神经网络很强大，在自然语言处理领域有着相当特别的作用。 为什么如此特别？到目前为止，我们研究的网络，标准神经网络和卷积神经网络都接受固定大小的向量作为输','循环神经网络（RNN）','<div class=\"show-content-free\">
            <p>你曾想过文本预测算法是怎么工作的吗？语音识别软件如何辨别我们的声音？在图像分类方面，卷积神经网络像一个神秘的黑盒。而这些事情，都可以用循环神经网络（RNN）实现。循环神经网络很强大，在自然语言处理领域有着相当特别的作用。</p>
<p>为什么如此特别？到目前为止，我们研究的网络，标准神经网络和卷积神经网络都接受固定大小的向量作为输入，并产生固定大小的向量作为输出。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 475px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 67.86999999999999%;\"></div>
<div class=\"image-view\" data-width=\"1080\" data-height=\"733\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/7803390-7c485d8ec7f20c68\" data-original-width=\"1080\" data-original-height=\"733\" data-original-format=\"image/jpeg\" data-original-filesize=\"144990\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>但是，人类的思考却不是这样的，我们不会丢弃掉所有东西从头开始，而是会应用之前获得的上下文和信息。当你阅读该文章时，你对文字的理解都会基于前面的文字，这说明我们人类是按照顺序思考的——新的输入+之前的经验形成思考。</p>
<p>循环神经网络也是这样的，对输入和输出序列进行操作，并将结果反馈给我们。</p>
<p>RNN的结构</p>
<p>循环神经网络的结构和人工神经网络的结构一样，不过会把输出返回给输入。用T时刻的输出来计算T+1时刻的网络输出。RNN简单结构图示：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 620px; max-height: 427px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 68.87%;\"></div>
<div class=\"image-view\" data-width=\"620\" data-height=\"427\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/7803390-a0c46e5b80734eaa\" data-original-width=\"620\" data-original-height=\"427\" data-original-format=\"image/jpeg\" data-original-filesize=\"9718\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>这就是说输出会被送回到输入，然后在下一个时刻，和下一个输入一同使用。基本的，网络的状态是通过时间向前传播的。我们可以按照时间展开这个结构，如下所示：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 620px; max-height: 427px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 68.87%;\"></div>
<div class=\"image-view\" data-width=\"620\" data-height=\"427\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/7803390-0ac245bff70a51e1\" data-original-width=\"620\" data-original-height=\"427\" data-original-format=\"image/jpeg\" data-original-filesize=\"15220\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>我们可以这样理解，RNN具有“记忆”，里面存储着迄今为止计算出的信息。无论如何，从上面的图片中可以看出我们正在使用某种过程来组合前一时间的输出和当前时刻的输入，以计算当前时刻的输出。</p>
<p>有人可能会想序列输入或序列输出的数据很少，但重要的是要意识到即使输入/输出是固定的向量我们也可以用这种方式处理。例如，在下面的图片中，可以看到RNN如何通过学习逐层向画布添加颜色来生成数字图片：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 240px; max-height: 232px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 96.67%;\"></div>
<div class=\"image-view\" data-width=\"240\" data-height=\"232\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/7803390-8d36563d9b03de08\" data-original-width=\"240\" data-original-height=\"232\" data-original-format=\"image/gif\" data-original-filesize=\"608347\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>RNN背后的数学原理</p>
<p>循环神经网络有一个简单的数学表达式：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 192px; max-height: 57px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 29.69%;\"></div>
<div class=\"image-view\" data-width=\"192\" data-height=\"57\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/7803390-2b6c671d1cb478fd\" data-original-width=\"192\" data-original-height=\"57\" data-original-format=\"image/png\" data-original-filesize=\"1017\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>从本质上讲，该等式表示网络在当前时间ht的状态可以被描述为在前一时间步长中的状态和在当前时间步长中的输入的函数。函数f通常是非线性的，例如tanh或者ReLU。当前时间步长ht中的网络状态成为下一时间步长的输入值。如果采用的是最简单的RNN形式，即使用tanh作为激活函数的RNN，可以表示如下：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 346px; max-height: 58px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 16.76%;\"></div>
<div class=\"image-view\" data-width=\"346\" data-height=\"58\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/7803390-7d9bbfa65eab2f7d\" data-original-width=\"346\" data-original-height=\"58\" data-original-format=\"image/png\" data-original-filesize=\"1715\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>其中，Whh是循环神经元的权重，而Wxh是输入神经元的权重。在这个例子中，考虑的只是前一个时间步长，但是一般来说，是可以观察多个时间步长的状态，这样预测会更加精确。</p>
<p>简化的RNN示例</p>
<p>RNN的数学表达式看起来好像没有想象中那么可怕。如果将数学表达式转化为代码，就能实现一个简单的RNN。这里会用时间步长（timestep）来模拟时间。x代表一个时间步长的输入；y表示RNN的输出。在类内部，将保留有关以前输入和网络状态的信息。C#实现简单RNN的代码：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 512px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 73.15%;\"></div>
<div class=\"image-view\" data-width=\"1080\" data-height=\"790\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/7803390-4b8f07c23f4080a4\" data-original-width=\"1080\" data-original-height=\"790\" data-original-format=\"image/jpeg\" data-original-filesize=\"73646\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>这里注意一点，要使用Matrix类型，需要安装Mathnet。Numerics NuGet软件包。使用tanh函数将激活压缩到[-1,1]的范围。在函数调用中，将当前状态与循环神经元权重相乘，将输入与输入神经元相乘，然后两者相加（同上面的公式），然后将当前状态乘以输出权重来计算输出。</p>
<p>由于Python是实现神经网络的首选语言，下面使用Python来实现：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 430px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 39.81%;\"></div>
<div class=\"image-view\" data-width=\"1080\" data-height=\"430\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/7803390-2102a8cab1fc7851\" data-original-width=\"1080\" data-original-height=\"430\" data-original-format=\"image/jpeg\" data-original-filesize=\"32070\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>当然，上面的都只是简化的循环神经网络表达。这个例子只是让大家感受一下网络的状态是如何随时间保存下来的。</p>
<p>时间反向传播</p>
<p>反向传播算法是神经网络调整权重的一种方法。简而言之，在训练过程中，网络计算训练数据集的输出。然后，将计算结果和所需要的结果进行比较，并调整权重从输出层返回到输入层。更多关于反向传播算法的信息：</p>
<p>https://rubikscode.net/2018/01/22/backpropagation-algorithm-in-artificial-neural-networks/</p>
<p>在循环神经网络中，有一个更复杂的情况。因为，循环神经网络会多一个维度——时间，我们必须根据时间及时更新权重。这就是为什么RNN中的这个过程称为时间反向传播。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 620px; max-height: 427px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 68.87%;\"></div>
<div class=\"image-view\" data-width=\"620\" data-height=\"427\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/7803390-5c0fb5595295b718\" data-original-width=\"620\" data-original-height=\"427\" data-original-format=\"image/jpeg\" data-original-filesize=\"14122\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>上图与展开RNN的表示相同，增加了更多的附加信息。展开的RNN和标准神经网络差别不大，这就是为什么RNN中的反向传播算法和标准神经网络的差别不大。唯一的区别是因为RNN要在不同层之间共享参数，所以我们要将所有时间步长的梯度进行累加。如下图：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 330px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 46.48%;\"></div>
<div class=\"image-view\" data-width=\"710\" data-height=\"330\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/7803390-c8e09e7d127a0581\" data-original-width=\"710\" data-original-height=\"330\" data-original-format=\"image/png\" data-original-filesize=\"28422\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>通常，整个数据序列被认为是一个训练样本。这很大程度上简化了这个问题，因为可以计算每个时间步长的误差，并计算全局误差（所有误差的总和）。可以注意到，各层是相互依赖的，使用随机梯度下降来计算梯度，并将这些信息传递到前一个时刻，并用它来计算误差和梯度，以此类推。这就是如何压缩时间维并使用反向传播算法来调整权重。</p>
<p>总结</p>
<p>循环神经网络是一种非常有用的工具，具有广泛的应用，比如各种语言建模和文本生成器，还有语音识别等。当与卷积神经网络相结合时，可以用来标记图像。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 349px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 34.08%;\"></div>
<div class=\"image-view\" data-width=\"1024\" data-height=\"349\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/7803390-f9dc068b20007384\" data-original-width=\"1024\" data-original-height=\"349\" data-original-format=\"image/jpeg\" data-original-filesize=\"76338\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>但是，循环神经网络有一个问题。在学习长期依赖关系时有困难，就是说目前的循环神经网络不能学到相隔步长太远的依赖关系。例如，在预测单词时，有时候需要更多的上下文信息。这个问题被称为梯度消失，它可以通过特殊类型的循环神经网络——LSTM来解决。后面我们将继续讨论LSTM。</p>
<p>欢迎持续关注我们微信公众号（geetest_jy），还可以添加技术助理微信“geetest1024”微信，一起交流进步！</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 658px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 94.13%;\"></div>
<div class=\"image-view\" data-width=\"750\" data-height=\"706\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/7803390-35123cdd9e10f3f9\" data-original-width=\"750\" data-original-height=\"706\" data-original-format=\"image/jpeg\" data-original-filesize=\"44879\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
          </div>','1531183735'),
('325582','{62}{30}{2149}{131}{975714}{1124379}{1085805}{36}{190}{582}{975728}{975757}{87575}{195}{4153}{74}{2217}{201}{2438}{184}{219}{199}{779}{975741}{975915}{1099}{1031}{761}{1192}{763}{417}{975721}{975940}{1785}{975768}{975878}{975733}{975744}{1277}{616}{6078}{327}{288731}{1015}{7800}{785}{976063}{2875}{1672}{2214}','深度学习中的正则化策略综述（附Python代码） 前 言 数据科学专家面临的最常见问题之一是如何避免过拟合。 你是否遇到过模型在训练数据上表现特别好，却无法预测测试数据的情形？ 或者你在公共排行榜比赛中刚开始名列前茅，但在最终却落后数百个名额？ 那么，你需要阅读这篇文章！ 仅靠避免过拟合就可以提升模型性能。 image 在本文中，你将理解过拟合的概念以及如','深度学习中的正则化策略综述（附Python代码）','<div class=\"show-content-free\">
            <h2><strong>前  言</strong></h2>
<p>数据科学专家面临的最常见问题之一是如何避免<strong>过拟合</strong>。 你是否遇到过模型在训练数据上表现特别好，却无法预测测试数据的情形？ 或者你在公共排行榜比赛中刚开始名列前茅，但在最终却落后数百个名额？ 那么，你需要阅读这篇文章！</p>
<p>仅靠避免过拟合就可以提升模型性能。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 648px; max-height: 591px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 91.2%;\"></div>
<div class=\"image-view\" data-width=\"648\" data-height=\"591\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-0e0a53217d7d9b31\" data-original-width=\"648\" data-original-height=\"591\" data-original-format=\"image/jpeg\" data-original-filesize=\"105883\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>在本文中，你将理解过拟合的概念以及如何采用正规化来克服这一问题。 然后，我将介绍几种不同的正则化技术，并使用Python进行案例研究，以进一步巩固这些概念。</p>
<p><strong>注意：本文假设你已经掌握神经网络及使用keras实现的基本知识。 如果没有，你可以先参考下面的文章：</strong></p>
<ul>
<li><p><a href=\"https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/\" target=\"_blank\" rel=\"nofollow\">Fundamentals of Deep Learning – Starting with Artificial Neural Network</a></p></li>
<li><p><a href=\"https://www.analyticsvidhya.com/blog/2016/10/tutorial-optimizing-neural-networks-using-keras-with-image-recognition-case-study/\" target=\"_blank\" rel=\"nofollow\">Tutorial: Optimizing Neural Networks using Keras (with Image recognition case study)</a></p></li>
</ul>
<h2><strong>什么是正则化？</strong></h2>
<p>在我们讲解之前，先看一下这张图：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 612px; max-height: 161px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 26.31%;\"></div>
<div class=\"image-view\" data-width=\"612\" data-height=\"161\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-ecdda7b0e38422ff\" data-original-width=\"612\" data-original-height=\"161\" data-original-format=\"image/jpeg\" data-original-filesize=\"11899\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>你之前是否看过这个张图？从左到右，模型试图很好地学习训练数据中的细节和噪声，最终导致在未知数据表现不佳。换句话说，在向右移动时，模型的复杂性增加，训练误差减少，但测试误差却不会，如下图所示：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 297px; max-height: 226px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 76.09%;\"></div>
<div class=\"image-view\" data-width=\"297\" data-height=\"226\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-81f5a2e1f589d9b1\" data-original-width=\"297\" data-original-height=\"226\" data-original-format=\"image/jpeg\" data-original-filesize=\"8693\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>如果你已经设计过神经网络模型，那么你知道其比较复杂，这使得它们很容易过拟合。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 604px; max-height: 338px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 55.96%;\"></div>
<div class=\"image-view\" data-width=\"604\" data-height=\"338\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-47c267ff25fb0847\" data-original-width=\"604\" data-original-height=\"338\" data-original-format=\"image/jpeg\" data-original-filesize=\"28566\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>正则化通过对学习算法进行微调以使得该模型更好地泛化，这反过来也改善了模型在未知数据上的表现。</p>
<h2><strong>正则化为什么有助于降低过拟合？</strong></h2>
<p>如下图所示，一个神经网络模型在训练样本上是过拟合的。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 600px; max-height: 263px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 43.830000000000005%;\"></div>
<div class=\"image-view\" data-width=\"600\" data-height=\"263\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-03b075a735e32938\" data-original-width=\"600\" data-original-height=\"263\" data-original-format=\"image/jpeg\" data-original-filesize=\"22835\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>如果你已经学习过机器学习上的正则化概念，你知道它是惩罚系数。在深度学习中，它是惩罚每个节点的权重矩阵。假定我们的正则化系数很大以至于权重矩阵的一部分元素为0。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 583px; max-height: 248px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 42.54%;\"></div>
<div class=\"image-view\" data-width=\"583\" data-height=\"248\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-6546c6494f0ec5af\" data-original-width=\"583\" data-original-height=\"248\" data-original-format=\"image/jpeg\" data-original-filesize=\"23416\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>这将导致最终的模型为一个简单线性网络，并且可能会在训练样本上是欠拟合。因此，如此大的正则化系数是没有用的，我们需要一个合适的正则化系数，它正好得到一个如下图所示的泛化模型：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 225px; max-height: 239px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 106.22%;\"></div>
<div class=\"image-view\" data-width=\"225\" data-height=\"239\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-985091920a600953\" data-original-width=\"225\" data-original-height=\"239\" data-original-format=\"image/jpeg\" data-original-filesize=\"11293\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<h2><strong>深度学习中的正则化策略</strong></h2>
<p>现在我们已经理解正规化如何帮助减少过拟合。为了将正则化应用于深度学习，这里介绍一些不同的正则化技巧。</p>
<ul>
<li><strong>L2 &amp; L1 正则化</strong></li>
</ul>
<p>L1和L2是最常见的正则化方法。它们在损失函数（cost function）中增加一个正则项：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 666px; max-height: 45px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 6.76%;\"></div>
<div class=\"image-view\" data-width=\"666\" data-height=\"45\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-bdc0848f960d5fe0\" data-original-width=\"666\" data-original-height=\"45\" data-original-format=\"image/png\" data-original-filesize=\"16589\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>由于添加了这个正则化项，权重矩阵的值减小，因为它假定具有更小权重矩阵的神经网络导致更简单的模型。 因此，它也会在一定程度上减少过拟合。然而，这个正则化项在L1和L2中是不同的。</p>
<ul>
<li>对于L2：</li>
</ul>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 353px; max-height: 49px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 13.88%;\"></div>
<div class=\"image-view\" data-width=\"353\" data-height=\"49\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-340fe0da8cb35137\" data-original-width=\"353\" data-original-height=\"49\" data-original-format=\"image/png\" data-original-filesize=\"8700\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p></p>
<p></p>
这里，<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 13px; max-height: 19px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 146.15%;\"></div>
<div class=\"image-view\" data-width=\"13\" data-height=\"19\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-6a193551368104c1\" data-original-width=\"13\" data-original-height=\"19\" data-original-format=\"image/png\" data-original-filesize=\"350\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>是正则化参数。它是一个需要优化的超参数。L2正则化又称为权重衰减（weight decay，从梯度下降的角度）因为其导致权重趋向于0（但不全是0）。</p>
<ul>
<li>对于L1：</li>
</ul>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 1px; max-height: 1px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 100.0%;\"></div>
<div class=\"image-view\" data-width=\"1\" data-height=\"1\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-ae3a56c3e1d46568.gif\" data-original-width=\"1\" data-original-height=\"1\" data-original-format=\"image/png\" data-original-filesize=\"70\"></div>
</div>
<div class=\"image-caption\">image.gif</div>
</div>
<p>这里，我们惩罚权重矩阵的绝对值。不同于L2，权重值可能被减少到0.因此，L1对于压缩模型很有用。其它情况下，一般选择优先选择L2正则化。</p>
<p>在<strong>Keras</strong>中，我们使用<a href=\"https://keras.io/regularizers/\" target=\"_blank\" rel=\"nofollow\"><strong>regularizers模块</strong></a>来在某个层上应用L1或者L2正则化。下面是在Dense层应用L2正则化：</p>
<pre><code>from keras import regularizersmodel.add(Dense(64, input_dim=64,kernel_regularizer=regularizers.l2(0.01)
</code></pre>
<p></p>
<p></p>
注意：0.01是前面所说的<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 13px; max-height: 19px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 146.15%;\"></div>
<div class=\"image-view\" data-width=\"13\" data-height=\"19\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-6bc99e5cf28c1370\" data-original-width=\"13\" data-original-height=\"19\" data-original-format=\"image/png\" data-original-filesize=\"350\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>值，是需要进一步优化的超参数，可以使用<strong>[网格搜索方法(grid-search)]</strong>(<a href=\"http://scikit-learn.org/stable/modules/grid_search.html\" target=\"_blank\" rel=\"nofollow\">http://scikit-learn.org/stable/modules/grid_search.html</a>)来优化。</p>
<p>同样地，我们也可以使用L1正则化，在后面的实例中会更详细地讲解。</p>
<h2><strong>Dropout</strong></h2>
<p>这是最有趣的正规化技术之一。它可以实现非常好的结果，因此是深度学习领域中最常用的正则化技术。为了理解dropout，假设我们的神经网络结构类似于下面显示的那样：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 321px; max-height: 315px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 98.13%;\"></div>
<div class=\"image-view\" data-width=\"321\" data-height=\"315\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-a84a790d17ca891f\" data-original-width=\"321\" data-original-height=\"315\" data-original-format=\"image/jpeg\" data-original-filesize=\"21939\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>Dropout的原理很简单：在每个迭代过程中，随机选择某些节点，并且删除前向和后向连接，如下图所示：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 321px; max-height: 321px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 100.0%;\"></div>
<div class=\"image-view\" data-width=\"321\" data-height=\"321\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-d688dd86ccd96afe\" data-original-width=\"321\" data-original-height=\"321\" data-original-format=\"image/jpeg\" data-original-filesize=\"15640\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>因此，每个迭代过程都会有不同的节点组合，从而导致不同的输出。这可以看成机器学习中的集成方法（ensemble technique）。集成模型一般优于单一模型，因为它们可以捕获更多的随机性。相似地，dropout使得神经网络模型优于正常的模型。</p>
<p>选择移除多少节点的概率值是一个超参数。如上图所示，dropout不仅可以应用在隐含层，也可以应用在输入层。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 452px; max-height: 261px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 57.74%;\"></div>
<div class=\"image-view\" data-width=\"452\" data-height=\"261\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-ef06c234fd74d8a4\" data-original-width=\"452\" data-original-height=\"261\" data-original-format=\"image/png\" data-original-filesize=\"13108\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>由于这些原因，当我们具有较大的神经网络时，通常首选dropout以引入更多的随机性。</p>
<p>在Keras中，我们可以使用<strong>[Dropout层]</strong>(<a href=\"https://keras.io/layers/core/#dropout\" target=\"_blank\" rel=\"nofollow\">https://keras.io/layers/core/#dropout</a>)实现dropout，代码如下：</p>
<pre><code>from keras.layers.core import Dropout      
model = Sequential([    
Dense(output_dim=hidden1_num_units, input_dim=input_num_units, activation=\'relu\'),    
Dropout(0.25),      
Dense(output_dim=output_num_units, input_dim=hidden5_num_units, activation=\'softmax\'),    ])
</code></pre>
<p>可以看到，dropout设置的丢弃概率值为0.25，这个值也可以采用网格搜索方法进一步优化。</p>
<h2><strong>数据扩增</strong></h2>
<p>减少过拟合的最简单方法是增加训练样本。在机器学习中，由于标注数据是昂贵的，我们不能够增加训练样本数量。但是对于图像问题，有几种可以增加训练样本的方法-旋转（rotaing）、翻转（flipping）、放缩（scaling）及平移（shfiting）等。下面为在MNIST数据集上的一些图像变换：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 598px; max-height: 154px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 25.75%;\"></div>
<div class=\"image-view\" data-width=\"598\" data-height=\"154\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-b931adda4f2b92d1\" data-original-width=\"598\" data-original-height=\"154\" data-original-format=\"image/png\" data-original-filesize=\"29038\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>这种技术称为数据扩增（data agumentation），这通常可以极大提升模型的准确度。它一般被认为是必须要使用的方法来提升预测值。</p>
<p>在keras中，你可以使用<a href=\"https://keras.io/preprocessing/image/\" target=\"_blank\" rel=\"nofollow\"><strong>ImageDataGenerator</strong></a>来实现上述的图像变换，它有很多参数来控制你预处理训练数据的方式。下面是一些样例代码：</p>
<pre><code>from keras.preprocessing.image import ImageDataGenerator   
datagen = ImageDataGenerator(horizontal_flip=True)   
datagen.fit(train)
</code></pre>
<h2><strong>早期停止</strong></h2>
<p>早期停止（early stopping）是一种交叉验证策略，我们将一部分训练集作为验证集（validation set）。 当我们看到验证集的性能越来越差时，我们立即停止对该模型的训训。 这被称为早期停止。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 309px; max-height: 205px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 66.34%;\"></div>
<div class=\"image-view\" data-width=\"309\" data-height=\"205\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-45fb47e11c6e3088\" data-original-width=\"309\" data-original-height=\"205\" data-original-format=\"image/png\" data-original-filesize=\"10901\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>在上图中，我们在虚线处停止模型的训练，此时模型开始在训练数据上过拟合。</p>
<p>在Keras中，我们可以使用<a href=\"https://keras.io/callbacks/\" target=\"_blank\" rel=\"nofollow\"><strong>callbacks</strong></a>函数实现早期停止，下面是样例代码：</p>
<pre><code>from keras.callbacks import EarlyStopping      
EarlyStopping(monitor=\'val_err\', patience=5)
</code></pre>
<p>上面，<em>monitor</em>参数表示监测量，这里<em>val_err</em>表示验证集误差。而<em>patience</em>参数epochs数量，当在这个过程性能无提升时会停止训练。为了更好地理解，让我们再看看上面的图片。 在虚线之后，每个epoch都会导致更高的验证集误差。 因此，虚线后5个epoch（<em>patience</em>等于5），模型将停止训练，因为没有进一步的改善。</p>
<p><strong>注意</strong>：在5个epochs（这是通常设置的<em>patience</em>值）之后，模型可能会再次开始改善，并且验证集误差也开始减少。 因此，我们需要在调整这个超参数时要多加小心。</p>
<h2><strong>基于Keras的MNIST实例</strong></h2>
<p>至此，你已经对不同的正则化策略有了理论认识。下面，我们将使用这些知识来解决一个深度学习问题-手写字体识别，即MNIST数据集。Keras里面包含该数据集。首先，我们导入一些基本库。</p>
<pre><code>import numpy as np
import matplotlib.pyplot as plt
from keras import Sequential
from keras.layers import Dense, Dropout
from keras.datasets import mnist
from keras.utils import to_categorical
from keras.preprocessing.image import ImageDataGenerator
from keras import regularizers
from keras.callbacks import EarlyStopping

# 避免随机性，可以重复试验
seed = 128
rng = np.random.RandomState(seed)
</code></pre>
<p>然后加载数据集：</p>
<pre><code># 加载数据集   (train_images, train_labels), (test_images, test_labels) = mnist.load_data()
</code></pre>
<p>可视化一些图片：</p>
<pre><code>img_idx = rng.randint(len(train_images))
img = train_images[img_idx]
plt.imshow(img, cmap=\'gray\')
plt.axis(\'off\')
plt.show()
</code></pre>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 210px; max-height: 215px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 102.38000000000001%;\"></div>
<div class=\"image-view\" data-width=\"210\" data-height=\"215\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-b3e2bcd70ec885a8\" data-original-width=\"210\" data-original-height=\"215\" data-original-format=\"image/png\" data-original-filesize=\"717\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>创建验证集以优化模型，这里使得训练集和验证集比率为7:3：</p>
<pre><code>train_images, train_labels = train_images[:50000], train_labels[:50000] # 使用部分数据
train_images = train_images.reshape((-1, 28*28)).astype(np.float32) / 255.0
test_images = test_images.reshape((-1, 28*28)).astype(np.float32) / 255.0
train_labels = to_categorical(train_labels, 10)
test_labels = to_categorical(test_labels, 10)

split_size = int(len(train_images) * 0.7)
x_train, y_train = train_images[:split_size], train_labels[:split_size]
x_val, y_val = train_images[split_size:], train_labels[split_size:]
</code></pre>
<p>首先，我们创建一个包含5个隐含层的简单神经网络，隐含层神经元数为500。</p>
<pre><code># 定义参数
input_num_units = 784
hidden1_num_units = 500
hidden2_num_units = 500
hidden3_num_units = 500
hidden4_num_units = 500
hidden5_num_units = 500
output_num_units = 10

epochs = 10
batch_size = 128

model = Sequential([
 Dense(hidden1_num_units, input_shape=(input_num_units,), activation=\'relu\'),
 Dense(hidden2_num_units, activation=\'relu\'),
 Dense(hidden3_num_units, activation=\'relu\'),
 Dense(hidden4_num_units, activation=\'relu\'),
 Dense(hidden5_num_units, activation=\'relu\'),
 Dense(output_num_units, activation=\'softmax\'),]
  )
</code></pre>
<p>然后我们训练10个epochs，可以看到模型性能：</p>
<pre><code>model.compile(loss=\'categorical_crossentropy\', optimizer=\'adam\', metrics=[\'accuracy\'])

trained_model_5d = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=2, validation_data=(x_val, y_val))
</code></pre>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 607px; max-height: 399px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 65.73%;\"></div>
<div class=\"image-view\" data-width=\"607\" data-height=\"399\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-96c5a189213df7e0\" data-original-width=\"607\" data-original-height=\"399\" data-original-format=\"image/jpeg\" data-original-filesize=\"62620\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>然后，我们加上L2正则化，看看模型是否比原始模型有提升：</p>
<pre><code>reg_w = 1e-4

model = Sequential([
 Dense(hidden1_num_units, input_shape=(input_num_units,), activation=\'relu\', kernel_regularizer=regularizers.l2(reg_w)),
 Dense(hidden2_num_units, activation=\'relu\', kernel_regularizer=regularizers.l2(reg_w)),
 Dense(hidden3_num_units, activation=\'relu\', kernel_regularizer=regularizers.l2(reg_w)),
 Dense(hidden4_num_units, activation=\'relu\', kernel_regularizer=regularizers.l2(reg_w)),
 Dense(hidden5_num_units, activation=\'relu\', kernel_regularizer=regularizers.l2(reg_w)),
 Dense(output_num_units, activation=\'softmax\'),]
  )

model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])

trained_model_5d = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=2, validation_data=(x_val, y_val))
</code></pre>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 607px; max-height: 398px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 65.57%;\"></div>
<div class=\"image-view\" data-width=\"607\" data-height=\"398\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-ac57796c2c7b6e17\" data-original-width=\"607\" data-original-height=\"398\" data-original-format=\"image/jpeg\" data-original-filesize=\"59544\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p></p>
<p></p>
上面我们采用的<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 13px; max-height: 19px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 146.15%;\"></div>
<div class=\"image-view\" data-width=\"13\" data-height=\"19\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-f1aae31ab050997a\" data-original-width=\"13\" data-original-height=\"19\" data-original-format=\"image/png\" data-original-filesize=\"350\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>值为0.0001，训练后模型准确度比原始模型高。</p>
<p>接着，尝试L1正则化：</p>
<pre><code>## l1

reg_w = 1e-4

model = Sequential([
 Dense(hidden1_num_units, input_shape=(input_num_units,), activation=\'relu\', kernel_regularizer=regularizers.l1(reg_w)),
 Dense(hidden2_num_units, activation=\'relu\', kernel_regularizer=regularizers.l1(reg_w)),
 Dense(hidden3_num_units, activation=\'relu\', kernel_regularizer=regularizers.l1(reg_w)),
 Dense(hidden4_num_units, activation=\'relu\', kernel_regularizer=regularizers.l1(reg_w)),
 Dense(hidden5_num_units, activation=\'relu\', kernel_regularizer=regularizers.l1(reg_w)),
 Dense(output_num_units, activation=\'softmax\'),]
  )

model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])

trained_model_5d = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=2, validation_data=(x_val, y_val))
</code></pre>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 611px; max-height: 393px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 64.32%;\"></div>
<div class=\"image-view\" data-width=\"611\" data-height=\"393\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-185f2ae7f455f445\" data-original-width=\"611\" data-original-height=\"393\" data-original-format=\"image/jpeg\" data-original-filesize=\"60972\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>可以看到，使用L1正则化性能无提升。然后尝试dropout策略：</p>
<pre><code>## dropout

from keras.layers.core import Dropout
model = Sequential([
 Dense(output_dim=hidden1_num_units, input_dim=input_num_units, activation=\'relu\'),
 Dropout(0.25),
 Dense(output_dim=hidden2_num_units, input_dim=hidden1_num_units, activation=\'relu\'),
 Dropout(0.25),
 Dense(output_dim=hidden3_num_units, input_dim=hidden2_num_units, activation=\'relu\'),
 Dropout(0.25),
 Dense(output_dim=hidden4_num_units, input_dim=hidden3_num_units, activation=\'relu\'),
 Dropout(0.25),
 Dense(output_dim=hidden5_num_units, input_dim=hidden4_num_units, activation=\'relu\'),
 Dropout(0.25),

Dense(output_dim=output_num_units, input_dim=hidden5_num_units, activation=\'softmax\'),
 ])
model.compile(loss=\'categorical_crossentropy\', optimizer=\'adam\', metrics=[\'accuracy\'])
trained_model_5d = model.fit(x_train, y_train, nb_epoch=epochs, batch_size=batch_size, validation_data=(x_test, y_test))
</code></pre>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 609px; max-height: 402px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 66.01%;\"></div>
<div class=\"image-view\" data-width=\"609\" data-height=\"402\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-e7449481010ca48a\" data-original-width=\"609\" data-original-height=\"402\" data-original-format=\"image/jpeg\" data-original-filesize=\"64524\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>可以看到，dropout使用后模型性能有稍微提升。接着，尝试数据扩增技术：</p>
<pre><code>train_images = np.reshape(train_images, (-1, 28, 28, 1))
datagen = ImageDataGenerator(rotation_range=20)
datagen.fit(train_images, augment=True)
</code></pre>
<p>这里，采用rotation_range参数，它将以一定角度旋转图片，然后训练模型：</p>
<pre><code>model = Sequential([
 Dense(hidden1_num_units, input_shape=(input_num_units,), activation=\'relu\'),
 Dense(hidden2_num_units, activation=\'relu\'),
 Dense(hidden3_num_units, activation=\'relu\'),
 Dense(hidden4_num_units, activation=\'relu\'),
 Dense(hidden5_num_units, activation=\'relu\'),
 Dense(output_num_units, activation=\'softmax\'),]
  )

model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])

for e in range(epochs):
    print(\'Epoch\', e)
    batches = 0
    for x_batch, y_batch in datagen.flow(x_train, y_train, batch_size=batch_size):
        x_batch = np.reshape(x_batch, (-1, 784)) / 255.0
        model.train_on_batch(x_batch, y_batch)
        batches += 1
        if batches &gt;= len(x_train) // batch_size:
         # we need to break the loop by hand because
         # the generator loops indefinitely
         break

    results = model.evaluate(x_val.reshape(-1, 784), y_val, verbose=0, batch_size=batch_size)
    print(results)
</code></pre>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 402px; max-height: 420px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 104.47999999999999%;\"></div>
<div class=\"image-view\" data-width=\"402\" data-height=\"420\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-de1058848c56e553\" data-original-width=\"402\" data-original-height=\"420\" data-original-format=\"image/png\" data-original-filesize=\"8092\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>使用数据扩增之后，性能有提升，你还可以尝试其它数据扩增方式。</p>
<p>最后，我们使用<strong>早期停止</strong>策略：</p>
<pre><code>model = Sequential([
 Dense(hidden1_num_units, input_shape=(input_num_units,), activation=\'relu\'),
 Dense(hidden2_num_units, activation=\'relu\'),
 Dense(hidden3_num_units, activation=\'relu\'),
 Dense(hidden4_num_units, activation=\'relu\'),
 Dense(hidden5_num_units, activation=\'relu\'),
 Dense(output_num_units, activation=\'softmax\'),]
  )
model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])
trained_model_5d = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=2,
                             validation_data=(x_val, y_val), callbacks=[EarlyStopping(monitor=\"val_acc\", patience=2)])
</code></pre>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 607px; max-height: 330px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 54.37%;\"></div>
<div class=\"image-view\" data-width=\"607\" data-height=\"330\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-b84edd81e12b4a1c\" data-original-width=\"607\" data-original-height=\"330\" data-original-format=\"image/jpeg\" data-original-filesize=\"49926\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>此时可以看到训练在第8个epoch就停止了，因为验证集准确度连续2个epochs没有提升。早期停止对训练较大的epochs时比较有效，你可以认为它是对训练的epochs数目进行优化。</p>
<h2><strong>结束语</strong></h2>
<p>我希望现在你对正规化以及在深度学习模型中不同正则化技术有了了解。我强烈建议你在处理深度学习任务时应用它，它将帮助你拓展视野并更好地理解这个技术。</p>
<blockquote>
<p>注：MNIST实例部分相比原文略有改动。</p>
</blockquote>
<hr>
<blockquote>
<p>微信：TonyJeemy520      加个人微信拉你进机器学习、深度学习交流群，请备注 : 来自简书<br>
ML / DL QQ交流群：651616387   请备注 : 来自简书<br>
微信公众号：机器学习算法工程师   ----二维码见下图</p>
</blockquote>
<hr>
<p><strong>扫码关注微信号：机器学习算法工程师，更多干货分享， 或加个人微信，拉你进机器学习、深度学习交流群</strong><br></p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 430px; max-height: 430px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 100.0%;\"></div>
<div class=\"image-view\" data-width=\"430\" data-height=\"430\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-0bdf004ac80c0197\" data-original-width=\"430\" data-original-height=\"430\" data-original-format=\"image/jpeg\" data-original-filesize=\"32906\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p></p>

          </div>','1531183738'),
('325583','{1124384}{36}{62}{2841}{4466}{975761}{975757}{39}{192306}{975785}{408}{785228}{130651}{761}{1124385}{990}{1124386}{975798}{24976}{975910}{74}{569}{1352}{1039}{158}{463}{1124387}{257}{647}{975786}{1483}{594}{1314}{682}{1277}{975707}{975789}{2098}{975892}{2219}{2210}{1651}{3307}{975728}{2074}{976111}{40}{785}','/models/路径下。 baseline模型下载 训练步骤： 运行classifier_train.py即可。 python FeedbackNet_train.py 每10个epoch后，训练好的模型将以. 网络模型参考 @maxspero, 并根据原始论文进行修正 @bzcheeseman_pytorch-feedbacknet','CVPR 2017 Feedback-Network 的 pytorch 实现','<div class=\"show-content-free\">
            <h3>项目地址</h3>
<p><a href=\"https://github.com/LiMeng95/pytorch_feedback-network\" target=\"_blank\" rel=\"nofollow\">我的github地址</a></p>
<h3>目的</h3>
<p>根据 <a href=\"http://feedbacknet.stanford.edu/\" target=\"_blank\" rel=\"nofollow\">Feedback-Network (CVPR 2017, Zamir et al.)</a> 论文提出的反馈网络结构，对CIFAR100或类似数据集进行分类。当前实现了CIFAR100数据集上的训练和测试，基本达到论文效果。</p>
<h3>结果</h3>
<ul>
<li>
<p>Feedback-Network48，CIFAR100验证精度</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 618px; max-height: 80px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 12.94%;\"></div>
<div class=\"image-view\" data-width=\"618\" data-height=\"80\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/5742045-eb1aab0f446bc31c.png\" data-original-width=\"618\" data-original-height=\"80\" data-original-format=\"image/png\" data-original-filesize=\"7682\"></div>
</div>
<div class=\"image-caption\">val accuracy</div>
</div>
</li>
</ul>
<h3>Requirements</h3>
<ul>
<li>Pytorch = 0.3.1</li>
<li>python = 2.7</li>
<li>numpy &gt;= 1.14.2</li>
</ul>
<h3>步骤</h3>
<p>使用Pytorch为工具，实现CIFAR100数据集分类。</p>
<ul>
<li>数据准备：
<ul>
<li>训练：将CIFAR100数据集放在<code>./data/</code>路径下。<a href=\"https://www.cs.toronto.edu/~kriz/cifar.html\" target=\"_blank\" rel=\"nofollow\">CIFAR100数据集下载</a>
</li>
<li>测试：将训练好的模型放在<code>./models/</code>路径下。 <a href=\"https://cloud.tsinghua.edu.cn/f/8b1affe99ba5494ba636/\" target=\"_blank\" rel=\"nofollow\">baseline模型下载</a>
</li>
</ul>
</li>
<li>训练步骤：
<ul>
<li>运行<code>classifier_train.py</code>即可。</li>
</ul>
<pre><code class=\"python\">python FeedbackNet_train.py
</code></pre>
<ul>
<li>每10个epoch后，训练好的模型将以<code>.pth</code>文件的形式保存在<code>./models/</code>文件夹下。</li>
</ul>
</li>
<li>验证步骤：
<ul>
<li>修改<code>classifier_test.py</code>文件相关参数，其中<code>ckpt</code>表示模型加载位置，默认采用CIFAR100数据集中的test数据。</li>
<li>然后运行<code>classifier_test.py</code>即可。在控制台输出验证结果。</li>
</ul>
<pre><code class=\"python\">python FeedbackNet_test.py
</code></pre>
</li>
</ul>
<h3>方法</h3>
<ul>
<li>
<p>FeedbackNet：<br>
以ConvLSTM为基础，实现网络结构。详细解读待更新。</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 645px; max-height: 553px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 85.74000000000001%;\"></div>
<div class=\"image-view\" data-width=\"645\" data-height=\"553\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/5742045-10ef6d8c9e9b33c0.png\" data-original-width=\"645\" data-original-height=\"553\" data-original-format=\"image/png\" data-original-filesize=\"103356\"></div>
</div>
<div class=\"image-caption\">FeedbackNet</div>
</div>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 651px; max-height: 315px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 48.39%;\"></div>
<div class=\"image-view\" data-width=\"651\" data-height=\"315\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/5742045-fe2ba5a8c99c7c27.png\" data-original-width=\"651\" data-original-height=\"315\" data-original-format=\"image/png\" data-original-filesize=\"75743\"></div>
</div>
<div class=\"image-caption\">ConvLSTM with skip connections</div>
</div>
</li>
</ul>
<h3>训练代码流程</h3>
<ol>
<li>Hyper-params: 设置数据加载路径、模型保存路径、初始学习率等参数。</li>
<li>Training parameters: 用于定义模型训练中的相关参数，例如最大迭代次数、优化器、损失函数、是否使用GPU等、模型保存频率等</li>
<li>load data: 定义了用于读取数据，在其中实现了数据、标签读取及预处理过程。预处理过程在<code>__getitem__</code>中。</li>
<li>models: 定义的FeedbackNet类，并实例化</li>
<li>optimizer、criterion、lr_scheduler: 定义优化器为SGD优化器，损失函数为CrossEntropyLoss，学习率调整策略采用ReduceLROnPlateau。</li>
<li>trainer: 定义了用于模型训练和验证的类Trainer，trainer为Trainer的实例化。在Trainer的构造函数中根据步骤二中的参数设定，对训练过程中的参数进行设置，包括训练数据、测试数据、模型、是否使用GPU等。<br>
Trainer中定义了训练和测试函数，分别为<code>train()</code>和<code>_val_one_epoch()</code>。<code>train()</code>函数中，根据设定的最大循环次数进行训练，每次循环调用<code>_train_one_epoch()</code>函数进行单步训练。</li>
</ol>
<h3>测试代码流程</h3>
<ol>
<li>Test parameters: 用于定义模型测试中的相关参数</li>
<li>models: 定义的FeedbackNet类，并实例化</li>
<li>tester: 对测试类Tester实例化，Tester中主要进行模型加载函数与预测函数。<br><code>_load_ckpt()</code>函数加载模型；<br><code>test()</code>函数进行预测，其中定义了对单张图片进行预处理的过程，并输出预测结果。</li>
</ol>
<h3>参考</h3>
<ul>
<li>
<a href=\"https://github.com/amir32002/feedback-networks\" target=\"_blank\" rel=\"nofollow\">@amir32002 feedback-networks</a> : 论文作者，原始 Torch7实现.</li>
<li>网络模型参考 <a href=\"https://github.com/maxspero/feedback-networks-pytorch\" target=\"_blank\" rel=\"nofollow\">@maxspero</a>, 并根据<a href=\"http://feedbacknet.stanford.edu/\" target=\"_blank\" rel=\"nofollow\">原始论文</a>进行修正</li>
<li><a href=\"https://github.com/bzcheeseman/pytorch-feedbacknet\" target=\"_blank\" rel=\"nofollow\">@bzcheeseman_pytorch-feedbacknet</a></li>
</ul>
</div>','1531183738'),
('325584','{39}{76666}{975721}{247}{975794}{36}{26}{975755}{2956}{14928}{1124390}{975714}{975707}{4196}{1124391}{1124392}{975728}{2046}{105714}{921}{65321}{7}{1740}{975865}{463}{1757}{366}{761}{164941}{166412}{65329}{975757}{2193}{1468}{1476}{198152}{975783}{1330}{408}{207}{1124396}{975880}{975715}{975741}{11331}{975785}{975904}{975734}{4916}{834}','text)) assert(xmax  xmin) assert(ymax  ymin) o_width = abs(xmax - xmin) o_height = abs(ymax - ymin) ann = {\'area\': o_width*o_height, \'iscrowd\': 0, \'image_id\': image_id, \'bbox\':[xmin, ymin, o_width, o_height], \'category_id\': category_id, \'id\': bnd_id, \'ignore\': 0, \'segmentation\': []} json_dict[\'annotations\']./detectron-output 我印象中前面某个位置会报错，需要将cityperson中的jpg图片转换为png图片，用下面的指令即可： $ ls -1 *.','Detectron平台下实现Cityperson数据库的使用','<div class=\"show-content-free\">
            <p>Detectron平台做检测的应该都知道，网上关于配置的教程也很多，应该使用的挺多的。该平台对COCO数据集支持良好。<br>
Cityperson数据集，在16年CVPR上被提出，是张姗姗一波人在cityscapes数据集上进行标注得到的行人检测数据集。做行人检测的应该都不陌生。<a href=\"https://link.jianshu.com?t=https%3A%2F%2Farxiv.org%2Fabs%2F1702.05693\" target=\"_blank\" rel=\"nofollow\">论文地址</a>，<a href=\"https://link.jianshu.com?t=https%3A%2F%2Fwww.cityscapes-dataset.com%2F\" target=\"_blank\" rel=\"nofollow\">数据库地址</a>，<a href=\"https://link.jianshu.com?t=https%3A%2F%2Fbitbucket.org%2Fshanshanzhang%2Fcitypersons\" target=\"_blank\" rel=\"nofollow\">张姗姗提供的数据库地址</a><br>
这篇文章中，我将详细介绍如何在Detectron平台下实现Cityperson数据集的训练和测试。</p>
<p>首先下载Cityperson数据集，网站上提供了很多的下载选项，下载需要注册。对于行人检测任务，下载这两个文件就行了：</p>
<ol>
<li><a href=\"https://link.jianshu.com?t=https%3A%2F%2Fwww.cityscapes-dataset.com%2Ffile-handling%2F%3FpackageID%3D3\" target=\"_blank\" rel=\"nofollow\">train+val图片</a></li>
<li>
<a href=\"https://link.jianshu.com?t=https%3A%2F%2Fwww.cityscapes-dataset.com%2Ffile-handling%2F%3FpackageID%3D28\" target=\"_blank\" rel=\"nofollow\">cityperson标注</a><br>
需要说明的是，目前cityperson只公布了训练和验证数据集，没有公布测试数据。希望尽早公布吧。</li>
</ol>
<p>接下来要做两个工作，一个是数据库的转换，另外一个是detectron平台相应代码的修改。<br>
首先说数据库的转换吧：</p>
<p>这里我是将cityperson数据集转换成coco集之后用来训练的。虽然detectron平台对citysapes数据集有支持，但是似乎不是行人检测这一块的。没有过多的研究，如果有同样在做这类似工作的小伙伴，欢迎通过各种方法联系我，因为我确实也没弄得太清楚。</p>
<p>cityperson数据集的标注文件，是每一个图片对应一个标注json文件，而coco的标注格式，可以参考<a href=\"https://link.jianshu.com?t=http%3A%2F%2Fcocodataset.org%2F\" target=\"_blank\" rel=\"nofollow\">官网</a>上的说明，需要强调的就是他是整个集对应一个json文件，每一个图片对应一个唯一的图片编号。</p>
<p>因为不能看到coco集下具体的形式，为了快速解决这个问题，我决定用现有的代码实现。在github上面搜索了一下做数据库转换的代码。发现只有这个：<a href=\"https://link.jianshu.com?t=https%3A%2F%2Fgithub.com%2FMicroos%2Fcitypersons2voc\" target=\"_blank\" rel=\"nofollow\">cityperson2voc</a>，拿下来试了一下，确实可以用。具体的使用方法参照代码中的说明即可。现在我们得到了一个voc形式的cityperson数据集。其实detectron平台对voc也支持，但是我看了一下代码。他支持的voc数据集仍然需要一个总体的json标注文件，而不是传统的voc那样每一个图片对应的xml文件。这时需要使用voc到coco的转换工具了，这样的转换工具，github上也有现成的，可以拿来直接用。不过对于转换过来的cityperson数据集，需要对代码进行一点点修改：<br><a href=\"https://link.jianshu.com?t=https%3A%2F%2Fgithub.com%2Fshiyemin%2Fvoc2coco\" target=\"_blank\" rel=\"nofollow\">代码地址</a><br>
修改部分如下面注释;</p>
<pre><code class=\"python\">#!/usr/bin/python

# pip install lxml

import sys
import os
import json
import xml.etree.ElementTree as ET


START_BOUNDING_BOX_ID = 1
PRE_DEFINE_CATEGORIES = {}
# If necessary, pre-define category and its id
#  PRE_DEFINE_CATEGORIES = {\"aeroplane\": 1, \"bicycle\": 2, \"bird\": 3, \"boat\": 4,
                         #  \"bottle\":5, \"bus\": 6, \"car\": 7, \"cat\": 8, \"chair\": 9,
                         #  \"cow\": 10, \"diningtable\": 11, \"dog\": 12, \"horse\": 13,
                         #  \"motorbike\": 14, \"person\": 15, \"pottedplant\": 16,
                         #  \"sheep\": 17, \"sofa\": 18, \"train\": 19, \"tvmonitor\": 20}


def get(root, name):
    vars = root.findall(name)
    return vars


def get_and_check(root, name, length):
    vars = root.findall(name)
    if len(vars) == 0:
        raise NotImplementedError(\'Can not find %s in %s.\'%(name, root.tag))
    if length &gt; 0 and len(vars) != length:
        raise NotImplementedError(\'The size of %s is supposed to be %d, but is %d.\'%(name, length, len(vars)))
    if length == 1:
        vars = vars[0]
    return vars


def get_filename_as_int(filename):
    try:
        filename = os.path.splitext(filename)[0]
######################这里加一行###########################
        filename = filename.split(\'_\')[1] + filename.split(\'_\')[2]
        return int(filename)
    except:
        raise NotImplementedError(\'Filename %s is supposed to be an integer.\'%(filename))


def convert(xml_list, xml_dir, json_file):
    list_fp = open(xml_list, \'r\')
    json_dict = {\"images\":[], \"type\": \"instances\", \"annotations\": [],
                 \"categories\": []}
    categories = PRE_DEFINE_CATEGORIES
    bnd_id = START_BOUNDING_BOX_ID
    for line in list_fp:
        line = line.strip()
        print(\"Processing %s\"%(line))
        xml_f = os.path.join(xml_dir, line)
        tree = ET.parse(xml_f)
        root = tree.getroot()
        path = get(root, \'path\')
        if len(path) == 1:
            filename = os.path.basename(path[0].text)
        elif len(path) == 0:
            filename = get_and_check(root, \'filename\', 1).text
        else:
            raise NotImplementedError(\'%d paths found in %s\'%(len(path), line))
        ## The filename must be a number
        image_id = get_filename_as_int(filename)
        size = get_and_check(root, \'size\', 1)
        width = int(get_and_check(size, \'width\', 1).text)
        height = int(get_and_check(size, \'height\', 1).text)
        image = {\'file_name\': filename, \'height\': height, \'width\': width,
                 \'id\':image_id}
        json_dict[\'images\'].append(image)
        ## Cruuently we do not support segmentation
        #  segmented = get_and_check(root, \'segmented\', 1).text
        #  assert segmented == \'0\'
        for obj in get(root, \'object\'):
            category = get_and_check(obj, \'name\', 1).text
            if category not in categories:
                new_id = len(categories)
                categories[category] = new_id
            category_id = categories[category]
            bndbox = get_and_check(obj, \'bndbox\', 1)
########################下面部分改成######################
            #xmin = int(get_and_check(bndbox, \'xmin\', 1).text) - 1
            #ymin = int(get_and_check(bndbox, \'ymin\', 1).text) - 1
            #xmax = int(get_and_check(bndbox, \'xmax\', 1).text)
            #ymax = int(get_and_check(bndbox, \'ymax\', 1).text)
            xmin = int(float(get_and_check(bndbox, \'xmin\', 1).text)) - 1
            ymin = int(float(get_and_check(bndbox, \'ymin\', 1).text)) - 1
            xmax = int(float(get_and_check(bndbox, \'xmax\', 1).text))
            ymax = int(float(get_and_check(bndbox, \'ymax\', 1).text))
            assert(xmax &gt; xmin)
            assert(ymax &gt; ymin)
            o_width = abs(xmax - xmin)
            o_height = abs(ymax - ymin)
            ann = {\'area\': o_width*o_height, \'iscrowd\': 0, \'image_id\':
                   image_id, \'bbox\':[xmin, ymin, o_width, o_height],
                   \'category_id\': category_id, \'id\': bnd_id, \'ignore\': 0,
                   \'segmentation\': []}
            json_dict[\'annotations\'].append(ann)
            bnd_id = bnd_id + 1

    for cate, cid in categories.items():
        cat = {\'supercategory\': \'none\', \'id\': cid, \'name\': cate}
        json_dict[\'categories\'].append(cat)
    json_fp = open(json_file, \'w\')
    json_str = json.dumps(json_dict)
    json_fp.write(json_str)
    json_fp.close()
    list_fp.close()


if __name__ == \'__main__\':
    if len(sys.argv) &lt;= 1:
        print(\'3 auguments are need.\')
        print(\'Usage: %s XML_LIST.txt XML_DIR OUTPU_JSON.json\'%(sys.argv[0]))
        exit(1)

    convert(sys.argv[1], sys.argv[2], sys.argv[3])
</code></pre>
<p>理由说明：<br>
修改1：原代码针对的是voc数据库，读取了图片文件名的编号作为coco集下图片的id，但是对于cityperson数据集，文件名的格式和voc不同，所以需要稍作处理。<br>
修改2：cityperson和voc中bbox的数据格式不同，在这里会报数据转换的错，先将其转换为float再转为int可以快速解决问题。</p>
<p><em>运行该代码还需要提供一个xml文件的目录txt，很多方法实现了。如果不想写程序的话，去imageset文件下找到train和test文件的目录txt，使用文本编辑器替换的方式，再每一个项目末尾加上.xml即可。</em></p>
<p>至此，cityperson数据集的转换工作已经完成。将其链接到Detectron文件夹下的detectron/datasets/data下，我们开始修改Detectron平台的代码，这里参照了博客<a href=\"https://link.jianshu.com?t=https%3A%2F%2Fblog.csdn.net%2Fzziahgf%2Farticle%2Fdetails%2F79488025\" target=\"_blank\" rel=\"nofollow\">Caffe2 - (十九) 基于 Detectron 的 DeepFashion 服装 bbox 检测实现</a></p>
<p>首先修改detectron/datasets/dataset_catalog.py文件，用于添加新的数据集：<br>
按照之前的格式，在DATASETS中增加两项：</p>
<pre><code class=\"python\">    \'cityperson_train\': {
        IM_DIR:
            _DATA_DIR + \'/cityperson/data/JPEGImages\',
        ANN_FN:
            _DATA_DIR + \'/cityperson/data/cityperson_train.json\'
    },
    \'cityperson_val\': {
        IM_DIR:
            _DATA_DIR + \'/cityperson/data/JPEGImages\',
        ANN_FN:
            _DATA_DIR + \'/cityperson/data/cityperson_val.json\'
    },
    },
</code></pre>
<p>然后修改网络文件：<br>
configs/getting_started/tutorial_1gpu_e2e_faster_rcnn_R-50-FPN.yaml<br>
只需要修改三项：</p>
<pre><code class=\"yaml\">NUM_CLASSES: 2 # 一个类别 + 一个background 类
TRAIN：
DATASETS: (\'cityperson_train\',)
 TEST：
DATASETS: (\'cityperson_val\',)
</code></pre>
<p>这时应该已经可以开始训练了，</p>
<pre><code class=\"bash\">python tools/train_net.py --cfg ./configs/getting_started/tutorial_1gpu_e2e_faster_rcnn_R-50-FPN.yaml OUTPUT_DIR ./detectron-output
</code></pre>
<p>我印象中前面某个位置会报错，需要将cityperson中的jpg图片转换为png图片，用下面的指令即可：</p>
<pre><code class=\"bash\">$ ls -1 *.jpg | xargs -n 1 bash -c \'convert \"$0\" \"${0%.jpg}.png\"\'
</code></pre>
<p>在1080ti下单张卡训练大概是两个小时，不得不感叹相比与caffe，caffe2很快了。<br>
训练完成后，会报没有验证函数的错。对啊，就是没有。这一块我目前正在解决，不过可以使用coco默认的测试函数算一下，虽然和行人检测中常用的MR-FPPI不相同，不过也可以反应一下训练效果。不过从我的实验来看，训练算成功了，但是效果不太理想。接下来会去找问题。也欢迎大家和我讨论。<br>
将验证函数默认为coco，需要修改detectron/core/config.py下的项</p>
<pre><code class=\"python\">_C.TEST.FORCE_JSON_DATASET-EVAL = Ture
</code></pre>
<p>当然你也可以和demo一样，可视化一下检测结果：<br>
修改tools/infer_simple.py</p>
<pre><code>dummy_coco_dataset = dummy_datasets.get_cityperson_dataset()
</code></pre>
<p>然后在detectron/datasets/dummy_datasets.py中增加函数：</p>
<pre><code class=\"python\">def get_cityperson_dataset():
    ds = AttrDict()
    classes = [\'__background__\', \'ped\',  ]
    ds.classes = {i: name for i, name in enumerate(classes)}
    return ds
</code></pre>
<p>然后仿照demo运行infer_simple.py就行了。</p>

          </div>','1531183740'),
('325585','{2956}{20255}{952}{975843}{1976}{975757}{1124409}{975707}{8287}{3539}{975865}{976051}{72}{105714}{366}{23456}{975812}{975878}{975899}{1747}{975755}{975837}{1124411}{211301}{304}{11925}{899}{41405}{83849}{1124413}{40}{1124415}{1124416}{1124417}{1124418}{180402}{1124419}{215}{1124421}{750}{775}{838}{2046}{3779}{207}{14928}{8086}{976094}{1410}','#此处省略七个license .format(dataDir,dataType) coco=COCO(annFile) cats = coco.loadCats(coco.getCatIds()) #这里loadCats就是coco提供的接口，获取类别 nms=[cat[\'name\'] for cat in cats] print(\'COCO categories: \\n{}\\n\'.join(nms))) nms = set([cat[\'supercategory\'] for cat in cats]) #cat[\'supercategory\']，从这里可以看出来，cat是一个包含多个属性的数组\\说是字典更好 print(\'COCO supercategories: \\n{}\'.','如何获取COCO的检测或分割结果。MS-COCO共有哪些类，有哪些标注。今天来看一下MS-COCO数','<div class=\"show-content-free\">
            <p><a href=\"http://cocodataset.org/#download\" target=\"_blank\" rel=\"nofollow\">http://cocodataset.org/#download</a> 官网地址<br>
本文的目的是获取所有图像的分割结果并保存的工作。</p>
<p>Mask API 中介绍<br>
COCO为每个目标实例都提供了分割Msak，instance_train201X.json表示的是整个数据集的结构,下面这部分主要介绍他有哪些数据以及其数据类型</p>
<pre><code>{
 \"info\" : info,
 \"images\" : [image],
 \"annotations\" : [annotation],
 \"licenses\" : [license],
}

info{
\"year\" : int,
 \"version\" : str,
 \"description\" : str,
 \"contributor\" : str,
 \"url\" : str,
 \"date_created\" : datetime,
}

image{
\"id\" : int,
 \"width\" : int,
 \"height\" : int,
 \"file_name\" : str,
 \"license\" : int,
 \"flickr_url\" : str,
 \"coco_url\" : str,
 \"date_captured\" : datetime,
}

license{
\"id\" : int,
 \"name\" : str,
 \"url\" : str,
}
</code></pre>
<p>刚介绍完他的数据类型，现介绍下具体到json文件中的每一部分的具体形式<br>
解析下instance_train2014.json文件，最后我们讲一下如何获取并展示分割结果。</p>
<pre><code>{
     \"info\":      #第一个info信息
          {       #数据集信息
                  \"description\": \"COCO 2014 Dataset\",
                  \"url\": \"http://cocodataset.org\",
                  \"version\": \"1.0\",
                  \"year\": 2014,
                  \"contributor\": \"COCO Consortium\",
                  \"date_created\": \"2017/09/01\"
         },



      \"images\":  #第二个图片信息，数组包含了多张图像
      [   {      #每张图像的具体信息
                  \"license\": 5,
                  \"file_name\": \"COCO_train2014_000000057870.jpg\",
                  \"coco_url\": \"http://images.cocodataset.org/train2014/COCO_train2014_000000057870.jpg\",
                  \"height\": 480,
                  \"width\": 640,
                  \"date_captured\": \"2013-11-14 16:28:13\",
                  \"flickr_url\": \"http://farm4.staticflickr.com/3153/2970773875_164f0c0b83_z.jpg\",
                  \"id\": 57870
           },
          ......
          ......   #此处省略很多图片
         {
                  \"license\": 4,
                  \"file_name\": \"COCO_train2014_000000475546.jpg\",
                  \"http://images.cocodataset.org/train2014/COCO_train2014_000000475546.jpg\",
                  \"height\": 375,
                  \"width\":500,；、
                  \"date_captured\": \"2013-11-25 21:20:23\",
                  \"flickr_url\": \"http://farm1.staticflickr.com/167/423175046_6cd9d0205a_z.jpg\",
                  \"id\": 475546
           }]，         #图像描述结束，下面开始介绍licenses


    \"licenses\":
         [ {
                  \"url\": \"http://creativecommons.org/licenses/by-nc-sa/2.0/\",
                  \"id\": 1,
                  \"name\": \"Attribution-NonCommercial-ShareAlike License\"
           },
            .....#此处省略七个license
            .....
         {
                  \"url\": \"http://creativecommons.org/licenses/by-nc-nd/2.0/\",
                  \"id\": 8,
                  \"name\": \"Attribution-NonCommercial-NoDerivs License\"
         }],



      \"annotations\":
      [   { 
#如果你想了解这个annotations中segment里面是什么，首先它是通过压缩处理后的分割区域的一个表示，TFRecord

                 \"segmentation\":[[312.29,562.89,402.25,511.49,400.96,425.38,398.39,372.69,
                                  388.11,332.85,318.71,325.14,295.58,305.86,269.88,314.86,
                                  258.31,337.99,217.19,321.29,182.49,343.13,141.37,348.27,
                                  132.37,358.55,159.36,377.83,116.95,421.53,167.07,499.92,
                                  232.61,560.32,300.72,571.89]],
                \"area\": 54652.9556,
                \"iscrowd\": 0,
                \"image_id\": 480023,
                \"bbox\": [116.95,305.86,285.3,266.03],
                \"category_id\": 58,\"id\": 86
          },
            .....#此处省略很多图像的分割标签
            .....
                \"segmentation\":[[312.29,562.89,402.25,511.49,400.96,425.38,398.39,372.69,
                                388.11,332.85,318.71,325.14,295.58,305.86,269.88,314.86,
                                258.31,337.99,217.19,321.29,182.49,343.13,141.37,348.27,
                                132.37,358.55,159.36,377.83,116.95,421.53,167.07,499.92,
                                232.61,560.32,300.72,571.89]],
              \"area\": 54652.9556,
              \"iscrowd\": 0,
              \"image_id\": 480023,
              \"bbox\": [116.95,305.86,285.3,266.03],
              \"category_id\": 58,
              \"id\": 86
          },


      \"categories\":#类别信息
     [   {
              \"supercategory\": \"person\",
              \"id\": 1,
              \"name\": \"person\"
          },
              .......#此处省略很多图像的类标签
              .......
          {
              \"supercategory\": \"vehicle\",
              \"id\": 2,
              \"name\": \"bicycle\"
          },
        {
              \"supercategory\": \"kitchen\",#大类
              \"id\": 50,
              \"name\": \"spoon\"
        }
</code></pre>
<p>首先，下载COCOAPI中的pythonAPI到coco文件夹中</p>
<pre><code>git clone https://github.com/cocodataset/cocoapi.git
</code></pre>
<p>然后，cd到pythonApi下，执行make，可能会出现下面情况</p>
<pre><code>cd coco/PythonAPI
make
#错误提示pycocotools/_mask.c：没有那个文件或目录
pip install cython  #解决方式
make#再次执行make，如果你没有出现上面的错误，可以跳过
#接下来，验证cocoApi是否安装成功
python
&gt;&gt;&gt;import pycocotools
#不报错就成功了一半了
</code></pre>
<p>接下来，在coco文件夹，下载好image和anotation。</p>
<p>这里介绍我们的分割结果图获取方法</p>
<p>官方给的使用示例在下载目录下的pycocoDemo.ipynb文件下<a href=\"https://github.com/dengdan/coco/blob/master/PythonAPI/pycocoDemo.ipynb\" target=\"_blank\" rel=\"nofollow\">https://github.com/dengdan/coco/blob/master/PythonAPI/pycocoDemo.ipynb</a></p>
<p>那么我们首先了解一下，之前为什么要引入pycocotools，因为这个问件下包含了对coco数据的json文件的解析工具，他定义了coco.py这个文件，中包含一下几个接口。</p>
<pre><code>#  decodeMask - Decode binary mask M encoded via run-length encoding.
#  encodeMask - Encode binary mask M using run-length encoding.
#  getAnnIds  - Get ann ids that satisfy given filter conditions.
#  getCatIds  - Get cat ids that satisfy given filter conditions.
#  getImgIds  - Get img ids that satisfy given filter conditions.
#  loadAnns   - Load anns with the specified ids.
#  loadCats   - Load cats with the specified ids.
#  loadImgs   - Load imgs with the specified ids.
#  annToMask  - Convert segmentation in an annotation to binary mask.
#  showAnns   - Display the specified annotations.
#  loadRes    - Load algorithm results and create API for accessing them.
#  download   - Download COCO images from mscoco.org server.
# Throughout the API \"ann\"=annotation, \"cat\"=category, and \"img\"=image.
</code></pre>
<p>------        首先，我们获取COCO数据集中共有多少类，需要在pythonAPI下新建一个python文件，命名为segcoco.py用于获取分割图,先执行下面这段话，获取下COCO中共有多少类别</p>
<pre><code>import numpy as np
import skimage as io
import matplotlib as mpl
mpl.use(\'Agg\')
#这里为了防止linux没有GUI报错
import matplotlib.pyplot as plt
import pylab
import urllib
import numpy as np
from io import BytesIO
import requests as req
from PIL import Image

pylab.rcParams[\'figure.figsize\'] = (8.0, 10.0)

dataDir=\'..\'
dataType=\'val2014\'  #这里改为train2017的话，类别是相同的
annFile=\'{}/annotations/instances_{}.json\'.format(dataDir,dataType)

coco=COCO(annFile)


cats = coco.loadCats(coco.getCatIds())
#这里loadCats就是coco提供的接口，获取类别
nms=[cat[\'name\'] for cat in cats]
print(\'COCO categories: \\n{}\\n\'.format(\' \'.join(nms)))

nms = set([cat[\'supercategory\'] for cat in cats])
#cat[\'supercategory\']，从这里可以看出来，cat是一个包含多个属性的数组\\说是字典更好
print(\'COCO supercategories: \\n{}\'.format(\' \'.join(nms)))

</code></pre>
<p>显示</p>
<pre><code>loading annotations into memory...
Done (t=5.08s)
creating index...
index created!
COCO categories: 
person bicycle car motorcycle airplane bus train truck boat traffic light fire hydrant stop sign
 parking meter bench bird cat dog horse sheep cow elephant bear zebra giraffe backpack 
umbrella handbag tie suitcase frisbee skis snowboard sports ball kite baseball bat baseball glove 
skateboard surfboard tennis racket bottle wine glass cup fork knife spoon bowl banana apple 
sandwich orange broccoli carrot hot dog pizza donut cake chair couch potted plant bed dining 
table toilet tv laptop mouse remote keyboard cell phone microwave oven toaster sink refrigerator 
book clock vase scissors teddy bear hair drier toothbrush



COCO supercategories: 
outdoor food indoor appliance sports person animal vehicle furniture accessory electronic kitchen


</code></pre>
<p>接下来，获取分割图，按照github的示例，我们在之前代码的基础上，添加一下代码</p>
<pre><code>imgIds = coco.getImgIds(imgIds=[324158])
img = coco.loadImgs(imgIds[np.random.randint(0, len(imgIds))])[0]
print(img)
print(img[\'flickr_url\'])

response = req.get(img[\'flickr_url\'])
#这里跟github中不一样，通过request来获取的图像url来得到图像的，因为发现如果使用coco_url会下载不了图片，可能跟外网有关
image = Image.open(BytesIO(response.content))

plt.imshow(image)
#在这里，如果在linux服务器上，由于没有GUI,会导致错误,所以， 在前面导包的时候加了import matplotlib as mpl
mpl.use(\'Agg\')
plt.axis(\'off\')

annIds = coco.getAnnIds(imgIds=img[\'id\'])
anns = coco.loadAnns(annIds)
#print(anns)
ax=coco.showAnns(anns)
#如果这里提示了TKL的错误
</code></pre>
<p>注意，在linux服务器上，由于没有GUI,除非你有Xmanager，否则会导致错误TKL...，所以， 在coco.py前面导包的时候加了<br><code>import matplotlib as mpl</code><br><code>mpl.use(\'Agg\')</code><br>
如果在没有GUI的情况下，是在想看的话，那你就在coco.py的showAnns(ans)这个方法后面，加上plt.save(\'起个名.jpg\')，再次执行，就能够看到这个图的分割结果了。<br></p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 409px; max-height: 275px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 67.24%;\"></div>
<div class=\"image-view\" data-width=\"409\" data-height=\"275\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/5193446-891cd857a7e14468.png\" data-original-width=\"409\" data-original-height=\"275\" data-original-format=\"image/png\" data-original-filesize=\"269461\"></div>
</div>
<div class=\"image-caption\">image.png</div>
</div>
<p></p>

          </div>','1531183741'),
('325586','{6906}{553}{62}{186929}{1124440}{975714}{1124441}{1076}{975892}{975717}{5664}{197435}{975728}{394}{14294}{1124442}{31365}{3426}{2376}{975757}{36}{9879}{884}{7303}{761}{22684}{975877}{39019}{9086}{9136}{197434}{1124443}{4981}{1124444}{1124445}{1124446}{190}{5380}{2430}{1277}{975768}{975721}{2341}{4056}{975838}{247}{1706}{105957}{975761}{775}','化:所有的代码都使用标准的Python文档,yn) Encoder：是将输入序列通过非线性变换编码成一个指定长度的向量C（中间语义表示），得到c有多种方式，最简单的方法就是把Encoder的最后一个隐状态赋值给c，还可以对最后的隐状态做一个变换得到c，也可以对所有的隐状态做变换。 C = F(x1,x2,.,xm) Decoder：是根据向量C（encoder的输出结果）和之前生成的历史信息y1,y2,.','机器翻译不可不知的Seq2Seq模型','<div class=\"show-content-free\">
            <h2><strong>前  言</strong></h2>
<p>Seq2Seq，全称Sequence to Sequence。它是一种通用的编码器——解码器框架，可用于机器翻译、文本摘要、会话建模、图像字幕等场景中。Seq2Seq并不是GNMT（Google Neural Machine Translation）系统的官方开源实现。框架的目的是去完成更广泛的任务，而神经机器翻译只是其中之一。在循环神经网络中我们了解到如何将一个序列转化成定长输出。在本文中，我们将探究如何将一个序列转化成一个不定长的序列输出（如机器翻译中，源语言和目标语言的句子往往并没有相同的长度）。</p>
<h2><strong>简单入门</strong></h2>
<p><strong>（1）****设计目标</strong></p>
<ul>
<li><p>通用<br>
这个框架最初是为了机器翻译构建的，但是后来使用它完成了各种其他任务，包括文本摘要、会话建模和图像字幕。只要我们的任务，可以将输入数据以一种格式编码并将其以另一种格式解码，我们就可以使用或者扩展这个框架。</p></li>
<li><p>可用性<br>
支持多种类型的输入数据，包括标准的原始文本。</p></li>
<li><p>重现性<br>
用YAML文件来配置我们的pipelines和models，容易复现。</p></li>
<li><p>可扩展性<br>
代码以模块化的方式构建，添加一种新的attention机制或编码器体系结构只需要最小的代码更改。</p></li>
<li><p>文档化:所有的代码都使用标准的Python文档字符串来记录，并且编写了使用指南来帮助我们着手执行常见的任务。</p></li>
<li><p>良好的性能:为了代码的简单性，开发团队并没有试图去尽力压榨每一处可能被拓展的性能，但是对于几乎所有的生产和研究项目，当前的实现已经足够快了。此外，tf-seq2seq还支持分布式训练。</p></li>
</ul>
<p><strong>（2）****主要概念</strong></p>
<ul>
<li>Configuration</li>
</ul>
<p>许多objects都是使用键值对来配置的。这些参数通常以YAML的形式通过配置文件传递，或者直接通过命令行传递。配置通常是嵌套的，如下例所示:</p>
<pre><code>model_params:    attention.class:                                                       seq2seq.decoders.attention.AttentionLayerBahdanau    attention.params:        num_units: 512        embedding.dim: 1024        encoder.class: seq2seq.encoders.BidirectionalRNNEncoder    encoder.params:        rnn_cell:        cell_class: LSTMCell    cell_params:        num_units: 512
</code></pre>
<ul>
<li>Input Pipeline</li>
</ul>
<p>InputPipeline定义了如何读取、解析数据并将数据分隔成特征和标签。如果您想要读取新的数据格式，我们需要实现自己的输入管道。</p>
<ul>
<li><p>Encoder(编码)</p></li>
<li><p>Decoder（解码）</p></li>
<li><p>Model（Attention）</p></li>
</ul>
<h2><strong>Encoder-Decoder</strong></h2>
<p>整个过程可以用下面这张图来诠释：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 233px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 32.36%;\"></div>
<div class=\"image-view\" data-width=\"720\" data-height=\"233\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-c5a6f3a0e836ee1c\" data-original-width=\"720\" data-original-height=\"233\" data-original-format=\"image/jpeg\" data-original-filesize=\"13840\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>图 1：最简单的Encoder-Decoder模型</p>
<p>其中，X、Y均由各自的单词序列组成（X,Y可以是同一种语言，也可以是两种不同的语言）：</p>
<p>X = (x1,x2,...,xm)</p>
<p>Y = (y1,y2,...,yn)</p>
<p>Encoder：是将输入序列通过非线性变换编码成一个指定长度的向量C（中间语义表示），得到c有多种方式，最简单的方法就是把Encoder的最后一个隐状态赋值给c，还可以对最后的隐状态做一个变换得到c，也可以对所有的隐状态做变换。</p>
<p>C = F(x1,x2,...,xm)</p>
<p>Decoder：是根据向量C（encoder的输出结果）和之前生成的历史信息y1,y2,...,yn来生成i时刻要生成的单词yi。</p>
<p>yi = G( C , y1,y2,...,yn-1)</p>
<p>下图是一个生成对联的示意图。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 211px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 29.310000000000002%;\"></div>
<div class=\"image-view\" data-width=\"720\" data-height=\"211\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-f8de641c249d39a2\" data-original-width=\"720\" data-original-height=\"211\" data-original-format=\"image/jpeg\" data-original-filesize=\"17359\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>图 2：生活中的小栗子</p>
<ul>
<li><strong>编码阶段</strong></li>
</ul>
<p></p>
<p></p>
在RNN中，当前时间的隐藏状态由上一时间的状态和当前时间输入决定的，即： <div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 192px; max-height: 45px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 23.44%;\"></div>
<div class=\"image-view\" data-width=\"192\" data-height=\"45\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-da87b446cc2006a0\" data-original-width=\"192\" data-original-height=\"45\" data-original-format=\"image/png\" data-original-filesize=\"6313\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p></p>
<p></p>
获得了各个时间段的隐藏层以后，再将隐藏层的信息汇总，生成最后的语义向量 <div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 255px; max-height: 41px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 16.08%;\"></div>
<div class=\"image-view\" data-width=\"255\" data-height=\"41\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-cc3385d28cc8d802\" data-original-width=\"255\" data-original-height=\"41\" data-original-format=\"image/png\" data-original-filesize=\"10695\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p></p>
<p></p>
当然，有一种最简单的方法是将最后的隐藏层作为语义向量C，即 <div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 304px; max-height: 38px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 12.5%;\"></div>
<div class=\"image-view\" data-width=\"304\" data-height=\"38\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-315b9ed43ab91cac\" data-original-width=\"304\" data-original-height=\"38\" data-original-format=\"image/png\" data-original-filesize=\"13882\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<ul>
<li><strong>解码阶段</strong></li>
</ul>
<p></p>
<p></p>
可以看做编码的逆过程。这个阶段，我们根据给定的语义向量C和之前已经生成的输出序列y1,y2,...,yt-1来预测下一个输出的单词yt，即 <div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 428px; max-height: 63px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 14.719999999999999%;\"></div>
<div class=\"image-view\" data-width=\"428\" data-height=\"63\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-f1286030ca85280e\" data-original-width=\"428\" data-original-height=\"63\" data-original-format=\"image/png\" data-original-filesize=\"23575\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>也可以写作</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 237px; max-height: 38px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 16.03%;\"></div>
<div class=\"image-view\" data-width=\"237\" data-height=\"38\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-36687595b6bf46dd\" data-original-width=\"237\" data-original-height=\"38\" data-original-format=\"image/png\" data-original-filesize=\"10432\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>在RNN中，也可以简化成</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 173px; max-height: 39px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 22.54%;\"></div>
<div class=\"image-view\" data-width=\"173\" data-height=\"39\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-5ae43eaaf3e6c9d9\" data-original-width=\"173\" data-original-height=\"39\" data-original-format=\"image/png\" data-original-filesize=\"7215\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>其中s是输出RNN（即RNN解码器）中的隐藏层，C代表之前编码器得到的语义向量，yt-1表示上个时间段的输出，反过来作为这个时间段的输入。g可以是一个非线性的多层神经网络，产生词典中各个词语属于yt的概率。</p>
<h2><strong>Attention模型</strong></h2>
<pre><code>        encoder-decoder模型虽然非常经典，但是局限性也非常大。最大的局限性就在于编码和解码之间的唯一联系就是一个固定长度的语义向量C。也就是说，编码器要将整个序列的信息压缩进一个固定长度的向量中去。但是这样做有两个弊端，一是语义向量无法完全表示整个序列的信息，二是先输入的内容携带的信息会被后输入的信息稀释掉。输入序列越长，这个现象就越严重。这就使得在解码的时候一开始就没有获得输入序列足够的信息， 那么解码时准确率就要打一定折扣。

        为了解决上述问题，在 Seq2Seq出现一年之后，Attention模型被提出了。该模型在产生输出的时候，会产生一个注意力范围来表示接下来输出的时候要重点关注输入序列的哪些部分，然后根据关注的区域来产生下一个输出，如此反复。attention 和人的一些行为特征有一定相似之处，人在看一段话的时候，通常只会重点注意具有信息量的词，而非全部词，即人会赋予每个词的注意力权重不同。attention 模型虽然增加了模型的训练难度，但提升了文本生成的效果。模型的大概示意图如下。
</code></pre>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 396px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 55.00000000000001%;\"></div>
<div class=\"image-view\" data-width=\"720\" data-height=\"396\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-17b930b3655a1076\" data-original-width=\"720\" data-original-height=\"396\" data-original-format=\"image/jpeg\" data-original-filesize=\"18092\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>图 3：经典的attention模型</p>
<pre><code>        每一个c会自动去选取与当前所要输出的y最合适的上下文信息。具体来说，我们用 aij 衡量编码中第j阶段的hj和解码时第i阶段的相关性，最终Decoder中第i阶段的输入的上下文信息 ci 就来自于所有 hj对 aij的加权和。
</code></pre>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 281px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 39.03%;\"></div>
<div class=\"image-view\" data-width=\"720\" data-height=\"281\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-0f87fe8202a4f2d5\" data-original-width=\"720\" data-original-height=\"281\" data-original-format=\"image/jpeg\" data-original-filesize=\"24496\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>图 4：不同关注度示意图</p>
<pre><code>        输入的序列是“我爱中国”，因此，Encoder中的h1、h2、h3、h4就可以分别看做是“我”、“爱”、“中”、“国”所代表的信息。在翻译成英语时，第一个上下文c1应该和“我”这个字最相关，因此对应的 a11就比较大，而相应的 a12 、a13 、 a14 就比较小。c2应该和“爱”最相关，因此对应的 a22 就比较大。最后的c3和h3、h4最相关，因此 a33 、 a34的值就比较大。具体模型权重 aij 是如何计算出来的呢？
</code></pre>
<p>比如：<br>
输入的是英文句子：Tom chase Jerry，生成：“汤姆”，“追逐”，“杰瑞”。<br>
注意力分配概率分布值的通用计算过程：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 640px; max-height: 483px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 75.47%;\"></div>
<div class=\"image-view\" data-width=\"640\" data-height=\"483\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-134f4be35af1f6af\" data-original-width=\"640\" data-original-height=\"483\" data-original-format=\"image/jpeg\" data-original-filesize=\"21749\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>图 5：权重计算示意图</p>
<pre><code>        当前输出词Yi针对某一个输入词j的注意力权重由当前的隐层Hi，以及输入词j的隐层状态（hj）共同决定；然后再接一个sofrmax得到0-1的概率值。即通过函数F（hj,Hi）来获得目标单词Yi和每个输入单词对应的对齐可能性。更多细节，请大家参看知乎何之源的文章，文末会给出文章链接。
</code></pre>
<h2><strong>CNN的seq2seq</strong></h2>
<pre><code>        现在大多数场景下使用的Seq2Seq模型是基于RNN构成的，虽然取得了不错的效果，但也有一些学者发现使用CNN来替换Seq2Seq中的encoder或decoder可以达到更好的效果。最近，FaceBook发布了一篇论文：《Convolutional Sequence to Sequence Learning》，提出了完全使用CNN来构成Seq2Seq模型，用于机器翻译，超越了谷歌创造的基于LSTM机器翻译的效果。此网络获得暂时性胜利的重要原因在于采用了很多的窍门，这些技巧值得学习：
</code></pre>
<ul>
<li><strong>捕获long-distance依赖关系</strong></li>
</ul>
<p>底层的CNN捕捉相聚较近的词之间的依赖关系，高层CNN捕捉较远词之间的依赖关系。通过层次化的结构，实现了类似RNN（LSTM）捕捉长度在20个词以上的Sequence的依赖关系的功能。</p>
<ul>
<li><strong>效率高</strong></li>
</ul>
<p>假设一个sequence序列长度为n，采用RNN（LSTM）对其进行建模 需要进行n次操作，时间复杂度O（n）。相比，采用层叠CNN只需要进行n/k次操作，时间复杂度O（n/k）,k为卷积窗口大小。</p>
<ul>
<li><strong>并行化实现</strong></li>
</ul>
<p>RNN对sequence的建模依赖于序列的历史信息，因此不能并行实现。相比，层叠CNN对整个sequence进行卷积，不依赖序列历史信息，可以并行实现，特别是在工业生产，面临处理大数据量和实时要求比较高的情况下，模型训练更快。</p>
<ul>
<li><strong>融合多层attention</strong></li>
</ul>
<p>融合了Residual connection、liner mapping的多层attention。通过attention决定输入的哪些信息是重要的，并逐步往下传递。把encoder的输出和decoder的输出做点乘（dot products），再归一化，再乘以encoder的输入X之后做为权重化后的结果加入到decoder中预测目标语言序列。</p>
<ul>
<li><strong>gate mechanism</strong></li>
</ul>
<p>采用GLU做为gate mechanism。GLU单元激活方式如下公式所示：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 256px; max-height: 30px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 11.72%;\"></div>
<div class=\"image-view\" data-width=\"256\" data-height=\"30\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-7473f2c2f6c0a405\" data-original-width=\"256\" data-original-height=\"30\" data-original-format=\"image/png\" data-original-filesize=\"851\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<ul>
<li>
<p><strong>进行了梯度裁剪和精细的权重初始化，加速模型训练和收敛</strong></p>
<pre><code>    基于CNN的seq2seq模型和基于LSTM的Seq2Seq模型孰好孰坏，我们不能妄加评判。采用CNN的Seq2Seq最大的优点在于速度快，效率高，缺点就是需要调整的参数太多。在CNN和RNN用于NLP问题时，CNN也是可行的，且网络结构搭建更加灵活，效率高，由于RNN训练时往往需要前一时刻的状态，很难并行，特别是在大数据集上，CNN-Seq2Seq往往能取得比RNN-Seq2Seq更好的效果。
</code></pre>
</li>
</ul>
<h2><strong>应用领域</strong></h2>
<ul>
<li>**机器翻译 **</li>
</ul>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 265px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 37.86%;\"></div>
<div class=\"image-view\" data-width=\"700\" data-height=\"265\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-2184c981b0a33a4d\" data-original-width=\"700\" data-original-height=\"265\" data-original-format=\"image/jpeg\" data-original-filesize=\"15153\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>&lt;center style=\"margin: 0px; padding: 0px; max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;\"&gt;图 6：采用Seq2Seq效果对比&lt;/center&gt;</p>
<p>从图像可以看出，模型中的语境向量很明显的包涵了输入序列的语言意义，能够将由不同次序所产生的不同意思的语句划分开来，这对于提升机器翻译的准确率很有帮助。当前，主流的在线翻译系统都是基于深度学习模型来构建的，包括 Google、百度等。</p>
<ul>
<li><p>**语音识别 **<br>
输入是语音信号序列，输出是文字序列。</p></li>
<li><p>**文本摘要 **<br>
输入是一段文本序列，输出是这段文本序列的摘要序列。通常将文本摘要方法分为两类，extractive 抽取式摘要和 abstractive 生成式摘要。前者是从一篇文档或者多篇文档中通过排序找出最有信息量的句子，组合成摘要；后者类似人类编辑一样，通过理解全文的内容，然后用简练的话将全文概括出来。在应用中，extractive摘要方法更加实用一些，也被广泛使用，但在连贯性、一致性上存在一定的问题，需要进行一些后处理；abstractive 摘要方法可以很好地解决这些问题，但研究起来非常困难。</p></li>
<li>
<p>**对话生成 **<br></p>
<p></p>
Seq2Seq 模型提出之后，就有很多的工作将其应用在 Chatbot 任务上，希望可以通过海量的数据来训练模型，做出一个智能体，可以回答任何开放性的问题；而另外一拨人，研究如何将 Seq2Seq 模型配合当前的知识库来做面向具体任务的 Chatbot，在一个非常垂直的领域（比如：购买电影票等）也取得了一定的进展。 <div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 328px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 38.01%;\"></div>
<div class=\"image-view\" data-width=\"863\" data-height=\"328\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-e396f4e672f050dd\" data-original-width=\"863\" data-original-height=\"328\" data-original-format=\"image/jpeg\" data-original-filesize=\"63937\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
</li>
</ul>
<p>&lt;center data-anchor-id=\"p096\" style=\"margin: 0px; padding: 0px; max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important; color: rgb(44, 62, 80);\"&gt;图 7：对话生成Chatbot&lt;/center&gt;</p>
<ul>
<li><p>**诗词生成 ** 让机器为你写诗并不是一个遥远的梦，Seq2Seq 模型一个非常有趣的应用正是诗词生成，即给定诗词的上一句来生成下一句。</p></li>
<li>
<p>**生成代码补全 **</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 377px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 52.35999999999999%;\"></div>
<div class=\"image-view\" data-width=\"720\" data-height=\"377\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-b21e488f88b85566\" data-original-width=\"720\" data-original-height=\"377\" data-original-format=\"image/jpeg\" data-original-filesize=\"30253\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>&lt;center style=\"margin: 0px; padding: 0px; max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;\"&gt;图 8：代码补全示意图&lt;/center&gt;</p>
</li>
<li><p>**预训练 ** 2015年，Google提出了将Seq2Seq的自动编码器作为LSTM文本分类的一个预训练步骤，从而提高了分类的稳定性。这使得Seq2Seq技术的目的不再局限于得到序列本身，为其应用领域翻开了崭新的一页。</p></li>
<li>
<p><strong>阅读理解</strong></p>
<p>将输入的文章和问题分别编码，再对其进行解码得到问题的答案。</p>
</li>
</ul>
<p>小结</p>
<pre><code>         Seq-to-Seq模型从一开始在机器翻译领域被提出，到后来被广泛应用到NLP各个领域，原因就在于其对序列数据的完美使用，而且解决了以前RNN模型输出维度固定的难题，所以很快得到了推广。但Seq-to-Seq不是万能药，只有在合适的场景，它才能发挥它最大的作用。
</code></pre>
<p>参考资料</p>
<ol>
<li><p>源码地址：<a href=\"https://link.jianshu.com?t=https%3A%2F%2Fgithub.com%2Fgoogle%2Fseq2seq\" target=\"_blank\" rel=\"nofollow\">https://github.com/google/seq2seq</a></p></li>
<li><p>《Convolutional Sequence to Sequence Learning》：<br><a href=\"https://link.jianshu.com?t=https%3A%2F%2Flink.zhihu.com%2F%3Ftarget%3Dhttps%253A%2F%2Farxiv.org%2Fabs%2F1705.03122\" target=\"_blank\" rel=\"nofollow\">https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1705.03122</a></p></li>
<li><p>《Language modeling with gated linear units》：<br><a href=\"https://link.jianshu.com?t=https%3A%2F%2Flink.zhihu.com%2F%3Ftarget%3Dhttps%253A%2F%2Farxiv.org%2Fabs%2F1612.08083\" target=\"_blank\" rel=\"nofollow\">https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1612.08083</a></p></li>
<li><p>《A Convolutional Encoder Model for Neural Machine Translation》：<br><a href=\"https://link.jianshu.com?t=https%3A%2F%2Flink.zhihu.com%2F%3Ftarget%3Dhttps%253A%2F%2Farxiv.org%2Fabs%2F1611.02344\" target=\"_blank\" rel=\"nofollow\">https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1611.02344</a></p></li>
<li><p>Google Neural Machine Translation：<br><a href=\"https://link.jianshu.com?t=https%3A%2F%2Fresearch.googleblog.com%2F2016%2F09%2Fa-neural-network-for-machine.html\" target=\"_blank\" rel=\"nofollow\">https://research.googleblog.com/2016/09/a-neural-network-for-machine.html</a></p></li>
<li><p>简书：Datartisan<br><a href=\"https://www.jianshu.com/p/124b777e0c55\" target=\"_blank\">https://www.jianshu.com/p/124b777e0c55</a></p></li>
<li><p>知乎作者：李宁<br><a href=\"https://link.jianshu.com?t=https%3A%2F%2Fzhuanlan.zhihu.com%2Fp%2F30516984\" target=\"_blank\" rel=\"nofollow\">https://zhuanlan.zhihu.com/p/30516984</a></p></li>
<li><p>知乎作者：何之源<br><a href=\"https://link.jianshu.com?t=https%3A%2F%2Fzhuanlan.zhihu.com%2Fp%2F28054589\" target=\"_blank\" rel=\"nofollow\">https://zhuanlan.zhihu.com/p/28054589</a></p></li>
<li><p>PaperWeekly：张俊<br><a href=\"https://link.jianshu.com?t=https%3A%2F%2Fzhuanlan.zhihu.com%2Fp%2F26753131\" target=\"_blank\" rel=\"nofollow\">https://zhuanlan.zhihu.com/p/26753131</a></p></li>
</ol>
<hr>
<blockquote>
<p>QQ交流群：651616387   请备注 : 来自简书<br>
微信公众号：机器学习算法工程师   ----二维码见下图</p>
</blockquote>
<hr>
<p><strong>扫码关注微信号：机器学习算法工程师，更多干货分享， 或加个人微信，拉你进机器学习、深度学习交流群</strong><br></p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 430px; max-height: 430px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 100.0%;\"></div>
<div class=\"image-view\" data-width=\"430\" data-height=\"430\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-0bdf004ac80c0197\" data-original-width=\"430\" data-original-height=\"430\" data-original-format=\"image/jpeg\" data-original-filesize=\"32906\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p></p>

          </div>','1531183744'),
('325587','{442}{2786}{587}{976193}{6499}{887}{975703}{2354}{1076}{4271}{1352}{569}{351}{975714}{975892}{1094}{2790}{6020}{1124447}{976001}{975721}{247}{74}{2435}{975789}{975911}{975757}{975738}{605}{775}{1887}{975940}{975761}{975729}{975915}{884}{975859}{975727}{1129}{976042}{975775}{975744}{975787}{1687}{1483}{975839}{2372}{7825}{26951}{701}','深度学习 SSD的理解和细节分析 先放大神的论文和源码镇楼： SSD Github: https://github.com/weiliu89/caffe 请选择分支 SSD SSD paper: http://arxiv.org/abs/1512.02325 对于SSD来说，最有新意的就是它的多尺度特征，而整个代码中调整频度最高的应该是它的Prior_box，我们就从这些方面来分享一下我自己的理解。 多尺度 先说一下多尺度特征。在之前的Faster-RCNN中，特征向量都是从最后一层的Feature Maps','深度学习  SSD的理解和细节分析','<div class=\"show-content-free\">
            <p><strong>先放大神的论文和源码镇楼：</strong></p>
<blockquote>
<p><strong>SSD Github:</strong> <a href=\"https://github.com/weiliu89/caffe\" target=\"_blank\" rel=\"nofollow\"> https://github.com/weiliu89/caffe </a>  <code>请选择分支 SSD</code>  <br><strong>SSD paper:</strong>  <a href=\"http://arxiv.org/abs/1512.02325\" target=\"_blank\" rel=\"nofollow\">http://arxiv.org/abs/1512.02325</a></p>
</blockquote>
<p>对于SSD来说，最有新意的就是它的多尺度特征，而整个代码中调整频度最高的应该是它的Prior_box，我们就从这些方面来分享一下我自己的理解。</p>
<h2>多尺度</h2>
<p>先说一下多尺度特征。在之前的Faster-RCNN中，特征向量都是从最后一层的Feature Maps上得到的，对于这种单一的特征层而言，感受野是十分有限的，没有完全利用好前面几级的特征网络。在SSD中，作者从CONV4_3开始，利用多级Feature Maps的组合作为分类和回归的依据，达到了论文中提到的多尺度的效果。<br>
借用论文中的一张图来说明，作者是拿YOLO和SSD做的对比：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 437px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 52.65%;\"></div>
<div class=\"image-view\" data-width=\"830\" data-height=\"437\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6292169-bcd364aa7fed8f34.jpg\" data-original-width=\"830\" data-original-height=\"437\" data-original-format=\"image/png\" data-original-filesize=\"68700\"></div>
</div>
<div class=\"image-caption\">1.jpg</div>
</div>
<p>可以看出SSD 的特征是从不同的卷积层提取出来（上图红线），进行组合再进行回归和分类，而YOLO只有一层，在YOLO之后的版本中也借鉴了 SSD的这种多尺度的思想来提高mAp。<strong>也就是说，SSD就是Faster-RCNN和YOLO中做了一次的分类和检测过程放在不同的图像大小上做了多次。</strong></p>
<h2>Prior_box</h2>
<p>知道了SSD的特征是从不同尺度上提取的，那么论文中所说的8732 BOXES又是怎么来的呢？用下面这张表来告诉你。</p>
<table>
<thead><tr>
<th style=\"text-align:center\">name</th>
<th style=\"text-align:center\">Out_size</th>
<th style=\"text-align:center\">Prior_box_num</th>
<th style=\"text-align:center\">Total_num</th>
</tr></thead>
<tbody>
<tr>
<td style=\"text-align:center\">conv4-3</td>
<td style=\"text-align:center\">38x38</td>
<td style=\"text-align:center\">4</td>
<td style=\"text-align:center\">5776</td>
</tr>
<tr>
<td style=\"text-align:center\">fc7</td>
<td style=\"text-align:center\">19x19</td>
<td style=\"text-align:center\">6</td>
<td style=\"text-align:center\">2166</td>
</tr>
<tr>
<td style=\"text-align:center\">conv5-2</td>
<td style=\"text-align:center\">10x10</td>
<td style=\"text-align:center\">6</td>
<td style=\"text-align:center\">600</td>
</tr>
<tr>
<td style=\"text-align:center\">conv7-2</td>
<td style=\"text-align:center\">5x5</td>
<td style=\"text-align:center\">6</td>
<td style=\"text-align:center\">150</td>
</tr>
<tr>
<td style=\"text-align:center\">conv8-2</td>
<td style=\"text-align:center\">3x3</td>
<td style=\"text-align:center\">6</td>
<td style=\"text-align:center\">36</td>
</tr>
<tr>
<td style=\"text-align:center\">conv9-2</td>
<td style=\"text-align:center\">1x1</td>
<td style=\"text-align:center\">4</td>
<td style=\"text-align:center\">4</td>
</tr>
<tr>
<td style=\"text-align:center\"></td>
<td style=\"text-align:center\"></td>
<td style=\"text-align:center\"></td>
<td style=\"text-align:center\">8732</td>
</tr>
</tbody>
</table>
<p>和 Faster-RCNN一样，SSD也是特征图上的每一个点对应一组预选框。然后每一层中每一个点对应的prior box的个数，是由PriorBox这一层的配置文件决定的。拿conv4-3对应的priorbox来说，caffe的模型配置文件如下：</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 344px; max-height: 345px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 100.28999999999999%;\"></div>
<div class=\"image-view\" data-width=\"344\" data-height=\"345\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6292169-6f3efd1c1324e41d.png\" data-original-width=\"344\" data-original-height=\"345\" data-original-format=\"image/png\" data-original-filesize=\"22125\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>那么SSD是怎么生成对应的四个priorbox的呢？<br>
框的生成过程大概分为下面三种方式:</p>
<ol>
<li><p>先以 <strong>min_size</strong>为宽高生成一个框。</p></li>
<li><p>如果存在<strong>max_size</strong>则用sqrt(min_size_ * max_size_)，生成一个框。</p></li>
<li>
<p>然后根据 <strong>aspect_ratio</strong>，再去生成。如上面的配置文件，aspect_ratio=2，那么会自动的再添加一个aspect_ratiod = 1/2，然后根据下面的计算方法：<br></p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 334px; max-height: 40px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 11.98%;\"></div>
<div class=\"image-view\" data-width=\"334\" data-height=\"40\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6292169-833f4af9b8262400.png\" data-original-width=\"334\" data-original-height=\"40\" data-original-format=\"image/png\" data-original-filesize=\"3457\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p></p>
<p>分别生成两个框，一个对应 ar = 2 一个对应 ar= 1/2。</p>
</li>
</ol>
<blockquote>
<p>直观点说，就是min_size和max_size会分别生成一个正方形的框，aspect_ratio参数会生成2个长方形的框。所以输出框的个数 ：<br>
prior_box_num = count(min_size)<em>1+count(max_size)</em>1+count(aspect_ratio)*2。</p>
</blockquote>
<p><strong>PS：</strong> <strong>min_size</strong>是必须要有的参数，否则不会进入对应的框的生成过程。论文跟实际代码是有一些出入的，Git上也有人在讨论这个，基本都选择无视论文。。。</p>
<p>这里还有一个比较关键的参数，就是<strong>step</strong>，在conv4-3中设置为8，这个又是怎么来的呢？还是用一个表来看一下：</p>
<table>
<thead><tr>
<th style=\"text-align:center\">name</th>
<th style=\"text-align:center\">Out_size</th>
<th style=\"text-align:center\">Cal_scale</th>
<th style=\"text-align:center\">Real_scale</th>
</tr></thead>
<tbody>
<tr>
<td style=\"text-align:center\">conv4-3</td>
<td style=\"text-align:center\">38x38</td>
<td style=\"text-align:center\">7.8</td>
<td style=\"text-align:center\">8</td>
</tr>
<tr>
<td style=\"text-align:center\">fc7</td>
<td style=\"text-align:center\">19x19</td>
<td style=\"text-align:center\">15.78</td>
<td style=\"text-align:center\">16</td>
</tr>
<tr>
<td style=\"text-align:center\">conv5-2</td>
<td style=\"text-align:center\">10x10</td>
<td style=\"text-align:center\">30</td>
<td style=\"text-align:center\">32</td>
</tr>
<tr>
<td style=\"text-align:center\">conv7-2</td>
<td style=\"text-align:center\">5x5</td>
<td style=\"text-align:center\">60</td>
<td style=\"text-align:center\">64</td>
</tr>
<tr>
<td style=\"text-align:center\">conv8-2</td>
<td style=\"text-align:center\">3x3</td>
<td style=\"text-align:center\">100</td>
<td style=\"text-align:center\">100</td>
</tr>
<tr>
<td style=\"text-align:center\">conv9-2</td>
<td style=\"text-align:center\">1x1</td>
<td style=\"text-align:center\">300</td>
<td style=\"text-align:center\">300</td>
</tr>
</tbody>
</table>
<p><em>Cal_scale = 300/out_size</em><br>
实际就是 原图与特征图 大小的比值，比如conv4-3 width = 38 ，输入的大小为300，那么scale=7.8，所以这里设置的step=8。代码中实现如下：<br></p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 536px; max-height: 200px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 37.31%;\"></div>
<div class=\"image-view\" data-width=\"536\" data-height=\"200\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6292169-8dcd0ee5b3fda99b.png\" data-original-width=\"536\" data-original-height=\"200\" data-original-format=\"image/png\" data-original-filesize=\"20564\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p></p>
<p>这一部分的计算过程可以在 prior_box_layer.cpp的Forward_cpu中看到。</p>
<h2>特征的表出形式</h2>
<p>如果你看了SSD的网络结构会发现，每一个 convXXXX_mbox_loc 或者 convXXXX_mbox_conf后面都会跟一个permute+flatten layer,如下图：</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 657px; max-height: 320px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 48.71%;\"></div>
<div class=\"image-view\" data-width=\"657\" data-height=\"320\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6292169-e35c6a94f052f390.png\" data-original-width=\"657\" data-original-height=\"320\" data-original-format=\"image/png\" data-original-filesize=\"24255\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>这是在干什么呢？<br>
使用CAFFE的同学都知道 ，CAFFE的数据结构是 NCHW的形式（N:样本个数， C：通道数，H：高，W：宽）,而SSD的 XX_conf 和 XX_loc层的输出，是用通道来保存特征向量的，所以这里需要将通道数调整到最后，也就是 permute所做的事情，通过该层后，数据的顺序被换成了 NHWC，再通过 flatten拉成一列。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 400px; max-height: 533px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 133.25%;\"></div>
<div class=\"image-view\" data-width=\"400\" data-height=\"533\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6292169-9c749712d7bbb79f.png\" data-original-width=\"400\" data-original-height=\"533\" data-original-format=\"image/png\" data-original-filesize=\"295668\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>这里还有还是要说一下 XX_LOC 和 XX_CONF 层的输出通道的规则，XX_LOC层是用来回归框的，所以需要4个坐标信息，而XX_CONF是用来做分类的，所以需要class_num个信息，同时每个点会有多个prior_box ，我们令 <strong>K = count(prior_box)</strong>,那么相应的XX_LOC的输出的通道个数应为<strong>4*K</strong>，而XX_CONF的输出通道个数应为 <strong>class_num*K</strong>,作为验证，我们还是看一下针对于VOC的模型的参数设置，<br></p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 250px; max-height: 290px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 115.99999999999999%;\"></div>
<div class=\"image-view\" data-width=\"250\" data-height=\"290\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6292169-da955db5715886c6.png\" data-original-width=\"250\" data-original-height=\"290\" data-original-format=\"image/png\" data-original-filesize=\"13523\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p></p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 241px; max-height: 298px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 123.64999999999999%;\"></div>
<div class=\"image-view\" data-width=\"241\" data-height=\"298\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6292169-9df18dd30de7ddfe.png\" data-original-width=\"241\" data-original-height=\"298\" data-original-format=\"image/png\" data-original-filesize=\"13339\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>还是看conv4_3,这一层对应了4个prior_box ，VOC的分类个数是<strong>21(20个分类+1个背景)</strong>，所以对应的conv4_3_norm_mbox_loc 的num output = 16 = 4*4 ,而 conf的 num_output = 84 = 21*4。<strong>所以，如果针对的是自己的训练集，一定要记着修改  XX_CONF的输出通道数。</strong></p>
<blockquote>
<p>其实简单点理解，就是SSD的最后几层的输出信息都是保存在Channel这一维度的，而一个LOC+CONF+PRIOR的模块可以认为等效于一个 Faster-rcnn的最后的回归+分类过程，通过将这些子模块的特征拼接起来，得到一组特征向量，达到提取多尺度特征的目的（多个F-RCNN同时工作于同一图片的不同尺度上）。</p>
</blockquote>
<h5>再看一下为什么一个特征点要对应几个prior_box。</h5>
<p>原图中的某一个片区域，在经过几层的提取后，会抽象成特征图上的一个点，那么多对于多个prior_box而言，他们对应的都是同一组信息，那么多个prior_box的意义是什么呢？看下图：</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 424px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 53.0%;\"></div>
<div class=\"image-view\" data-width=\"800\" data-height=\"424\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6292169-6c2fc8806a8f9f7c.png\" data-original-width=\"800\" data-original-height=\"424\" data-original-format=\"image/png\" data-original-filesize=\"83955\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>该图只是示意作用，我假设某一层的特征输出中的一个点，在原图中的感受野刚好是上图左上角的区域，可以看出卡片遮挡了笔的部分特征（橙色和蓝色的框是我标注上去的，2根蓝线是示意作用，可以忽略），如果没有多个prioro_box的时候，这种场景就无法正确分类，要么认为是卡片，要么认为是笔。这时候多个Prior_box的价值就来了，因为多个框都会输出自己的坐标回归和分类，它们会去关注自己对应的特征，然后不同的框给出不同的分类得分，个人觉得有点类似于一个Attention的结构。</p>
<h2>最后还是要提一下SSD的数据增强</h2>
<p>SSD的数据增强有很多，随机的剪裁，放缩，亮度，饱和度的调整，等等。参数也基本是见名知意的，所以最好自己跟着代码看一下比较有效。如果自己需要做数据增强不妨学习一下他的用法。<br>
这里推荐一个 GIT：<a href=\"https://github.com/eric612/MobileNet-SSD-windows\" target=\"_blank\" rel=\"nofollow\"> https://github.com/eric612/MobileNet-SSD-windows</a></p>
<p>这个GIT的SSD版本是可以在 WINDOWS上跑的，这样就能用宇宙最强IDE——VS一步一步的跟着看图片的变化了。</p>

          </div>','1531183745'),
('325588','{975935}{1124449}{837}{186119}{975724}{976193}{3630}{408}{975714}{1314}{975727}{975915}{975741}{533}{742}{211934}{3180}{454}{30152}{975768}{975843}{1328}{1352}{1903}{564}{976063}{975779}{975728}{517}{36}{11426}{1228}{975835}{93842}{1018}{975918}{148662}{1983}{5506}{364984}{975880}{189}{975755}{3150}{6982}{63}{1557}{975878}{459}','Fine-grained八篇其一：Embedding Label Structures for Fi 前言： 关于Fine-grained的工作之前没有接触过。所以打算从六篇论文开始，也算是一个学习的过程。因为没有基础，可能在文章理解上会存在偏差，如果文章中有什么问题，欢迎指正。 目录：2016[CVPR]-Embedding Label Structures for Fine-Grained Feature Representation2016[CVPR]-Learning Deep Representations of Fine-Grained Visual Descriptions2016[CVPR]-Part-Stacked CNN for Fi','Fine-grained八篇其一：Embedding Label Structures for Fi','<div class=\"show-content-free\">
            <p>前言：<br>
关于Fine-grained的工作之前没有接触过。所以打算从六篇论文开始，也算是一个学习的过程。因为没有基础，可能在文章理解上会存在偏差，如果文章中有什么问题，欢迎指正。<br>
目录：<br><a href=\"https://arxiv.org/pdf/1512.02895\" target=\"_blank\" rel=\"nofollow\">2016[CVPR]-Embedding Label Structures for Fine-Grained Feature Representation</a><br><a href=\"https://arxiv.org/pdf/1605.05395\" target=\"_blank\" rel=\"nofollow\">2016[CVPR]-Learning Deep Representations of Fine-Grained Visual Descriptions</a><br><a href=\"https://arxiv.org/pdf/1512.08086\" target=\"_blank\" rel=\"nofollow\">2016[CVPR]-Part-Stacked CNN for Fine-Grained Visual Categorization</a><br><a href=\"https://arxiv.org/pdf/1611.05109\" target=\"_blank\" rel=\"nofollow\">2017[CVPR]-Low-rank Bilinear Pooling for Fine-Grained Classification</a><br><a href=\"https://arxiv.org/pdf/1605.01130\" target=\"_blank\" rel=\"nofollow\">2017[CVPR]-Mining Discriminative Triplets of Patches for Fine-Grained Classification</a><br><a href=\"\" target=\"_blank\">2017[ICCV]-Efficient Fine-grained Classification and Part Localization Using One Compact Network</a><br><a href=\"\" target=\"_blank\">2017[ICCV]-Fine-grained recognition in the wild A multi-task domain adaptation approach</a><br><a href=\"\" target=\"_blank\">2017[TMM]-Diversified visual attention networks for fine-grained object classification</a></p>
<h1>论文：Embedding Label Structures for Fine-Grained Feature Representation</h1>
<p>本文主要的创新点是提出了一种细粒度的结构化的特征表示，用于在不同的level关联和区分相近的图片。<br>
关键点有2：<br>
1.一种多任务的学习框架被设计用于学习细粒度的特征表示，通过联合优化分类和相似性约束。</p>
<ol start=\"2\">
<li>Triple loss的使用将多级的相关性和标签结构无缝的融合。</li>
</ol>
<p>对于现有的方法，作者指出，尽管Triple loss能够很好的区分类的实例，但是会照成分类准确度的下降，并且增加训练收敛的时间，此外之前的工作没有提出标签结构这样的框架，而这个框架对于在不同的级别定位图像至关重要。<br>
本文的创新点主要有两个，第一个是提出了一种多任务的学习框架，同时用雕了分类损失和相似性损失。第二个创新点是本文设计了一种嵌入标签结构，比如层级或者属性。用于区分图片。</p>
<h2>论文方法</h2>
<p>传统的用于分类的方法使用softmax loss来解决分类问题，但是这种loss往往会丢失类中的差别。为了缓解这个问题，本文采用了一个多任务的学习方法，引入了triplet loss。在文中水了一页softmax和triplet loss的介绍，我就不展开写了。如果对这两个loss的原理有不清楚的地方，可以通过百度或者google搜到很多讲解的文章。</p>
<p>为了结合这两种loss，网络被设计成了如下所示的三流的结构，这三流是共享参数的，但是输入的不同的样本，对应triplet loss的三个部分。</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 471px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 30.490000000000002%;\"></div>
<div class=\"image-view\" data-width=\"1545\" data-height=\"471\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11609151-e367621eb6193f8d.png\" data-original-width=\"1545\" data-original-height=\"471\" data-original-format=\"image/png\" data-original-filesize=\"199280\"></div>
</div>
<div class=\"image-caption\">算法流程图</div>
</div>
<p>接下来文章介绍了第二个创新点——标签结构，这里文章中提出了两种标签结构，一种是基于分层的，一种是基于属性的。基于分层的结构如下图所示：</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 461px; max-height: 245px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 53.15%;\"></div>
<div class=\"image-view\" data-width=\"461\" data-height=\"245\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11609151-a2cd89212fba3997.png\" data-original-width=\"461\" data-original-height=\"245\" data-original-format=\"image/png\" data-original-filesize=\"67869\"></div>
</div>
<div class=\"image-caption\">分层结构</div>
</div>
<br><p>为了在训练中实现这种分层结构，本文将传统的triplet loss增加了一项，该项表示那些和挑选的样本在父类中属于同一类的个体。然后扩写了triplet loss的函数，将其改为：</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 430px; max-height: 168px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 39.07%;\"></div>
<div class=\"image-view\" data-width=\"430\" data-height=\"168\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11609151-e8559434a6ea4a0e.png\" data-original-width=\"430\" data-original-height=\"168\" data-original-format=\"image/png\" data-original-filesize=\"24497\"></div>
</div>
<div class=\"image-caption\">qurdruplets函数</div>
</div>
<br>
按照这样的格式，这个公式可以扩写为x个level，这样由x子的triplets函数组成
<p>出了上面的分层的结构，细粒度的物体之间可以共享一些属性，如下图所示：</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 438px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 62.57%;\"></div>
<div class=\"image-view\" data-width=\"700\" data-height=\"438\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11609151-ab31f5ed4a743d90.png\" data-original-width=\"700\" data-original-height=\"438\" data-original-format=\"image/png\" data-original-filesize=\"166858\"></div>
</div>
<div class=\"image-caption\">共享属性</div>
</div>
<br><p>不同于上面的分层的结构，在讨论属性时，不同类别的物体可能共享相同的属性，所以，不能再像之前一样通过扩增loss函数来解决问题。这里再loss函数的常熟间隔上做文章，将m改为：</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 372px; max-height: 79px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 21.240000000000002%;\"></div>
<div class=\"image-view\" data-width=\"372\" data-height=\"79\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11609151-9c95896ab8a0bdc7.png\" data-original-width=\"372\" data-original-height=\"79\" data-original-format=\"image/png\" data-original-filesize=\"9407\"></div>
</div>
<div class=\"image-caption\">m</div>
</div>
<br>
其中括号中部分为1减属性的IoU，这样，两个物体共享的属性越多，他们在计算loss时需要考虑的间隔越小。<br>
实验结果验证了文章的方法。在此不再详细介绍，感兴趣的同学可以去看原文。

          </div>','1531183745'),
('325589','{1124451}{131}{975755}{111}{459}{1501}{975757}{1255}{975935}{594}{463}{1740}{975776}{36}{1930}{1932}{63}{210}{291129}{2046}{77012}{32425}{975773}{181364}{2193}{89132}{6003}{72280}{165}{2214}{1392}{149}{40}{32589}{1126}{1105}{1224}{10869}{2007}{761}{731}{17}{1623}{975818}{712}{442}{72}{1094}{975918}','read() # parse url data 1 html 2 \'html.parser\' 3 \'utf-8\' soup = BeautifulSoup(html,\'html.parser\',from_encoding=\'utf-8\') # img images = soup.findAll(\'img\') print(images) imageName = 0 for image in images: link = image.get(\'src\') print(\'link=\',link) link = \'http:\'+link fileFormat = link[-3:] if fileFormat == \'png\' or fileFormat == \'jpg\': fileSavePath = \'.','19- OpenCV+TensorFlow 入门人工智能图像处理-刷脸识别实现','<div class=\"show-content-free\">
            <h2>章节介绍</h2>
<p>实现一个刷脸识别</p>
<p>输入一张图片，找到图片中的人脸，识别这个是哪一个人。</p>
<p>案例代码简单 人工网络层级低。 隐层只有一层。</p>
<p>综合: 样本收集 + 图像预处理 + TensorFlow神经网络</p>
<ul>
<li>python爬虫</li>
<li>opencv预处理</li>
<li>TensorFlow神经网络 CNN</li>
</ul>
<p>每个item都有code，避免侵权。python 方法。</p>
<ul>
<li>视频-&gt; FFmpeg opencv</li>
</ul>
<p>图片预处理，刷脸功能，提取人脸(haar + Adaboost) 样本</p>
<p>cnn卷积神经网络。 yale开源识别库。</p>
<h2>爬虫获取样本</h2>
<pre><code class=\"python\"># \'https://class.imooc.com/?c=ios&amp;mc_marking=286b51b2a8e40915ea9023c821882e74&amp;mc_channel=L5
# 爬虫 1 理解爬虫原理 2 实现一个的图片爬虫
# 1 http 2 html 3 正则 过滤条件 4 其它
# 知识点多
# 1 url 2 html src 3 img 4 img url下载图片
import urllib
# import urllib3
import os
from bs4 import BeautifulSoup
# load url
html = urllib.request.urlopen(\'https://class.imooc.com/?c=ios&amp;mc_marking=286b51b2a8e40915ea9023c821882e74&amp;mc_channel=L5\').read()
# parse url data 1 html 2 \'html.parser\' 3 \'utf-8\'
soup = BeautifulSoup(html,\'html.parser\',from_encoding=\'utf-8\')
# img
images = soup.findAll(\'img\')
print(images)
imageName = 0 
for image in images:
    link = image.get(\'src\')
    print(\'link=\',link)
    link = \'http:\'+link
    fileFormat = link[-3:]
    if fileFormat == \'png\' or fileFormat == \'jpg\':
        fileSavePath = \'./img/\'+str(imageName)+\'.jpg\'
        imageName = imageName +1 
        urllib.request.urlretrieve(link,fileSavePath)
</code></pre>
<h2>FFmpeg初识</h2>
<pre><code># python 爬虫 # ffmpeg
# 样本采集车 -》路段-〉视频
# opencv 视频 本质-》ffmpeg
# 是什么？软件
# 文件格式 编解码 剪切 录制 提取 裁剪 复用
# 信息 ffmpeg -i 1.mp4
# 视频分解图片 ffmpeg -i 1.mp4 image%d.jpg
</code></pre>
<p>mac下安装ffmpeg</p>
<pre><code>brew install ffmpeg
</code></pre>
<pre><code>ffmpeg -i 1.mp4
</code></pre>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180329/Lb67LCG5j6.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180329/Lb67LCG5j6.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180329/bIG4I4Bdai.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180329/bIG4I4Bdai.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<p>分解图片</p>
<pre><code>ffmpeg -i 1.mp4 image%d.jpg
</code></pre>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180329/7C24c4hA2h.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180329/7C24c4hA2h.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<p>除过命令行，我们还可以通过api调用的方式。</p>
<h2>opencv预处理</h2>
<p>将检测到的人脸裁剪下来。</p>
<pre><code class=\"python\"># 1 load xml 2 load jpg 3 haar gray 4 detect 5 draw
import cv2
import numpy as np
# load xml 1 file name
face_xml = cv2.CascadeClassifier(\'haarcascade_frontalface_default.xml\')
eye_xml = cv2.CascadeClassifier(\'haarcascade_eye.xml\')
# load jpg
img = cv2.imread(\'face.jpg\')
cv2.imshow(\'src\',img)
# haar gray
gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
# detect faces 1 data 2 scale 3 5
faces = face_xml.detectMultiScale(gray,1.3,5)
print(\'face=\',len(faces))
# draw
index = 0
for (x,y,w,h) in faces:
    cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)
    roi_face = gray[y:y+h,x:x+w]
    roi_color = img[y:y+h,x:x+w]
    fileName = str(index)+\'.jpg\'
    cv2.imwrite(fileName,roi_color)
    index = index + 1
    # 1 gray
    eyes = eye_xml.detectMultiScale(roi_face)
    print(\'eye=\',len(eyes))
    for (e_x,e_y,e_w,e_h) in eyes:
        cv2.rectangle(roi_color,(e_x,e_y),(e_x+e_w,e_y+e_h),(0,255,0),2)
cv2.imshow(\'dst\',img)
cv2.waitKey(0)
</code></pre>
<h2>某个人脸识别</h2>
<p>数据yale加载进入， 准备label标签 组成训练数据</p>
<p>卷积神经网络 检测人脸。</p>
<pre><code class=\"python\"># 1 数据yale 2 准备train label-》train 
# 3 cnn 4 检测
import tensorflow as tf
import numpy as np
import scipy.io as sio

f = open(\'Yale_64x64.mat\',\'rb\')
mdict = sio.loadmat(f)

# fea gnd; key value
train_data = mdict[\'fea\']
train_label = mdict[\'gnd\']

# 实现数据的无序排列
train_data = np.random.permutation(train_data)
train_label = np.random.permutation(train_label)

test_data = train_data[0:64]
test_label = train_label[0:64]

# 设置随机种子
np.random.seed(100)
test_data = np.random.permutation(test_data)
np.random.seed(100)
test_label = np.random.permutation(test_label)
</code></pre>
<p>加载数据，打乱随机标签和label值的划分。</p>
<pre><code class=\"python\"># train [0-9] [10*N] [15*N]  onehot [0 0 1 0 0 0 0 0 0 0] -&gt; 2
# traindata进行处理 / 255完成归一化
train_data = train_data.reshape(train_data.shape[0],64,64,1).astype(np.float32)/255
train_labels_new = np.zeros((165,15))# 165 image 15人
for i in range(0,165):
    j = int(train_label[i,0])-1 # 1-15 转换为 0-14 
    train_labels_new[i,j] = 1

test_data_input = test_data.reshape(test_data.shape[0],64,64,1).astype(np.float32)/255
test_labels_input = np.zeros((64,15))# 165 image 15
for i in range(0,64):
    j = int(test_label[i,0])-1 # 1-15 0-14 
    test_labels_input[i,j] = 1
</code></pre>
<p>完成训练测试数据的维度处理。</p>
<h2>cnn神经网络</h2>
<p>tf.layer更简单</p>
<pre><code class=\"python\"># cnn acc  tf.nn tf.layer
data_input = tf.placeholder(tf.float32,[None,64,64,1])
label_input = tf.placeholder(tf.float32,[None,15])

layer1 = tf.layers.conv2d(inputs=data_input,filters=32,kernel_size=2,strides=1,padding=\'SAME\',activation=tf.nn.relu)
layer1_pool = tf.layers.max_pooling2d(layer1,pool_size=2,strides=2)
layer2 = tf.reshape(layer1_pool,[-1,32*32*32])
layer2_relu = tf.layers.dense(layer2,1024,tf.nn.relu)
output = tf.layers.dense(layer2_relu,15)

loss = tf.losses.softmax_cross_entropy(onehot_labels=label_input,logits=output)
train = tf.train.GradientDescentOptimizer(0.01).minimize(loss)
accuracy = tf.metrics.accuracy(labels=tf.argmax(label_input,axis=1),predictions=tf.argmax(output,axis=1))[1]
</code></pre>
<p>run</p>
<pre><code class=\"python\"># run acc
init = tf.group(tf.global_variables_initializer(),tf.local_variables_initializer())
with tf.Session() as sess:
    sess.run(init)
    for i in range(0,200):
        train_data_input = np.array(train_data)
        train_label_input = np.array(train_labels_new)
        sess.run([train,loss],feed_dict={data_input:train_data_input,label_input:train_label_input})
        acc = sess.run(accuracy,feed_dict={data_input:test_data_input,label_input:test_labels_input})
        print(\'acc:%.2f\',acc)
</code></pre>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180329/mcf8j9LbBH.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180329/mcf8j9LbBH.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<h2>本章小结</h2>
<p>从样本收集到图片预处理。</p>
<p>FFmpeg opencv预处理只要人脸。</p>
<p>cnn训练</p>
<h2>课程总结</h2>
<ul>
<li>计算机视觉入门</li>
<li>opencv + TensorFlow</li>
</ul>
<p>计算机视觉之TensorFlow: 手写数字识别<br>
计算机视觉之TensorFlow: 刷脸识别</p>
<p>主要案例</p>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180329/d8eaia8Bka.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180329/d8eaia8Bka.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<p>搭建使用两层神经网络</p>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180329/fK2c3aBK81.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180329/fK2c3aBK81.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<p>特征 + 分类器</p>
<p>hog特征(行人检测) + 分类器</p>
<p>刷脸识别: 样本收集 + 预处理 + 神经网络搭建</p>
<ul>
<li>基础知识 &amp; 实战案例 &amp; 论文分析+ 案例实战 + 算法优化</li>
</ul>
<p>物体识别，位置。</p>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180329/KDb07BKiaK.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180329/KDb07BKiaK.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<p>图片像素级别语义分析。</p>

          </div>','1531183746'),
('325590','{1124452}{1124453}{44460}{17}{442}{12494}{1124454}{975727}{976193}{296}{884}{1094}{742}{975935}{1314}{1396}{1277}{976045}{975951}{533}{22684}{28691}{1188}{255}{7700}{975738}{975767}{542}{26825}{157369}{5506}{364984}{544}{975788}{300}{3630}{979369}{127520}{976099}{2074}{1018}{459}{5135}{139}{1124456}{2269}{745}{111735}{4232}','Fine-grained八篇其二：Part-Stacked CNN for Fine-Grained 目录：2016[CVPR]-Embedding Label Structures for Fine-Grained Feature Representation2016[CVPR]-Learning Deep Representations of Fine-Grained Visual Descriptions2016[CVPR]-Part-Stacked CNN for Fine-Grained Visual Categorization2017[CVPR]-Low-rank Bilinear Pooling for Fine-Grained Classification2017[CVPR]-Mining Discriminative Triplets of Patches for Fine-Grained Classification2017[ICCV]-Efficient Fine-grained Classification and Part','Fine-grained八篇其二：Part-Stacked CNN for Fine-Grained','<div class=\"show-content-free\">
            <p>目录：<br><a href=\"https://arxiv.org/pdf/1512.02895\" target=\"_blank\" rel=\"nofollow\">2016[CVPR]-Embedding Label Structures for Fine-Grained Feature Representation</a><br><a href=\"https://arxiv.org/pdf/1605.05395\" target=\"_blank\" rel=\"nofollow\">2016[CVPR]-Learning Deep Representations of Fine-Grained Visual Descriptions</a><br><a href=\"https://arxiv.org/pdf/1512.08086\" target=\"_blank\" rel=\"nofollow\">2016[CVPR]-Part-Stacked CNN for Fine-Grained Visual Categorization</a><br><a href=\"https://arxiv.org/pdf/1611.05109\" target=\"_blank\" rel=\"nofollow\">2017[CVPR]-Low-rank Bilinear Pooling for Fine-Grained Classification</a><br><a href=\"https://arxiv.org/pdf/1605.01130\" target=\"_blank\" rel=\"nofollow\">2017[CVPR]-Mining Discriminative Triplets of Patches for Fine-Grained Classification</a><br><a href=\"\" target=\"_blank\">2017[ICCV]-Efficient Fine-grained Classification and Part Localization Using One Compact Network</a><br><a href=\"\" target=\"_blank\">2017[ICCV]-Fine-grained recognition in the wild A multi-task domain adaptation approach</a><br><a href=\"\" target=\"_blank\">2017[TMM]-Diversified visual attention networks for fine-grained object classification</a></p>
<p>摘要：本文提出了一种部件堆结构的CNN网络，用于得到细粒度分类图的明确解释。结构包含一个部件定位的全卷机网络，和一个双流的分类网络，用于同时编码部件区域和全局区域。并且在部件和全局之间采用一种共享的机制。</p>
<p>实际上从文章的introduction中的描述可以看出，本文做的还是一个细粒度图像分类的任务。摘要中提到的解释，应该指的是部件的分类。算法的整体框架如下：</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 460px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 53.49%;\"></div>
<div class=\"image-view\" data-width=\"860\" data-height=\"460\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11609151-eb5ce274c4fb0636.png\" data-original-width=\"860\" data-original-height=\"460\" data-original-format=\"image/png\" data-original-filesize=\"140555\"></div>
</div>
<div class=\"image-caption\">算法网络框架</div>
</div>
<br><p>结构显而易见，对于输入的一张待分类图片，将其送入特征提取网络，得到全局的体格特征，经过上采样，然后分别送入相同结构的特征提取网络，提取深度特征，和一个FCN网络用于产生部件区域，然后经过一个类似ROI pooing的操作，得到部件区域的深度特征，将其和整体的特征串联起来，送到后续的全连接网络做分类的操作。</p>

<p>部件区域检测方面，论文借鉴了人体姿势估计和语义分割的工作，将FCN网络用于关键点预测。具体结构如下：</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 633px; max-height: 189px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 29.86%;\"></div>
<div class=\"image-view\" data-width=\"633\" data-height=\"189\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11609151-db24c974f3e1431a.png\" data-original-width=\"633\" data-original-height=\"189\" data-original-format=\"image/png\" data-original-filesize=\"69802\"></div>
</div>
<div class=\"image-caption\">FCN结构</div>
</div>
<br><p>这里，在通过主干网络之后，通过三个1*1的卷积，将channel数降为M+1，M为部件的个数，1是背景。在计算loss时，每一个特征图对于一个关键点的标注图，计算全图的softmax loss。公式如下：</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 677px; max-height: 404px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 59.68%;\"></div>
<div class=\"image-view\" data-width=\"677\" data-height=\"404\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11609151-df1eac8c98148e88.png\" data-original-width=\"677\" data-original-height=\"404\" data-original-format=\"image/png\" data-original-filesize=\"66597\"></div>
</div>
<div class=\"image-caption\">公式1</div>
</div>
<p>特征网络这边采用了参数共享的策略，减少了网络的参数量。在得到部件的位置（点）后，取以该点为中心的6<em>6的区域作为部件的特征区域提取出来。将所有的部件区域的特征和整体的特征串联起来，这样形成了一个6</em>6<em>（256+32M）的特征图，经过一个1</em>1的卷积降维为6<em>6</em>32维之后做后面细粒度的分类（FC）。</p>

          </div>','1531183747'),
('325591','{975755}{837}{1124458}{1124122}{26}{976042}{17}{975935}{1124460}{1124461}{975910}{36}{1077}{1012876}{1124462}{975724}{1124463}{33484}{1501}{10895}{1870}{6538}{1124464}{62}{1076}{1277}{296}{1055}{35072}{5622}{1124465}{376241}{44844}{5992}{1124466}{975761}{442}{975714}{1075}{4186}{1301}{1103}{975717}{5553}{1121}{1328}{517}{54190}{975721}{6588}','IEEE Transactions on Image Processing, 2016, PP(99):1-1. [3] Li M, Zuo W, Zhang D. Convolutional Network for Attribute-driven and Identity-preserving Human Face Generation[J]. 2016. [4] Yeh R, Chen C, Lim T Y, et al. Semantic Image Inpainting with Perceptual and Contextual Losses[J].','Deep Identity-aware Transfer of Facial Attributes','<div class=\"show-content-free\">
            <p>深度身份感知的人脸属性转换器</p>
<p>摘要：<br>
本篇论文提出了一个深度卷积网络模型作为身份感知的人脸属性转换器（DIAT）。给定源图片和参照的属性，DIAT旨在生成一个人脸图片（也就是说目标图片），这个图片不仅拥有参照的属性，而且保持与输入图片一致或者相似的身份属性。我们研发了一个两阶段的方案来将输入的图片转换到每个参照的属性标签。一个前馈转换网络首先结合感觉身份感知的损失和基于GAN的属性损失来训练，接着一个人脸增强网络被引入来提升视觉质量。我们近一步定义了属性分辨器的卷积特征图上的感知身份损失，得到了一个DIAT-A模型。我们DIAT和DIAT-A模型可以为很多例如表情转换，配饰移除，年龄演变和性别转换等代表性的人脸属性转换任务提供一个统一的解决办法。实验结果证实了它们的有效性。即使对于一些身份相关的属性（例如性别），我们的DIAT-A能够通过转换属性，与此同时最大化的保留源图片的身份特征，来获得视觉上令人印象深刻的结果。</p>
<p>原文地址：<a href=\"https://arxiv.org/abs/1610.05586\" target=\"_blank\" rel=\"nofollow\">https://arxiv.org/abs/1610.05586</a></p>
<p></p>
<p></p>
这篇文章提出了用于处理人脸属性转换的一个通用的模型，整个模型如下图所示，包含了转换网络和增强网络两个部分，图片先通过转换网络，获得转换属性后的人脸图片；再通过增强网络，去除噪声，保留细节，从而得到最终的转换图片。网络结构图中命名有部分没有在文章中表现，比如Pixel Loss，但是根据文章可以推测出来这个Loss的具体表示。接下来就分两部分介绍这个生成网络： <div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 511px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 73.02%;\"></div>
<div class=\"image-view\" data-width=\"1175\" data-height=\"858\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6080435-bd701f2c39569e76.png\" data-original-width=\"1175\" data-original-height=\"858\" data-original-format=\"image/png\" data-original-filesize=\"713433\"></div>
</div>
<div class=\"image-caption\">网络结构</div>
</div>
<br><p>1.Face Transform Network人脸转换网络，对应上图a部分<br></p>
人脸转换的网络是一个16层的全卷积网络，其中包含了10层的残差网络，其具体的设置如下图所示，其中每个resigual block由两层卷积层构成。 <div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 573px; max-height: 348px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 60.73%;\"></div>
<div class=\"image-view\" data-width=\"573\" data-height=\"348\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6080435-8ce18c9cc6edd6ce.png\" data-original-width=\"573\" data-original-height=\"348\" data-original-format=\"image/png\" data-original-filesize=\"51435\"></div>
</div>
<div class=\"image-caption\">网络设置</div>
</div>
<br>
转换网络的loss的设置是文章思考较多的地方，很多人脸属性转换的问题中，很难有ground truth的结果来用于训练（比如类似于条件GAN的图片对的训练方式），因此需要小心的设计该网络的loss。<br>
文中首先提出的是identity loss（身份损失），这个loss限制的是属性转换前后的图片中，人脸的身份信息不会丢失。对于人脸的身份信息属于高层的语义信息，文章认为并不能从图片的像素角度来定义，因而选择了卷积层的feature map来定义，采用的是VGG网络的第4层和第5层转换前后图片的feature map的平方差作为身份损失，其具体在文中的定义如下： <div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 538px; max-height: 439px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 81.6%;\"></div>
<div class=\"image-view\" data-width=\"538\" data-height=\"439\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6080435-5d96c932e3441dd4.png\" data-original-width=\"538\" data-original-height=\"439\" data-original-format=\"image/png\" data-original-filesize=\"69031\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>这样主要约束了转换的图片和转换前的图片在高层的语义信息尽可能一致，由于VGG是人脸识别的网络，那么这个高层的语义信息主要就是指用于人脸识别的高层语义信息，一般就是指身份信息。这里有点不是太清楚的是，为什么作者选择了第4层和第5层作为这个损失的设计。<br></p>
<p></p>
除了身份损失，该网络中还设计了Attribute Loss（属性损失）。属性损失的设计是约束转换后的图片要有指定的属性（比如去除了眼睛，或者张开了嘴巴等等）。文章中认为有与没有某种属性的人脸，是两种不同的数据分布，进而引入了GAN的分辨器来判别这样不同的分布，选出具有指定属性的数据集（但不一定有转换图片带有该属性图片），那么转换图片就成了分辨器需要分辨出的伪造分布。既然有了分辨器，那么其对应的loss就是GAN常见的最小最大化的loss了，其定义如下，patt是指有指定属性图片构成的数据集，T(x)指转换图片x后得到的图片： <div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 529px; max-height: 88px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 16.64%;\"></div>
<div class=\"image-view\" data-width=\"529\" data-height=\"88\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6080435-1c4291819d313079.png\" data-original-width=\"529\" data-original-height=\"88\" data-original-format=\"image/png\" data-original-filesize=\"10636\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
最后，还有一个感知正则项（Perceptual regularization），用来平滑图片的。一般的文章采用的Total variation Loss来平缓生成的图片，但是文中指出了其在保留图片细节上的不足，进而提出了采用重构网络和去噪网络来平滑生成图片的思路。这部分有点复杂，所以分块来说明：<br><p></p>
首先训练一个重构网络g，这个网络的结构与转换网络一致，其训练Loss如下定义，这里的符号定义与identity loss里面的一致，是采用的VGG的卷基层得到的feature map来定义loss的： <div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 417px; max-height: 78px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 18.709999999999997%;\"></div>
<div class=\"image-view\" data-width=\"417\" data-height=\"78\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6080435-4dc281770b047f82.png\" data-original-width=\"417\" data-original-height=\"78\" data-original-format=\"image/png\" data-original-filesize=\"6831\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
有了重构网络，那么重构的图片g(x)与图片x本身的区别，就是图片需要平滑的内容，因而接着引入去噪网络f来减小这一区别，从而达到去噪效果，f的网络结构为一个2层的卷积网络，3*3的核。f网络的训练loss如下：<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 463px; max-height: 56px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 12.1%;\"></div>
<div class=\"image-view\" data-width=\"463\" data-height=\"56\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6080435-5798656986df6624.png\" data-original-width=\"463\" data-original-height=\"56\" data-original-format=\"image/png\" data-original-filesize=\"5748\"></div>
</div>
<div class=\"image-caption\"></div>
</div>其中后面f(x)-x部分，是为了防止去噪网络过渡平滑一个本身就很干净的图片。<br>
最终，基于上述两个网络，感知正则项部分定义如下，T为转换网络，f为去噪网络： <div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 480px; max-height: 59px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 12.29%;\"></div>
<div class=\"image-view\" data-width=\"480\" data-height=\"59\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6080435-3d368d8cd267abb8.png\" data-original-width=\"480\" data-original-height=\"59\" data-original-format=\"image/png\" data-original-filesize=\"6303\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p></p>
<p></p>
以上就是转换网络的loss设计部分，最终转换网络的训练Loss为： <div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 524px; max-height: 84px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 16.03%;\"></div>
<div class=\"image-view\" data-width=\"524\" data-height=\"84\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6080435-be96187a0ecfe405.png\" data-original-width=\"524\" data-original-height=\"84\" data-original-format=\"image/png\" data-original-filesize=\"13497\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p></p>
<p></p>
除此之外，转换网络并不是一次性完成训练的，其训练的过程分为预训练和训练两个阶段，在预训练阶段，网络的两个部分，一个图片转换网络，一个分辨器都分别进行了不同的预训练。对于图片转换网络，在预训练阶段将其看做一个重构网络，那么输入图片x，转换后的图片T(x)，两者之间的差异要尽可能少，因而其预训练阶段的loss为： <div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 525px; max-height: 71px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 13.52%;\"></div>
<div class=\"image-view\" data-width=\"525\" data-height=\"71\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6080435-7e485905ee8aa842.png\" data-original-width=\"525\" data-original-height=\"71\" data-original-format=\"image/png\" data-original-filesize=\"5523\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p></p>
<p></p>
对于分辨器，在预训练阶段将其看做一个分类器训练，分类输入图片的属性标签，因而其训练的Loss为： <div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 541px; max-height: 56px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 10.35%;\"></div>
<div class=\"image-view\" data-width=\"541\" data-height=\"56\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6080435-a774d167db6f58c4.png\" data-original-width=\"541\" data-original-height=\"56\" data-original-format=\"image/png\" data-original-filesize=\"5668\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>在最终的训练部分，采用ADAM训练器，在0.0001学习率下进行学习。</p>
<ol start=\"2\">
<li>
<p>Face enhancement networks（人脸增强网络）<br>
由图片转换网络生成的图片可能存在视觉上比较差的问题，因而文中加入了一个人脸增强网络，来提高生成图片的质量。由于属性转换有些属性是局部的，比如张嘴，去除眼镜等等，而有些属性是全局的，比如转换性别，这些局部属性部分存在一些特点，因此对于局部的属性和全局的属性会采用不同的网络进行人脸增强的部分。<br>
局部属性：<br></p>
<p></p>
对于局部属性的人脸转换，对于非属性区域其实应该是保持不变的，文章中首先利用文章[1]中的68个人脸特征点，对于不同的属性定义了由这些特征点组成凸包构成的属性相关区域，由此得到属性的掩码m，凸包区域内，掩码为1，其余为0。因此，此时的图片分为两个部分，一个是属性无关部分，其掩码为0，应该要求其和原图尽可能相似，因此这部分loss是增强图片与转换图片的差值的平方；另一部分是属性相关部分，要求的是增强图片在卷积的特征层面上相似，或者尽可能一致，因此这部分的loss是增强图片与转换图片的特征的差值的平方，这里描述的loss定义总结如下：（其实在论文人脸去遮挡的GAN[2]中有用到类似的不变区域的概念，不过那篇文章中不变区域直接借助掩码的方式，由原图直接生成，而变化区域才采纳GAN生成的内容） <div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 517px; max-height: 125px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 24.18%;\"></div>
<div class=\"image-view\" data-width=\"517\" data-height=\"125\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6080435-47ff721ca68bbafd.png\" data-original-width=\"517\" data-original-height=\"125\" data-original-format=\"image/png\" data-original-filesize=\"12939\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
全局属性：<br>
对于全局属性很难定义出不变的区域，因而增强部分不应从保留和非保留部分下手。增强网络实际在做的是去除生成图片中的噪声和伪造痕迹，这在早期的图片处理中采用高斯模糊就可以做到，但是高斯模糊后的图片本身也比较模糊，因而对于全局的增强网络是尽可能是的高斯模糊后的图片与模糊前的图片一致，因而对于全局属性的增强网络的loss为：<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 407px; max-height: 48px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 11.790000000000001%;\"></div>
<div class=\"image-view\" data-width=\"407\" data-height=\"48\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6080435-d714cc9945524f58.png\" data-original-width=\"407\" data-original-height=\"48\" data-original-format=\"image/png\" data-original-filesize=\"5059\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
</li>
</ol>
<p>其中B(x)表示高斯模糊后的x，E为增强网络。<br>
这两个部分的输入差别是，对于局部属性的增强网络，输入是原图和转换后的图片；对于全局属性的增强网络，输入是转换后的图片。<br>
这就是这篇文章中提出的第一个被称作DIAT的模型，在这个模型中，身份保持的Loss依赖于VGG网络卷积层中提取的特征定义，文中认为这样额外的提取并不高效，同时属性分辨的分辨器也可能难以收敛（两者之间不是因果关系），因此，文中觉得可以将身份保持和属性分辨相联系起来，利用分辨器的卷积层定义，这样对于分辨器提供了额外的监督信息，同时身份保持的Loss不需要引入额外的网络来定义。这样定义的身份保持loss被称作自适应感知身份损失(Adaptive perceptual identity loss)，其定义类似于之前的身份loss:</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 531px; max-height: 338px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 63.65%;\"></div>
<div class=\"image-view\" data-width=\"531\" data-height=\"338\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6080435-12d6174057284f3a.png\" data-original-width=\"531\" data-original-height=\"338\" data-original-format=\"image/png\" data-original-filesize=\"53281\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>采用的自适应感知身份损失的模型被称作DIAT-A，其训练的总的Loss如下定义：</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 528px; max-height: 97px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 18.37%;\"></div>
<div class=\"image-view\" data-width=\"528\" data-height=\"97\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6080435-b31bbed0bd744d0f.png\" data-original-width=\"528\" data-original-height=\"97\" data-original-format=\"image/png\" data-original-filesize=\"12996\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br><p>在DIAT-A模型中，分辨器的学习率降为0.00001，但是文中指出这么低的学习率，训练中DIAT-A收敛速度依旧比DIAT快。</p>

<p>实验部分<br>
实验的比较部分都是采用的直观的图片比较方式来进行，整个实验部分设计了多组对比。<br>
局部属性转换实验部分<br>
测试了三种局部属性转换，嘴巴张开，嘴巴闭上，眼镜移除。<br>
全局属性转换实验部分<br>
测试了两种全局属性转换，性别和年龄。对于性别的转换，只考虑男变女；对于年龄转换，只考虑年龄大的变年轻的。<br>
上述两部分的实验，都与CNIA[3]进行了比较，在眼镜移除的任务上，与语义去除[4]进行了比较。<br>
除此之外，文章比较了有与没有人脸增强网络之间结果的区别；比较了用自适应感知身份Loss的DIAT-A模型与DIAT模型之间结果的区别；探究了仅有属性loss产生图片与之前产生图片的区别；最后比较了没有感知正则项的DIAT模型与没有增强网络的DIAT模型之间结果的区别。</p>
<p>最后总结下，本篇文章提出的是一个两阶段的人脸属性转换的通用框架，第一阶段采用GAN的架构完成图片的转换部分，第二阶段分两种不同属性，对于转换后的图片进行近一步加工增强，来获取最终属性转换图片。本篇文章运用GAN的生成能力，同时增加了其他方法，来提升图片生成的质量，这可以说是运用GAN的另一个角度，其他文章中也有用其他模型生成图片（比如autoEncoder），然后采用GAN的架构或者对抗Loss来近一步优化生成图片。</p>
<p>[1] Zhang Z, Luo P, Chen C L, et al. Facial Landmark Detection by Deep Multi-task Learning[C]// European Conference on Computer Vision. 2014:94-108.<br>
[2] Zhao F, Feng J, Jian Z, et al. Robust LSTM-Autoencoders for Face De-Occlusion in the Wild[J]. IEEE Transactions on Image Processing, 2016, PP(99):1-1.<br>
[3] Li M, Zuo W, Zhang D. Convolutional Network for Attribute-driven and Identity-preserving Human Face Generation[J]. 2016.<br>
[4] Yeh R, Chen C, Lim T Y, et al. Semantic Image Inpainting with Perceptual and Contextual Losses[J]. 2016.</p>

          </div>','1531183749'),
('325592','{7689}{1020644}{1124470}{3087}{408}{17}{28945}{1124471}{216162}{975768}{854}{1099}{600}{576}{2239}{3477}{975915}{2342}{975740}{975741}{3969}{189}{11634}{1196}{11921}{975820}{975714}{975721}{975728}{2040}{36}{2089}{7703}{7341}{13301}{28400}{978260}{452}{1968}{975738}{1203}{15131}{57638}{3968}{975736}{321}{2518}{1344}{988352}','3- 深度学习之神经网络核心原理与算法-梯度消失问题 梯度消失问题 梯度消失问题是在早期的bp网络中比较常见的一个问题。一旦发生了梯度消失的问题，我们的训练就很难继续下去。训练不再收敛，也就是loss不再下降，准确率过早的无法提高。 mark 这是一个简单的神经元首尾相连组成一个神经网络。位于网络前部的w1在更新的时候，需要计算损失函数对w1的偏导。 我们根据','3- 深度学习之神经网络核心原理与算法-梯度消失问题','<div class=\"show-content-free\">
            <h2>梯度消失问题</h2>
<p>梯度消失问题是在早期的bp网络中比较常见的一个问题。一旦发生了梯度消失的问题，我们的训练就很难继续下去。训练不再收敛，也就是loss不再下降，准确率过早的无法提高。</p>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180331/AHbJ7c5di5.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180331/AHbJ7c5di5.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<p>这是一个简单的神经元首尾相连组成一个神经网络。位于网络前部的w1在更新的时候，需要计算损失函数对w1的偏导。</p>
<p>我们根据链式法则可以得到w1的偏导的表达式</p>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180331/iDIliD5hj9.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180331/iDIliD5hj9.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<p>第一项。和第三项实际是在找sigmoid函数上的斜率。</p>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180331/K3b7JIBAal.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180331/K3b7JIBAal.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180331/CBahJIIGCC.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180331/CBahJIIGCC.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<p>sigmoid函数在大于4或者小于负4时，它的导数，导数就是函数的切线斜率接近于0.</p>
<p>这两项式子只要任何一个处于大于4或者小于负4就会造成导数值接近于0.曲线的斜率接近水平了。</p>
<p>这个接近0的值连乘时会乘出一个非常小的数。当你的网络层数很多时，越往前传情况越糟糕。w的变化会越来越慢。导致这层的w没有学到什么东西。</p>
<p>这就是梯度消失，或者叫梯度弥散问题。</p>
<h3>如何避免梯度消失问题。</h3>
<p>我们刚才说导数小，导致每次更新的时候的值过于的小，是不是导数大每次更新的值就会大呢，网络的学习速度就会快呢？</p>
<p>如果你需要导数大，最好是:</p>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180331/jgcdAK2gmB.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180331/jgcdAK2gmB.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<p>在这个链式相乘的法则中每一项绝对值大于1.小于1，很小的数乘以很小的数会导致越乘越小。就会导致靠w的变化速度越来越慢，学习到的越来越低。</p>
<p>我们再来关系一下第二项和第三项。</p>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180331/Aj3dLb5KCK.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180331/Aj3dLb5KCK.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<p>第二项把z=wx+b带入。第三项其实就是对sigmoid函数求导(sigmoid函数的特性)</p>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180331/9db5I6ALik.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180331/9db5I6ALik.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180331/GjCE00Jg0b.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180331/GjCE00Jg0b.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<p>我们的想法是: 消除链式法则中发生连乘时每一项绝对值小于1的情况。</p>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180331/LJ6lGlg3aH.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180331/LJ6lGlg3aH.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<h3>方法1</h3>
<p>初始化一个合适的w，我们可以把w的值初始化的大一些。</p>
<p>把x=0带入方程:</p>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180331/H1iE4FIl0e.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180331/H1iE4FIl0e.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180331/I8daD3d52c.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180331/I8daD3d52c.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180331/F3h0ffd1dA.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180331/F3h0ffd1dA.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<p>看上去满足了我们的要求，链式相乘绝对值大于1.</p>
<blockquote>
<p>梯度爆炸。原来是因为因为前面的w1变化太慢而导致梯度消失的问题发生。<br>
现在是网络前端的这个变化率太高了。一次变化量很大，网络层数很多，有十层。十个2.5相乘就是9536</p>
</blockquote>
<p>这样会造成你挪动的步子太大了，梯度爆炸。</p>
<blockquote>
<p>因此我们给w初始化一个比较大的初始值，这个是不太可行的。</p>
</blockquote>
<h2>方法2</h2>
<p>我们要使用Relu函数来解决我们的梯度消失和爆炸的问题。</p>
<blockquote>
<p>解决这个问题的方法也就是使用导数值比较合适的激励函数来解决</p>
</blockquote>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180331/BLDgBK3cA9.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180331/BLDgBK3cA9.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180331/g9jml7gek4.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180331/g9jml7gek4.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<p>在x和0中取最大值，这个函数在原点左侧部分斜率为0，右侧则是一条斜率为1的直线。</p>
<p>在第一象限导数恒为0.x在大于0时呈线性特点，小于0时则会是一条直线。</p>
<p>relu函数提供了非常良好的非线性特征。第一象限的这条直线表明它的导数是恒为1的。</p>
<p>x小于0这部分，导数恒为0.relu函数两个显而易见的优点:</p>
<ul>
<li><p>在第一象限不会有明显的梯度消失问题。因为导数恒为1</p></li>
<li><p>我们在初始化w时，因为是随机化，w有的会很大，有的会很小。连乘时就不会出现很大一直连乘，或者很小一直连乘。</p></li>
<li><p>求导代价小。</p></li>
</ul>
</div>','1531183749'),
('325593','{975755}{975785}{36}{975757}{88560}{349155}{975761}{1551}{975835}{975872}{975714}{52921}{1025918}{975951}{2241}{111}{16750}{1124474}{210}{417}{1124475}{95441}{377701}{1124477}{1124478}{28584}{1124479}{1124481}{202351}{459}{63}{975918}{2397}{2405}{682}{658}{17}{1740}{1858}{72}{394}{15485}{13013}{252}{5503}{975767}{243533}{984626}','17- OpenCV+TensorFlow 入门人工智能图像处理-KNN最近邻域识别手写数字 样本介绍 KNN最近邻域 CNN卷积神经网络 传统的分类算法和人工神经网络 网络的搭建，每一层向量维度的变化。 cnn: 目标判决 每一层的维度变化。 mark # 1 重要 # 2 KNN CNN 2种方式 # 3 样本: MNIST_data # 4 旧瓶装新酒 ：数字识别的不同 # 4.1 网络的搭建 4.2 每一层结构输入输出 4.3 先原理 后代码 样本准备 http://yann.lecun.com/','17- OpenCV+TensorFlow 入门人工智能图像处理-KNN最近邻域识别手写数字','<div class=\"show-content-free\">
            <ul>
<li>样本介绍</li>
<li>KNN最近邻域</li>
<li>CNN卷积神经网络</li>
</ul>
<p>传统的分类算法和人工神经网络</p>
<p>网络的搭建，每一层向量维度的变化。</p>
<p>cnn: 目标判决</p>
<p>每一层的维度变化。</p>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180328/jE9g0jgl1F.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180328/jE9g0jgl1F.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<pre><code># 1 重要
# 2 KNN CNN 2种方式
# 3 样本: MNIST_data
# 4 旧瓶装新酒 ：数字识别的不同
# 4.1 网络的搭建 4.2 每一层结构输入输出 4.3 先原理 后代码 
</code></pre>
<h2>样本准备</h2>
<p><a href=\"https://link.jianshu.com?t=http%3A%2F%2Fyann.lecun.com%2Fexdb%2Fmnist%2F\" target=\"_blank\" rel=\"nofollow\">http://yann.lecun.com/exdb/mnist/</a></p>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180328/dKCCB90h2G.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180328/dKCCB90h2G.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180328/j8HgbHb481.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180328/j8HgbHb481.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<p>灰度图像，只有一个通道。</p>
<p>0表示白色。数字·代表有灰度值。</p>
<p>标签</p>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180328/EJ09DcfFD8.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180328/EJ09DcfFD8.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<p>十位，one-hot编码描述五</p>
<h2>knn识别</h2>
<p>一张待检测的图片与样本图片进行比较，找到k个与我们的图片相似的。</p>
<p>而我们取其中相似度最大的。</p>
<p>10个图片由8个都描述了1。我们就预测它是1</p>
<pre><code class=\"python\">import tensorflow as tf
import numpy as np
import random 
from tensorflow.examples.tutorials.mnist import input_data

# load data 2 one_hot : 1 0000 1 fileName 
mnist = input_data.read_data_sets(\'MNIST_data\',one_hot=True)

# 属性设置
# 训练多少张图片
trainNum = 55000
# 测试图片张数
testNum = 10000
# 训练500张
trainSize = 500
# 测试5张
testSize = 5
k = 4
</code></pre>
<p>数据装载到mnist中</p>
<h2>数据分解</h2>
<pre><code class=\"python\"># 获取4个最接近的
k = 4

# data 分解 

# 1 trainSize   2 范围: 0-trainNum 3 是否可以重复: replace=False
trainIndex = np.random.choice(trainNum,trainSize,replace=False)
testIndex = np.random.choice(testNum,testSize,replace=False)

# 获取训练数据
trainData = mnist.train.images[trainIndex]# 训练图片
trainLabel = mnist.train.labels[trainIndex]# 训练标签
testData = mnist.test.images[testIndex] # 测试图片
testLabel = mnist.test.labels[testIndex] # 测试标签
</code></pre>
<p>数据分解，取其中一部分为训练测试数据</p>
<h2>训练数据和测试数据维度</h2>
<pre><code class=\"python\"># 28*28 = 784
print(\'trainData.shape=\',trainData.shape)# 500*784 1 图片个数 2 784?
print(\'trainLabel.shape=\',trainLabel.shape)# 500*10
print(\'testData.shape=\',testData.shape)# 5*784
print(\'testLabel.shape=\',testLabel.shape)# 5*10
print(\'testLabel=\',testLabel)# 4 :testData [0]  3:testData[1] 6 
</code></pre>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180328/7hEDBeGag5.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180328/7hEDBeGag5.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<blockquote>
<p>行数表示图片的个数，784表示有784个像素。</p>
</blockquote>
<p>10列是因为进行了onehot编码。</p>
<h3>定义tf的输入输出</h3>
<pre><code class=\"python\"># 定义tf的输入输出
# tf input  784-&gt;一个完整的image
trainDataInput = tf.placeholder(shape=[None,784],dtype=tf.float32)
trainLabelInput = tf.placeholder(shape=[None,10],dtype=tf.float32)
testDataInput = tf.placeholder(shape=[None,784],dtype=tf.float32)
testLabelInput = tf.placeholder(shape=[None,10],dtype=tf.float32)
</code></pre>
<h2>使用knn的距离计算</h2>
<p>扩展维度，进行减法求差值，并进行累加。</p>
<p>简单公式</p>
<pre><code class=\"python\">#knn distance 5*784. 经过转换: 5*1*784
# 目的，计算数据。
# 测试5张 训练500张 每张784维 (3D) 行数表示训练数据，列数表示测试数据 第三个维度表示测试之差
# 2500组合*784
f1 = tf.expand_dims(testDataInput,1) # 维度扩展
f2 = tf.subtract(trainDataInput,f1)  # 减法 784 sum(784)
# 计算差距(距离有可能是负数所以取绝对值) reduction_indices在第二个维度进行累加
f3 = tf.reduce_sum(tf.abs(f2),reduction_indices=2)# 完成数据累加 784 abs
# 5*500 某一个点表示距离之差
</code></pre>
<h2>使用TensorFlow运行代码</h2>
<p>完成knn中距离的计算</p>
<pre><code class=\"python\">with tf.Session() as sess:
    # f1 &lt;- testData 5张图片
    p1 = sess.run(f1,feed_dict={testDataInput:testData[0:5]})
    print(\'p1=\',p1.shape)# p1= (5, 1, 784)
    # 训练数据与测试数据的差值
    p2 = sess.run(f2,feed_dict={trainDataInput:trainData,testDataInput:testData[0:5]})
    print(\'p2=\',p2.shape)# p2= (5, 500, 784) (1,100)点
    # p3五行五百列差值累加
    p3 = sess.run(f3,feed_dict={trainDataInput:trainData,testDataInput:testData[0:5]})
    print(\'p3=\',p3.shape)# p3= (5, 500)
    print(\'p3[0,0]=\',p3[0,0]) # 130.451 knn distance p3[0,0]= 155.812
</code></pre>
<h2>使用knn 找到最近的k个照片</h2>
<pre><code class=\"python\">f4 = tf.negative(f3)# 取反

p4 = sess.run(f4,feed_dict={trainDataInput:trainData,testDataInput:testData[0:5]})
    print(\'p4=\',p4.shape)
    print(\'p4[0,0]\',p4[0,0])
</code></pre>
<p>155 变成了-155</p>
<pre><code class=\"python\">f5,f6 = tf.nn.top_k(f4,k=4) # 选取f4 最大的四个值
</code></pre>
<p>f3选取的是距离最小的四个值。值的内容与下标。</p>
<h2>检验f5 f6运行</h2>
<pre><code class=\"python\"> p5,p6 = sess.run((f5,f6),feed_dict={trainDataInput:trainData,testDataInput:testData[0:5]})
    # p5= (5, 4) 每一张测试图片（5张）分别对应4张最近训练图片
    # p6= (5, 4)
    print(\'p5=\',p5.shape)
    print(\'p6=\',p6.shape)
    print(\'p5[0,0]\',p5[0])
    print(\'p6[0,0]\',p6[0])# p6 index
</code></pre>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180328/F2FaB499Ka.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180328/F2FaB499Ka.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<p>可以看出p6是我们的下标。</p>
<p>k个最近的图片 - 解析图片的内容 - 使用图片label标签</p>
<pre><code class=\"python\"># f3 最小的四个值
# f6 index-&gt;trainLabelInput
f7 = tf.gather(trainLabelInput,f6)

    p7 = sess.run(f7,feed_dict={trainDataInput:trainData,testDataInput:testData[0:5],trainLabelInput:trainLabel})
    print(\'p7=\',p7.shape)# p7= (5, 4, 10)
    print(\'p7[]\',p7)
</code></pre>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180328/f6H6gLH1Ch.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180328/f6H6gLH1Ch.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<h2>获取最终的数字</h2>
<p>测试图片与训练图片的距离，k个最近图片</p>
<pre><code class=\"python\">f8 = tf.reduce_sum(f7,reduction_indices=1)
# tf.argmax 选取在某一个维度上最大的值 index
f9 = tf.argmax(f8,dimension=1)
# f9 -&gt; test5 image -&gt; 5 num

 p8 = sess.run(f8,feed_dict={trainDataInput:trainData,testDataInput:testData[0:5],trainLabelInput:trainLabel})
    print(\'p8=\',p8.shape)
    print(\'p8[]=\',p8)
    
    p9 = sess.run(f9,feed_dict={trainDataInput:trainData,testDataInput:testData[0:5],trainLabelInput:trainLabel})
    print(\'p9=\',p9.shape)
    print(\'p9[]=\',p9)
</code></pre>
<h2>验证数字对不对</h2>
<pre><code class=\"python\">    p10 = np.argmax(testLabel[0:5],axis=1)
    print(\'p10[]=\',p10)
j = 0
for i in range(0,5):
    if p10[i] == p9[i]:
        j = j+1
print(\'ac=\',j*100/5)
</code></pre>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180328/jidEhgfFmf.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180328/jidEhgfFmf.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<h2>总结</h2>
<pre><code># 1 load Data  1.1 随机数 1.2 4组 训练 测试 （图片 和 标签）
# 2 knn test train distance 维度扩展 5*500 = 2500 784=28*28
# 3 knn k个最近的图片5 500 1-》500train （4）
# 4 k个最近的图片-&gt; parse centent label
# 5 label -&gt; 数字 p9 测试图片-》数据
# 6 检测概率统计
</code></pre>
<p>k值可以修改。测试训练数据可以进行修改。</p>

          </div>','1531183750'),
('325594','{3728}{17}{1124485}{131}{975768}{22531}{1074092}{1002394}{1045485}{62}{11637}{975714}{600}{975721}{975811}{975757}{975786}{975915}{5645}{975935}{36}{300}{816}{74}{975951}{975878}{189}{975720}{975779}{706}{1301}{232}{975919}{1785}{975899}{408}{96528}{85467}{7713}{1124486}{975736}{321}{1251}{976085}{554}{528}{975741}{976068}{1198}{149985}','这时候再通过算偏导求梯度，就会是这样： 其实数列的每一项都很小，再依此相乘就会越来越小，最后趋近于0，举个简单的例子就是0.9虽然很接近于1，但当有n个0.','深度神经网络优化策略之——残差学习','<div class=\"show-content-free\">
            <h1>问题起源</h1>
<p>深度学习普遍认为发端于2006年，根据<strong><em>Bengio</em></strong>的定义，深层网络由多层自适应非线性单元组成——即多层非线性模块的级联，所有层次上都包含可训练的参数，在工程实际操作中，深层神经网络通常是五层及以上，包含数百万个可学习的自由参数的庞然大物。理论上，网络模型无论深浅与否，都能通过函数逼近数据的内在关系和本质特征，但在解决真实世界的复杂问题时，需要指数增长的计算单元，浅层网络往往出现函数表达能力不足，而深层网络则可能仅仅需要较少的计算单元。<br>
　　不过网络并不是像理论上那样越深越好，除了显而易见的因为层数过多而导致浪费性质的占用显存和“吃”计算力的问题，还会出现以下三种问题。</p>
<ul>
<li>过拟合   (over fit)</li>
<li>梯度弥散 (vanishing gradient problem)</li>
<li>
<strong>网络退化</strong> (<strong>degenerate</strong>)</li>
</ul>
<p>其中，问题一、二并不是本文所讲的残差学习主要要解决的问题，所以就不多赘述，只讲述网络退化的问题。其现象如下图所示，是随着网络层数的增多，整体模型的表达能力增强，但是训练精度反而变差，并且因为训练精度本身也下降的缘故，故而可以排除是过拟合的原因，而确定是网络退化。</p>
<blockquote>
<p>When deeper networks are able to start converging, a degradation problem has been exposed: with the network depth increasing, accuracy gets saturated which might be unsurprising and then degrades rapidly. Unexpectedly, such degradation is not caused by overfitting, and adding more layers to a suitably deep model leads to higher training error,as reported in and thoroughly veried by our experiments.</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 590px; max-height: 202px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 34.239999999999995%;\"></div>
<div class=\"image-view\" data-width=\"590\" data-height=\"202\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/9400154-3abaf3d819b0337c.jpg\" data-original-width=\"590\" data-original-height=\"202\" data-original-format=\"image/jpeg\" data-original-filesize=\"68340\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
</blockquote>
<p>但是，很可惜的是，业界对于网络退化的原因及其标准情况依然没有定论，甚至说出现了随着网络变深而效果变差的问题的时候，也有可能无法分辨出是梯度弥散还是网络退化的问题。读者如果有兴趣，可以自行去寻找网络退化方面的研究论文，各家的观点虽然都不尽相同，但我们还是可以发现不少有用的信息。</p>
<h1>残差学习</h1>
<p>而对于上述问题，<strong><em>Kaiming He</em></strong>大神提出了一种简洁而不失优雅的残差学习的方法。多的不谈，我们直接甩出模型结构来讲解残差学习的思想。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 481px; max-height: 1206px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 172.35%;\"></div>
<div class=\"image-view\" data-width=\"481\" data-height=\"829\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/9400154-aed238cc49104055.jpg\" data-original-width=\"481\" data-original-height=\"829\" data-original-format=\"image/jpeg\" data-original-filesize=\"134507\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>首先，只看图的左半边，也就是橘红色的部分。左侧与普通网络连接方式的区别一目了然——在顺次直连而下的基础上加入了每隔两层的跨接桥（其实官方的叫法并非如此，然而这么叫它显得更加直观）。不过纯凭看图的感觉毕竟流于表面，用数学说话才是严谨的态度。</p>
<p>对于一个神经网络而言，我们需要用反向传播来更新参数，就像这样：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 155px; max-height: 19px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 12.26%;\"></div>
<div class=\"image-view\" data-width=\"155\" data-height=\"19\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/9400154-5ce919c83bf821a9.jpg\" data-original-width=\"155\" data-original-height=\"19\" data-original-format=\"image/jpeg\" data-original-filesize=\"2736\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 184px; max-height: 42px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 22.830000000000002%;\"></div>
<div class=\"image-view\" data-width=\"184\" data-height=\"42\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/9400154-fce0bc67e3995924.jpg\" data-original-width=\"184\" data-original-height=\"42\" data-original-format=\"image/jpeg\" data-original-filesize=\"4765\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>此时，第二个式子所得的结果就是我们常说的梯度。</p>
<p>而当如下图网络越来越深的时候：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 162px; max-height: 20px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 12.35%;\"></div>
<div class=\"image-view\" data-width=\"162\" data-height=\"20\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/9400154-f47f972ed14d2f52.jpg\" data-original-width=\"162\" data-original-height=\"20\" data-original-format=\"image/jpeg\" data-original-filesize=\"2800\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>......</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 230px; max-height: 20px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 8.7%;\"></div>
<div class=\"image-view\" data-width=\"230\" data-height=\"20\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/9400154-8eb2f2f8251ae55e.jpg\" data-original-width=\"230\" data-original-height=\"20\" data-original-format=\"image/jpeg\" data-original-filesize=\"3592\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 192px; max-height: 20px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 10.42%;\"></div>
<div class=\"image-view\" data-width=\"192\" data-height=\"20\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/9400154-7e2cf8c9502899d3.jpg\" data-original-width=\"192\" data-original-height=\"20\" data-original-format=\"image/jpeg\" data-original-filesize=\"3286\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>这时候再通过算偏导求梯度，就会是这样：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 399px; max-height: 42px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 10.530000000000001%;\"></div>
<div class=\"image-view\" data-width=\"399\" data-height=\"42\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/9400154-d97880067dbac88f.jpg\" data-original-width=\"399\" data-original-height=\"42\" data-original-format=\"image/jpeg\" data-original-filesize=\"8680\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>其实数列的每一项都很小，再依此相乘就会越来越小，最后趋近于0，举个简单的例子就是0.9虽然很接近于1，但当有n个0.9相乘时（n趋近于无限大），最后的结果就会无限趋近于0。</p>
<p>而当有了“跨接桥”之后，我们再算偏导的时候就会变成这样：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 361px; max-height: 42px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 11.63%;\"></div>
<div class=\"image-view\" data-width=\"361\" data-height=\"42\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/9400154-11f72d235adc53b8.jpg\" data-original-width=\"361\" data-original-height=\"42\" data-original-format=\"image/jpeg\" data-original-filesize=\"8582\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>说白了就是1.01的n次方依然大于1。</p>
<p>最后，我们可以发现对于相同的数据集来讲，残差网络比同等深度的其他网络表现出了更好的性能。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 600px; max-height: 370px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 61.67%;\"></div>
<div class=\"image-view\" data-width=\"600\" data-height=\"370\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/9400154-d47385ef52b35ce1.jpg\" data-original-width=\"600\" data-original-height=\"370\" data-original-format=\"image/jpeg\" data-original-filesize=\"38127\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 600px; max-height: 452px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 75.33%;\"></div>
<div class=\"image-view\" data-width=\"600\" data-height=\"452\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/9400154-f76a5d01b6969704.jpg\" data-original-width=\"600\" data-original-height=\"452\" data-original-format=\"image/jpeg\" data-original-filesize=\"46523\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>不过，这是大神的测试结果，没有什么说服力，而我在自己的项目里做了一组关于有无残差学习的对比，下面是数据图（项目是和图像增强有关，所以用PSNR作为评判标准）：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 682px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 35.52%;\"></div>
<div class=\"image-view\" data-width=\"1920\" data-height=\"682\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/9400154-44e349a976e4ec30.jpg\" data-original-width=\"1920\" data-original-height=\"682\" data-original-format=\"image/jpeg\" data-original-filesize=\"129942\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 686px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 35.730000000000004%;\"></di>
<div class=\"image-view\" data-width=\"1920\" data-height=\"686\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/9400154-670ccc75e0dca703.jpg\" data-original-width=\"1920\" data-original-height=\"686\" data-original-format=\"image/jpeg\" data-original-filesize=\"129448\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>最后可见，Loss的下降趋势，残差学习的方法明显更加平稳，而最后结果Loss和PSNR虽然差距目测不大，但最后的图片视觉效果却千差万别。</p>
<hr>
<p>下一节我们会讲模型结构图的右半边——同样是<strong><em>Kaiming</em> He</strong>大神的<strong>Skip Connection</strong>策略。</p>

          </div>','1531183751'),
('325595','{2038}{24}{1768}{87910}{975755}{975951}{5401}{63}{605}{1124489}{442}{62938}{1302}{62406}{3405}{884}{1124490}{600}{2099}{1740}{975761}{975749}{356}{153012}{1124492}{914}{975944}{382561}{70489}{1124494}{1580}{946}{975728}{975911}{975899}{749}{975915}{111}{2}{26438}{775}{215984}{4496}{379590}{1124496}{53387}{1124497}{1124498}{83317}{6462}','jpg\' img = cv2.imread(fileName) # 传入图片数据 和windows窗口步长 hist = hog.compute(img,(8,8)) # 3780维 for j in range(0,featureNum): featureArray[i,j] = hist[j] # 作用: featureArray hog [1,:] hog1 [2,:]hog2 labelArray[i,0] = 1 # 正样本 label 1 for i in range(0,NegNum): fileName = \'neg/\'+str(i+1)+\'.','16- OpenCV+TensorFlow 入门人工智能图像处理- Hog特征识别小狮子','<div class=\"show-content-free\">
            <h3>hog特征的维度</h3>
<p>haar特征是一个具体的值，白-黑</p>
<p>我们的hog特征是一个向量，就会有一个维度的问题: 必须完全描述一个obj的所有信息</p>
<p>维度 = 每个windows窗体中有多少个block,cell个数 (105,4,9)=3780</p>
<h3>梯度方向大小</h3>
<p>必须以像素为单位，每个像素都有一个梯度，所有的像素共同构成hog特征</p>
<p>windows窗体下的所有像素。</p>
<p>运算量很大，hog的特征模板 -&gt; haar类似</p>
<p>它的模板分两种: 水平和竖直</p>
<pre><code>[1,0,-1] [[1],[0],[-1]]
水平方向上，左中右三个数分别与模板相乘
a = p1*1+p2*0+p3*(-1) = 相邻像素之差
b = 上下像素之差
f = 根号下（a方+b方）
angle = arctan（a/b）
</code></pre>
<h2>bin的投影</h2>
<p>block中有36个bin，105个block。所以维度3780</p>
<p>hog特征维度的计算要基于windows窗体，一个窗体可以描述一个obj的所有信息。</p>
<p>hog特征也是描述一个对象完整的描述信息</p>
<p>梯度是根据像素来计算的，每一水平和竖直两个方向的模板。用像素和两个模板进行卷积运算，得到相邻像素之差和上下像素之差</p>
<p>使用根号下(a方+b方)得到浮值，使用arctan(a/b)得到角度</p>
<h3>bin的投影依赖于梯度</h3>
<p>bin的范围是0-360 9bin bin 0-40</p>
<p>bin1 0-20 180-200 水平方向想左，还是水平方向向右，都认为在同一个bin</p>
<p>某一个像素ij f a=10</p>
<p>位于0-20度之间。位于二者之间的中心位置，认为它投影在bin1上。</p>
<p>a 190 度，也可以认为投影在bin1上。</p>
<p>25度。会被分解到bin1 和bin2上</p>
<pre><code>f1 = f*f(夹角)
f2 = f*f(夹角) 夹角0-1之间
</code></pre>
<p>嵌入端移植。计算量太大。</p>
<h2>hog特征的最后一个问题</h2>
<p>如何计算整体的hog特征。</p>
<p>cell的复用</p>
<p>3780维向量。 来源于 win(block cell bin)</p>
<p>一个cell 分为9份。 bin0-bin8</p>
<pre><code># cell0 cell3 bin0-bin8
# cell0: bin0 bin1 。。。bin8
# cell1: bin0 bin1 。。。bin8
# cell2: bin0 bin1 。。。bin8
# cell3: bin0 bin1 。。。bin8
</code></pre>
<p>假设我们有一个像素ij，投影在了cell0上。<br>
计算出梯度bin0=f0</p>
<pre><code>#ij cell0 bin0=《f0，
#i+1 j cell0 bin0 = f1
#ij。。。。
# sumbin0（f0+f1.。）= bin0
# 权重累加
#ij bin0 bin1 

cell复用

# block 4个cell
# 【0】【1】【2】【3】
# cell0 bin0-bin9 
# cellx0 cellx2 cellx4
# cellx0:ij-》bin bin+1 只对当前cell0起作用
# cellx2：ij -》 cell2 cell3 -》bin bin+1 bin bin+1
# cellx4：ij

# 【cell 9】【4cell】【105】 = 3780
</code></pre>
<h3>判决</h3>
<p>svm 线性分类器</p>
<p>训练之后也会得到一个3780维向量</p>
<p>hog特征 与 svm得到的向量 相乘 得到一个值</p>
<p>这个值 与我们的阈值进行比较；</p>
<p>大于就是目标 小于就是非目标。</p>
<h2>hog特征总结</h2>
<p>block滑动不能能超出windows边缘。</p>
<p>windows包含所有信息。</p>
<p>0-20 180-200 都属于一个bin</p>
<p>中心位置就是一个bin 不然就要进行两个bin的合成运算</p>
<p>4个cell分为三组。</p>
<h2>Hog+SVM小狮子识别</h2>
<p>准备样本 进行训练 test预测</p>
<pre><code class=\"python\"># 1 样本 2 训练 3 test 预测
# 1 样本
# 1.1 pos 正样本 包含所检测目标 neg 不包含obj 图片大小均为64*128
# 1.2 如何获取样本 1 网络 2 公司内部 3 自己收集
# 一个好的样本 远胜过一个 复杂的神经网络 （K w）（M）
# 1.1 网络公司 样本：1张图 1元  贵
# 1.2 网络 爬虫 自己爬 
# 1.3 公司： 很多年积累（mobileeye ADAS 99%） 红外图像 
# 1.4 自己收集 视频 100秒 30 = 3000
# 正样本：尽可能的多样  环境 干扰
# 820 pos neg 1931 1:2 1:3
# name
</code></pre>
<p>其中的比如视频分解图片，缩放大小,图片质量控制，图片裁剪等都在前面讲过了</p>
<h2>训练</h2>
<p>正(820)负(1931)样本要在1:2 或1:3之间</p>
<p>名字按一定规则命名。</p>
<pre><code class=\"python\"># 训练
# 1 参数声明 2 hog 3 svm 4 computer hog 5 label 6 train 7 pred 8 draw
import cv2
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
# 1 par
# 正样本个数
PosNum = 820
# 负样本个数
NegNum = 1931
# 窗体大小
winSize = (64,128)
# block块大小
blockSize = (16,16) # 105 = （64-16) 是需要滑动的距离 / 步长 = block数 再+1
# (128-16)/8 +1 纵向需要滑动的block数
# 步长
blockStride = (8,8) # 4 cell
# cell大小
cellSize = (8,8)
# bin个数9个
nBin = 9 # 9 bin 3780个维度

# 2 hog create hog 1 win 2 block 3 blockStride 4 cell 5 bin
hog = cv2.HOGDescriptor(winSize,blockSize,blockStride,cellSize,nBin)

# 3 svm分类器创建
svm = cv2.ml.SVM_create()

# 4 computer hog
featureNum = int(((128-16)/8+1)*((64-16)/8+1)*4*9) # 3780
# 特征数组
featureArray = np.zeros(((PosNum+NegNum),featureNum),np.float32)
# 标签数组
labelArray = np.zeros(((PosNum+NegNum),1),np.int32)

# svm 监督学习 样本 标签 svm 学习使用 image hog特征(对于svm来说真正的样本)。  
for i in range(0,PosNum):
    fileName = \'pos/\'+str(i+1)+\'.jpg\'
    img = cv2.imread(fileName)
    # 传入图片数据 和windows窗口步长
    hist = hog.compute(img,(8,8)) # 3780维
    for j in range(0,featureNum):
        featureArray[i,j] = hist[j]
    # 作用: featureArray hog [1,:] hog1 [2,:]hog2 
    labelArray[i,0] = 1
    # 正样本 label 1
    
for i in range(0,NegNum):
    fileName = \'neg/\'+str(i+1)+\'.jpg\'
    img = cv2.imread(fileName)
    hist = hog.compute(img,(8,8))# 3780
    for j in range(0,featureNum):
        featureArray[i+PosNum,j] = hist[j]
    labelArray[i+PosNum,0] = -1
    # 负样本 label -1

# svm属性设置
svm.setType(cv2.ml.SVM_C_SVC)
svm.setKernel(cv2.ml.SVM_LINEAR)
svm.setC(0.01)

# 6 train
ret = svm.train(featureArray,cv2.ml.ROW_SAMPLE,labelArray)

# 7 myHog ：《-myDetect
# myDetect-《resultArray  rho
# myHog-》detectMultiScale

# 7 检测  核心：create Hog -》 myDetect—》array-》
# resultArray-》resultArray = -1*alphaArray*supportVArray
# rho-》svm-〉svm.train

# 一行一列
alpha = np.zeros((1),np.float32)
rho = svm.getDecisionFunction(0,alpha)
print(rho)
print(alpha)
alphaArray = np.zeros((1,1),np.float32)
# 支持向量
supportVArray = np.zeros((1,featureNum),np.float32)
resultArray = np.zeros((1,featureNum),np.float32)
alphaArray[0,0] = alpha
resultArray = -1*alphaArray*supportVArray

# detect
myDetect = np.zeros((3781),np.float32)
for i in range(0,3780):
    myDetect[i] = resultArray[0,i]
myDetect[3780] = rho[0]

# rho svm （判决）
# 构建hog
myHog = cv2.HOGDescriptor()
myHog.setSVMDetector(myDetect)

# load图片
imageSrc = cv2.imread(\'Test2.jpg\',1)

# (8,8) win 
objs = myHog.detectMultiScale(imageSrc,0,(8,8),(32,32),1.05,2)

# xy wh 三维 最后一维
x = int(objs[0][0][0])
y = int(objs[0][0][1])
w = int(objs[0][0][2])
h = int(objs[0][0][3])

# 绘制展示
cv2.rectangle(imageSrc,(x,y),(x+w,y+h),(255,0,0),2)
cv2.imshow(\'dst\',imageSrc)
cv2.waitKey(0)
</code></pre>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180328/EkGcja7CFc.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180328/EkGcja7CFc.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<h2>代码解析</h2>
<p>从后往前</p>
<pre><code class=\"python\"># 绘制展示
# 图片，起始位置，终止位置，颜色，线条宽度
cv2.rectangle(imageSrc,(x,y),(x+w,y+h),(255,0,0),2)
# 图片展示
cv2.imshow(\'dst\',imageSrc)
# 程序等待
cv2.waitKey(0)
</code></pre>
<pre><code class=\"python\"># load待检测图片
imageSrc = cv2.imread(\'Test2.jpg\',1)

# detectMultiScale实现对目标检测 (8,8) win 
# 图片内容  (8,8)windows步长  1.05缩放系数 32窗体大小
# 核心是我们的myhog对象
objs = myHog.detectMultiScale(imageSrc,0,(8,8),(32,32),1.05,2)

# 包含x y 宽高
# xy wh 三维信息 参数在最后一维
x = int(objs[0][0][0])
y = int(objs[0][0][1])
w = int(objs[0][0][2])
h = int(objs[0][0][3])
</code></pre>
<p>myhog对象是我们的核心</p>
<p>我们的核心是创建我们的myhog myhog的创建由下面代码</p>
<pre><code class=\"python\"># detect
myDetect = np.zeros((3781),np.float32)
for i in range(0,3780):
    myDetect[i] = resultArray[0,i]
myDetect[3780] = rho[0]

# rho svm （判决）
# 构建hog
myHog = cv2.HOGDescriptor()
myHog.setSVMDetector(myDetect)
</code></pre>
<p>cv2.HOGDescriptor()方法创建myhog，通过setSVMDetector，将当前的detect属性传递进去。</p>
<p>myDetect 来源于哪里呢？</p>
<pre><code class=\"python\">myDetect = np.zeros((3781),np.float32)
</code></pre>
<p>实际上它就是一个数组。看看它的内容来自哪里</p>
<p>mydetect这个array的内容来源于: result array 和 rho</p>
<p>核心: result array 和 rho如何计算</p>
<pre><code>rho = svm.getDecisionFunction(0,alpha)
</code></pre>
<p>rho是svm得到的hog的描述信息，会在最后判决累加的时候起作用。</p>
<p>而svm来源于哪里？</p>
<pre><code class=\"python\">ret = svm.train(featureArray,cv2.ml.ROW_SAMPLE,labelArray)
</code></pre>
<pre><code>resultArray-》resultArray = -1*alphaArray*supportVArray
</code></pre>
<p>来自支持向量的个数，当成一个参数。</p>
<p>result_array是3780 rho一维</p>
<pre><code>myHog ：《-myDetect myDetect-《 resultArray(公式)  rho
使用: myHog-》detectMultiScale
</code></pre>
<h2>机器学习小结</h2>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180328/gjcgjihIlj.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180328/gjcgjihIlj.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<p>准备样本，特征:haar特征，hog特征。分类器: Adaboost分类器</p>
<p>Hog+svm小狮子检测 haar+Adaboost 人脸检测</p>
<p>样本准备: 买 收集爬虫 视频分解</p>
<p>图片命名规则</p>
<p>haar特征用于人脸识别。积分图。</p>
<p>强分类器(判决) 弱分类器(产生强的输入) 节点</p>
<p>hog特征，模块划分。图片 窗体 block cell bin bin中的角度</p>
<p>hog特征维度 梯度计算: 相邻像素之差 bin的投影，cell的复用。</p>
<p>目标检测。</p>
<p>svm支持向量机身高体重的实现。</p>
<p>hog计算特征。myhog。</p>

          </div>','1531183752'),
('325596','{1124500}{1124501}{975755}{1124122}{1124502}{1124503}{1124504}{1108406}{1124505}{17}{1501}{3345}{6588}{975891}{62}{1477}{1124506}{1124507}{1076}{4153}{1124465}{1124508}{3000}{54190}{1145}{975714}{50330}{1121}{975724}{975878}{975727}{459}{975786}{26}{975768}{2945}{1277}{143}{1912}{775}{975918}{975741}{2714}{975779}{975728}{1328}{975757}{1354}{29708}{975935}','Face Aging with Contextual Generative Adversarial 使用上下文对抗生成网络的人脸老化 摘要： 人脸老化——它对于输入的人脸给出老化的人脸——在多媒体研究中具有广泛的关注。最近很多基于条件对抗生成网络的方法取得的巨大的成功。他们可以生成图片拟合在每个单独的年龄组中的真实的人脸分布。但是这些方法不能获取到转换模式，比如说在相邻年龄组之间逐渐的形状和纹理的变','Face Aging with Contextual Generative Adversarial ','<div class=\"show-content-free\">
            <p>使用上下文对抗生成网络的人脸老化<br>
摘要：<br>
人脸老化——它对于输入的人脸给出老化的人脸——在多媒体研究中具有广泛的关注。最近很多基于条件对抗生成网络的方法取得的巨大的成功。他们可以生成图片拟合在每个单独的年龄组中的真实的人脸分布。但是这些方法不能获取到转换模式，比如说在相邻年龄组之间逐渐的形状和纹理的变化。本文中我们提出了一种新的上下文对抗生成网络（C-GANs）来特别地考虑转换模式。C-GANs由一个条件转换网络和两个分辨网络组成。条件转换网络使用几个特别设计的残差块来模拟老化过程。年龄分辨网络引导合成的人脸来拟合真实的条件分布。转换模式分辨网络是新的，旨在将真实的转换模式同假的区分出来。它被当做条件转换网络的额外正则项，来确保生成的图片拟合了对应的真实转换模式的分布。实验结果表明提出的框架与最先进的模型和真实的数据相比较，产生了感人的结果。我们也观察到对于跨年龄人脸确认问题的性能提升。<br>
关键词：人脸老化，对抗生成网络，上下文模型<br>
原文地址：<a href=\"https://link.jianshu.com?t=https%3A%2F%2Farxiv.org%2Fabs%2F1802.00237\" target=\"_blank\" rel=\"nofollow\">https://arxiv.org/abs/1802.00237</a></p>
<p>文章中采用的是一个双分辨器单生成器的条件GAN的模型，其模型结构如下所示：</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 438px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 39.21%;\"></div>
<div class=\"image-view\" data-width=\"1117\" data-height=\"438\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6080435-90e11efaae32f80c.png\" data-original-width=\"1117\" data-original-height=\"438\" data-original-format=\"image/png\" data-original-filesize=\"47593\"></div>
</div>
<div class=\"image-caption\">文章整体模型架构</div>
</div>
<br><p>两个分辨器分别是分辨图片真伪的分辨器文中称作Age Discriminative Network，对应文章的3.3小节；分辨转换模式的分辨器文中称作Transition Pattern Discriminative Network，对应文章的3.4小节。一个生成器为条件GAN的类型，文中称作Conditional Transformation Network。<br>
1.Age Discriminative Network<br>
该分辨网络接收图片和对应年龄标签作为输入，输出判断该图片是否为伪造的结果。文中指出，年龄的标签和图片分别单独经过一次卷积，然后再连接在一起，送入该分辨网络。该网络的训练的loss为：</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 93px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 12.379999999999999%;\"></div>
<div class=\"image-view\" data-width=\"751\" data-height=\"93\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6080435-887a5d9553da04b0.png\" data-original-width=\"751\" data-original-height=\"93\" data-original-format=\"image/png\" data-original-filesize=\"20678\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>2.Transition Pattern Discriminative Network<br>
该网络接收的是相邻年龄的图片对和年龄标签作为输入，令x(y)表示年龄为y的图片x，那么该网络接收的是(x(y)，x(y+1)，y)三元组作为输入，而图片x可能全部是真实图片，也可能是生成的图片，最终该网络判断给出该图片对，是否为真实的图片对。因而，该网络的训练的loss为：</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 196px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 27.07%;\"></div>
<div class=\"image-view\" data-width=\"724\" data-height=\"196\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6080435-46d49fb2ee37ee1e.png\" data-original-width=\"724\" data-original-height=\"196\" data-original-format=\"image/png\" data-original-filesize=\"32305\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<ol start=\"3\">
<li>Conditional Transformation Network<br>
该网络接收人脸图片，以及需要生成的人脸图片的年龄标签。年龄标签为7维的向量，然后被扩充到与图片的空间维数一致（我的理解是，对于RGB图片，是宽<em>长</em>3，因而这样的图片对应的label也是3维的），同时标签中0被-1表示，原因是图片的数值范围是在[-1,1]区间范围内。与此同时，该生成器采用了残差网络的设计，最终的图片是特征图和原图结合起来生成的，具体的结构图下图所示，这样估计可以保证生成图既有feature和label的信息，同时也有原图的信息。<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 312px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 44.64%;\"></div>
<div class=\"image-view\" data-width=\"1568\" data-height=\"700\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6080435-01fc04319391f37a.png\" data-original-width=\"1568\" data-original-height=\"700\" data-original-format=\"image/png\" data-original-filesize=\"259104\"></div>
</div>
<div class=\"image-caption\">conditional transformation network</div>
</div>
</li>
</ol>
<p>最终，整个网络的训练的Loss为：</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 275px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 38.190000000000005%;\"></div>
<div class=\"image-view\" data-width=\"720\" data-height=\"275\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6080435-672ed9f1a8160e4a.png\" data-original-width=\"720\" data-original-height=\"275\" data-original-format=\"image/png\" data-original-filesize=\"57884\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>这里的TV就是total variation的损失函数，用来平滑生成的图片的，见诸多篇论文中。对于这样的双分辨器，单生成器的网络架构，文章采用的是交替训练的方法，一次迭代中优化一个分辨器和生成器，另一次迭代中优化另一个分辨器和生成器。</p>
<p>实验部分<br>
实验分成了定性的评估和定量的评估，定性的评估主要是以生成的图片，来给出一个直观的判断；同时也做了与一些明星ground truth的定性的比较分析。<br>
定量的评估，设计了一个人评估的系统，给出三种生成图片，以此判断哪张结果最好。首先这个评估结果，虽然是量化分析，但是依旧是人评价的系统，带有一定主观性（不过现阶段很少有对于GAN生成图片好坏的评价指标，有些量化的指标也被指出不准确）；其次文中并没有详细说明给志愿者判断的问题的内容描述部分。<br>
最具有说服力的定量分析，应该文中4.6小节给出的cross age face verification，实验的过程是，对于原始的图片对，是一张年轻的人脸和老年的人脸，这两张人脸之间年龄大于20；生成的图片对是，用原始图片对中年轻的人脸生成一张老龄化的人脸，然后与之前老化后的人脸组成生成图片对，采用基于center loss训练的人脸识别模型，来判断这样的人脸对是否属于同一个人。从实验的EER(the equal error rate)中可以看到这样的face aging模型生成的图片确实对于跨年龄的人脸识别有帮助，同时该方法的帮助最大。Figure11给出了FAR-FRR曲线，同一分类曲线下，不同数据对的FAR-FRR曲线，这表明了数据起了决定性作用，可以看到红色和绿色都比原始的图片对的蓝色曲线都低（这里估计有个小问题需要去查center loss based face verification那篇文章，就是FAR-FRR曲线中的接受率的阈值）。</p>
<p>最终总结下这篇文章的工作：首先采用的是之前有人提及的双分辨器的GAN网络结构（在之前论文中被称作双代理GAN），但是在整体架构没有创新的情况下，在生成器中采用了残差网络的结构，这是之前GAN结构中没有看到过的（至少我没有看到过）；其次是对于分辨器的创新，对于人脸老化的特定问题中，图片对中存在关系的原因，将有关系的图片对交给分辨器判断，以促进生成器学习到此类的相互关系，这种图片关系对的思路应该还可以应用于其他生成图片应用中；label归一化以及卷积处理是之前GAN的论文中没有看到过的处理，可能有利于生成器同时使用图片信息和输入的label信息；关于实验部分，现在仍然没有很好的定量分析GAN生成图片的方法，一般还是具体问题中，转换形式来进行判别，比如本文中的生成的图片提升了人脸识别率的对比。</p>

          </div>','1531183753'),
('325597','{1124513}{30}{222}{4926}{955}{975714}{36}{442}{975915}{4448}{156322}{975728}{1124514}{103195}{87396}{975757}{975835}{1124515}{975785}{530}{975843}{90856}{14694}{1476}{975925}{1124516}{975919}{975880}{189}{975890}{975786}{1099}{17}{975951}{72}{4339}{1190}{1096}{975929}{976685}{1436}{4343}{4604}{300}{37309}{139}{2535}{1635}{1413}{975841}','全面解读Group Normbalization-（吴育昕-何凯明 重磅之作) 前 言 Face book AI research（FAIR） 吴育昕-凯明联合推出重磅新作 Group Normbalization（GN），提出使用 Group Normalization (https://arxiv.org/abs/1803.08494) 替代深度学习里程碑式的工作Batchnormalization， 笔者见猎心喜，希望和各位分享此工作，本文将从以下三个方面为读者详细解读此篇文章： What\'s wrong with BN ? How GN work ?','全面解读Group Normbalization-（吴育昕-何凯明 重磅之作)','<div class=\"show-content-free\">
            <h2><strong>前  言</strong></h2>
<p>Face book AI research（FAIR）</p>
<p>吴育昕-凯明联合推出重磅新作</p>
<p>Group Normbalization（GN），提出使用</p>
<p>Group Normalization</p>
<p>(<a href=\"https://link.jianshu.com?t=https%3A%2F%2Farxiv.org%2Fabs%2F1803.08494\" target=\"_blank\" rel=\"nofollow\">https://arxiv.org/abs/1803.08494</a>)</p>
<p>替代深度学习里程碑式的工作Batchnormalization，</p>
<p>笔者见猎心喜，希望和各位分享此工作，本文将从以下三个方面为读者详细解读此篇文章：</p>
<ul>
<li><p><strong>What\'s wrong with BN ?</strong></p></li>
<li><p><strong>How GN work ?</strong></p></li>
<li>
<p><strong>Why GN work ?</strong></p>
<p>注：本文谨代表笔者观点，文中若有不足指出及疏忽之处，诚请批评指正</p>
</li>
</ul>
<h2>
<strong>1</strong> What is Group Normbalization</h2>
<p>一句话概括，GroupNormbalization（GN）是一种新的深度学习归一化方式，可以替代BN。</p>
<p>众所周知，BN是深度学习中常使用的归一化方法，在提升训练以及收敛速度上发挥了重大的作用，是深度学习上里程碑式的工作，但是其仍然存在一些问题，而新提出的GN解决了BN式归一化对batch size依赖的影响。详细的BN介绍可以参考我的另一篇博客(<a href=\"https://link.jianshu.com?t=https%3A%2F%2Fblog.csdn.net%2Fqq_25737169%2Farticle%2Fdetails%2F79048516\" target=\"_blank\" rel=\"nofollow\">https://blog.csdn.net/qq_25737169/article/details/79048516</a>)。</p>
<p>So, BN到底出了什么问题， GN又厉害在哪里？</p>
<h2>
<strong>2</strong> What\'s wrong with BN</h2>
<p>BN全名是BatchNormalization，见名知意，其是一种归一化方式，而且是以batch的维度做归一化，那么问题就来了，此归一化方式对batch是--dependent的，过小的batch size会导致其性能下降，一般来说每GPU上batch设为32最合适，但是对于一些其他深度学习任务batch size往往只有1-2，比如目标检测，图像分割，视频分类上，输入的图像数据很大，较大的batchsize显存吃不消。那么，对于较小的batchsize，其performance是什么样的呢？如下图：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 455px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 65.09%;\"></div>
<div class=\"image-view\" data-width=\"1080\" data-height=\"703\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-ba908358ef8121a5\" data-original-width=\"1080\" data-original-height=\"703\" data-original-format=\"image/jpeg\" data-original-filesize=\"40079\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>横轴表示每个GPU上的batch size大小，从左到右一次递减，纵轴是误差率，可见，在batch较小的时候，GN较BN有少于10%的误差率。</p>
<p>另外，BatchNormalization是在batch这个维度上Normalization，但是这个维度并不是固定不变的，比如训练和测试时一般不一样，一般都是训练的时候在训练集上通过滑动平均预先计算好平均-mean，和方差-variance参数，在测试的时候，不在计算这些值，而是直接调用这些预计算好的来用，但是，当训练数据和测试数据分布有差别是时，训练机上预计算好的数据并不能代表测试数据，这就导致在训练，验证，测试这三个阶段存在inconsistency。</p>
<p>既然明确了问题，解决起来就简单了，归一化的时候避开batch这个维度是不是可行呢，于是就出现了layer normalization和instance normalization等工作，但是仍比不上本篇介绍的工作GN。</p>
<h2>
<strong>3</strong> How GN Work</h2>
<p>GN本质上仍是归一化，但是它灵活的避开了BN的问题，同时又不同于Layer</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 335px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 31.019999999999996%;\"></div>
<div class=\"image-view\" data-width=\"1080\" data-height=\"335\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-fe4c6010f1fdba1d\" data-original-width=\"1080\" data-original-height=\"335\" data-original-format=\"image/jpeg\" data-original-filesize=\"60405\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>从左到右一次是BN，LN，IN，GN</p>
<p>众所周知，深度网络中的数据维度一般是[N, C, H, W]或者[N, H, W，C]格式，N是batch size，H/W是feature的高/宽，C是feature的channel，压缩H/W至一个维度，其三维的表示如上图，假设单个方格的长度是1，</p>
<p>那么其表示的是[6, 6，*, * ]</p>
<p>上图形象的表示了四种norm的工作方式：</p>
<ul>
<li><p>BN在batch的维度上norm，归一化维度为[N，H，W]，对batch中对应的channel归一化；</p></li>
<li><p>LN避开了batch维度，归一化的维度为[C，H，W]；</p></li>
<li><p>IN 归一化的维度为[H，W]；</p></li>
<li><p><strong>而****GN****介于****LN****和****IN****之****间****，其首先将****channel****分****为许****多****组****（****group****），****对****每一****组****做****归****一化，及先将****feature****的****维****度由****[N, C, H,     W]reshape****为****[N, G****，****C//G , H, W]****，****归****一化的****维****度****为****[C//G , H, W]</strong></p></li>
</ul>
<p>事实上，GN的极端情况就是LN和I N，分别对应G等于C和G等于1，作者在论文中给出G设为32较好</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 679px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 62.870000000000005%;\"></div>
<div class=\"image-view\" data-width=\"1080\" data-height=\"679\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-bd694e953b4c4ea4\" data-original-width=\"1080\" data-original-height=\"679\" data-original-format=\"image/jpeg\" data-original-filesize=\"85067\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<pre><code>          由此可以看出，GN和BN是有很多相似之处的，代码相比较BN改动只有一两行而已，论文给出的代码实现如下：
</code></pre>
<p>&lt;pre style=\"margin: auto 0px auto 48px; padding: 0px; max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important; color: rgb(51, 51, 51); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: 0.544px; orphans: 2; text-align: center; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-style: initial; text-decoration-color: initial; font-size: 16px;\"&gt;</p>
<pre><code>defGroupNorm(x, gamma, beta, G, eps=1e-5):     # x: input features with shape [N,C,H,W]     # gamma, beta: scale and offset, with shape [1,C,1,1]     # G: number of groups for GN     N, C, H, W = x.shape     x = tf.reshape(x,[N, G, C // G, H, W])   mean,var= tf.nn.moments(x,[2,3,4], keep dims=True)   x =(x - mean)/ tf.sqrt(var+ eps)   x = tf.reshape(x,[N, C, H, W])   return x * gamma + beta
</code></pre>
<p>&lt;/pre&gt;</p>
<p>其中beta 和gama参数是norm中可训练参数，表示平移和缩放因子.</p>
<p>从上述norm的对比来看，不得不佩服作者四两拨千斤的功力，仅仅是稍微的改动就能拥有举重若轻的效果。</p>
<h2>
<strong>4</strong> ## Why GN work</h2>
<p>上面三节分别介绍了BN的问题，以及GN的工作方式，本节将介绍GN work的原因。</p>
<p>传统角度来讲，在深度学习没有火起来之前，提取特征通常是使用SIFT，HOG和GIST特征，这些特征有一个共性，都具有按group表示的特性，每一个group由相同种类直方图的构建而成，这些特征通常是对在每个直方图（histogram）或每个方向（orientation）上进行组归一化（group-wise norm）而得到。而更高维的特征比如VLAD和Fisher Vectors(FV)也可以看作是group-wisefeature，此处的group可以被认为是每个聚类（cluster）下的子向量sub-vector。</p>
<p>从深度学习上来讲，完全可以认为卷积提取的特征是一种非结构化的特征或者向量，拿网络的第一层卷积为例，卷积层中的的卷积核filter1和此卷积核的其他经过transform过的版本filter2（transform可以是horizontal flipping等），在同一张图像上学习到的特征应该是具有相同的分布，那么，具有相同的特征可以被分到同一个group中，按照个人理解，每一层有很多的卷积核，这些核学习到的特征并不完全是独立的，某些特征具有相同的分布，因此可以被group。</p>
<pre><code>        导致分组（group）的因素有很多，比如频率、形状、亮度和纹理等，HOG特征根据orientation分组，而对神经网络来讲，其提取特征的机制更加复杂，也更加难以描述，变得不那么直观。另在神经科学领域，一种被广泛接受的计算模型是对cell的响应做归一化，此现象存在于浅层视觉皮层和整个视觉系统。

        作者基于此，提出了组归一化（Group Normalization）的方式，且效果表明，显著优于BN、LN、IN等。

         GN的归一化方式避开了batchsize对模型的影响，特征的group归一化同样可以解决的问题，并取得较好的效果。
</code></pre>
<h2>
<strong>5</strong> ## Show  Time</h2>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 391px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 36.199999999999996%;\"></div>
<div class=\"image-view\" data-width=\"1080\" data-height=\"391\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-02ab84e166d858ee\" data-original-width=\"1080\" data-original-height=\"391\" data-original-format=\"image/jpeg\" data-original-filesize=\"49318\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>以resnet50为base model，batchsize设置为32在imagenet数据集上的训练误差（左）和测试误差（右）<br>
GN没有表现出很大的优势，在测试误差上稍大于使用BN的结果。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 408px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 37.78%;\"></div>
<div class=\"image-view\" data-width=\"1080\" data-height=\"408\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-ebad8549507e9625\" data-original-width=\"1080\" data-original-height=\"408\" data-original-format=\"image/jpeg\" data-original-filesize=\"50834\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>可以很容易的看出，GN对batch size的鲁棒性更强</p>
<p>同时，作者以VGG16为例，分析了某一层卷积后的特征分布学习情况，分别根据不使用Norm 和使用BN，GN做了实验，实验结果如下：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 276px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 25.56%;\"></div>
<div class=\"image-view\" data-width=\"1080\" data-height=\"276\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-74e53b29ab78d7e3\" data-original-width=\"1080\" data-original-height=\"276\" data-original-format=\"image/jpeg\" data-original-filesize=\"34010\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>统一batch size设置的是32，最左图是不使用norm的conv5的特征学习情况，中间是使用了BN结果，最右是使用了GN的学习情况，相比较不使用norm，使用norm的学习效果显著，而后两者学习情况相似，不过更改小的batch size后，BN是比不上GN的。</p>
<p>作者同时做了实验展示了GN在object detector/segmentation 和video classification上的效果，详情可见原文，此外，作者在paper最后一节中大致探讨了discussion and future work, 实乃业界良心。</p>
<p>小结</p>
<p>本文从三个方面分析了BN的drawback，GN的工作机制，GN work的背后原理，希望对读者有所帮助。</p>

          </div>','1531183755'),
('325598','{548099}{24}{88597}{442}{975951}{296}{8110}{975761}{186626}{2099}{884}{153012}{1124517}{975755}{2686}{1188}{770}{1124518}{1037612}{86609}{975703}{303}{975786}{976011}{569}{453}{29067}{1580}{63}{975727}{975728}{36}{2}{6034}{975721}{3538}{8872}{4100}{11619}{2624}{978820}{26053}{2273}{975714}{3144}{20076}{975738}{7227}{975952}','5，我们正确的样本系数减小，负样本得到加强。 把出错的样本进行加强权值。再把整个结果送到下一个基本分类器 训练终止条件：1.','14- OpenCV+TensorFlow 入门人工智能图像处理-haar特征','<div class=\"show-content-free\">
            <h2>haar特征</h2>
<ol>
<li>什么是haar特征？</li>
</ol>
<blockquote>
<p>特征 = 某个区域的像素点经过某种四则运算之后得到的结果。</p>
</blockquote>
<p>这个结果可以是一个具体的值也可以是一个向量，矩阵，多维。</p>
<p>矩阵运算</p>
<ol start=\"2\">
<li>如何利用特征 区分目标？</li>
</ol>
<p>阈值判决，如果大于某个阈值，认为是目标。小于某个阈值认为是非目标。</p>
<ol start=\"3\">
<li>如何得到这个判决？</li>
</ol>
<p>使用机器学习。</p>
<ol>
<li>特征是什么？ 2. 如何进行判决特征 3. 如何得到这个判决</li>
</ol>
<p>什么是haar特征？</p>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180327/cl8IlBIdgd.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180327/cl8IlBIdgd.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<p>这些是在opencv中使用的haar特征。</p>
<ul>
<li>基础类型(5种) 核心(三种)</li>
</ul>
<p>这里的14个图片分别对应十四种特征。</p>
<p>第一种特征以黑白两色构成。分为上下结构。</p>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180327/4eIjjklKHe.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180327/4eIjjklKHe.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<p>蓝色区域表明我们所得到的图片。黑白矩形框是我们的特征模板。</p>
<p>比如: 我们的模板是一个(10,10)的矩阵，共覆盖了100个像素点。黑白各占50个像素点</p>
<p>将当前模板放在图像上的任意位置上，在这里，用白色区域覆盖的50个像素之和减去黑色区域的50个像素之和得到我们的特征。</p>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180327/ebe3KcIHB5.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180327/ebe3KcIHB5.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<p>其他两个公式</p>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180327/62KjjIiH20.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180327/62KjjIiH20.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<blockquote>
<p>这里的整个区域指黑白两色覆盖的整个100个像素。这两个权重是不一样的。</p>
</blockquote>
<p>推导公式的一样性。</p>
<p>整体的权重值为1 黑色部分权重值为-2</p>
<pre><code>公式二: 整个区域的像素值 * 权重1 + 黑色部分 * 权重2
= 整个区域 * 1 + 黑色部分 * -2
=(黑 + 白) * 1 + 黑色部分 * -2
= 白色 - 黑色
</code></pre>
<h2>haar特征遍历过程</h2>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180327/KjHi2jjH46.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180327/KjHi2jjH46.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<p>上一节讲解了haar特征的计算原理。</p>
<p>假设这个haar特征的模板是(10,10)共100个像素。图片的大小是(100,100)</p>
<p>如果想获取这个图片上所有的harr特征。使用当前模板沿着水平和竖直方向进行滑动。</p>
<p>从上到下，从左到右进行遍历。遍历的过程中要考虑步长问题。</p>
<p>这个模板一次滑动几个像素。一次滑动10个像素，就需要9次。加上最开始的第0次。<br>
10个模板。</p>
<p>计算这100个模板才能将haar特征计算完毕。</p>
<p>如果我们的步长设置为5.那么就要滑动20次。400个模板。运算量增加4倍。</p>
<p>其实仅仅这样的一次滑动并没有结束，对于每一个模板还要进行几级缩放，才能完成。</p>
<h3>haar特征:</h3>
<ol>
<li>什么是haar？</li>
</ol>
<blockquote>
<p>特征 = 像素 运算 -&gt; 结果(具体值 向量 矩阵 多维)</p>
</blockquote>
<ol start=\"2\">
<li>如何利用特征 区分目标？</li>
</ol>
<blockquote>
<p>阈值判决</p>
</blockquote>
<ol start=\"3\">
<li>得到判决？</li>
</ol>
<blockquote>
<p>机器学习</p>
</blockquote>
<ul>
<li>1 特征 2 判决 3 得到判决</li>
</ul>
<p>公式推导 权重1为1  权重2为-2</p>
<pre><code>特征 = 整个区域*权重1 + 黑色*权重2 = （黑+白）*1+黑*（-2）=
= 黑+白-2黑 = 白-黑
</code></pre>
<ul>
<li>haar模版 从上至下 从左至右</li>
<li>image size 模版 size</li>
</ul>
<pre><code>100*100图片 10*10模板 100次(个模板) step = 10
1. 100*100 2. 10*10 3. step 10 4. 模版1
</code></pre>
<ul>
<li>模版  不仅可以滑动 还能缩放 <code>10*10</code>  第二次滑动 <code>11*11</code> 20级(20次缩放)</li>
</ul>
<p>举例 图片大小<code>1080*720</code>  步长step<code>2</code> <code>10*10</code>模板</p>
<pre><code>计算量 = 14个模版*20级缩放*(1080/2*720/2) * (100点+ -) = 50-100亿
(50-100)*15 = 1000亿次
</code></pre>
<h2>积分图</h2>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180327/59AgGbAIfi.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180327/59AgGbAIfi.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<p>积分图听起来有点像数据累加。特征计算时讲解了前两种公式。</p>
<p>无论是哪种计算公式，都要求计算矩阵方框中的公式。</p>
<p>所以基于这种思想有人提出了一种简单的算法。</p>
<p>有ABCD四个区域，每个区域都是一个矩形方块。</p>
<p>A区域是左上角那一块小区域，而B区域是包含A区域的长条。</p>
<p>C区域又是包含A的竖直长条。D区域是四个方块之和。</p>
<p>1,2,3,4表明这四个小区域。</p>
<p>进行快速计算:</p>
<pre><code> A 1 B 1 2 C 1 3 D 1 2 3 4
 4 = A-B-C+D = 1+1+2+3+4-1-2-1-3 = 4(10*10 变成3次+-)
</code></pre>
<p>任意一个方框，可由周围的矩形进行相减得到。</p>
<p>问题: 在计算每一个方块之前，需要将图片上所有的像素全部遍历一次。</p>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180327/md68IdC8CC.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180327/md68IdC8CC.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<p>p1 p2 p3 p4 分别是指某一个特征相邻的abcd的模块指针。</p>
<h3>adaboost分类器 训练</h3>
<p>haar特征 + Adaboost是一个非常常见的组合，在人脸识别上取的非常大成功</p>
<p>Adaboost分类器优点在于前一个基本分类器分出的样本，在下一个分类器中会得到加权。<br>
加权后的全体样本再次被用于训练下一个基本分类器。</p>
<p>我们有 苹果 苹果 苹果 香蕉 找出苹果</p>
<p>第一次训练权值分别为: 0.1 0.1 0.1 0.5</p>
<p>因为香蕉是我们不需要的所以0.5，我们正确的样本系数减小，负样本得到加强。</p>
<p>把出错的样本进行加强权值。再把整个结果送到下一个基本分类器</p>
<ul>
<li>训练终止条件：1. for迭代的最大次数 count 2. 每次迭代完的检测概率p</li>
</ul>
<ol>
<li>分类器的结构 2. Adaboost分类器计算过程 3. opencv自带的人脸识别的Adaboost分类器的文件结构(xml)</li>
</ol>
<p>haar特征计算完之后要进行阈值判决，实际是一个个判决过程。</p>
<p>if(haar&gt; T1) 苹果[第一级分类器]；[第二级分类器]and haar&gt;T2</p>
<p>两级都达成了，才会被我们认定为苹果。</p>
<blockquote>
<p>两级分类器的阈值分别是t1 和 t2，对于每一级的分类器我们称之为强分类器。</p>
</blockquote>
<p>2个强分类器组成。一般有15-20强分类器。 要连续满足15-20个强分类器才能认证为目标。</p>
<p>分类器的结构:</p>
<div class=\"image-package\">
<img src=\"http://myphoto.mtianyan.cn/blog/180327/ClD85KDG1K.png?imageslim\" data-original-src=\"http://myphoto.mtianyan.cn/blog/180327/ClD85KDG1K.png?imageslim\" alt=\"mark\"><div class=\"image-caption\">mark</div>
</div>
<p>3个强分类器,每个强分类器会计算出一个独立的特征点。使用每一个独立的特征进行阈值判。</p>
<blockquote>
<p>强分类器1 特征x1 阈值t1 强分类器2 特征x2 阈值t2 3同理</p>
</blockquote>
<p>进行判决过程: x1&gt;t1 and x2&gt;t2 and x3&gt;t3</p>
<p>三个判决同时达成，目标-》苹果</p>
<p>三个强分类器只要有一个没通过就会被判定为非苹果。作用：判决。</p>
<p>每个强分类器由若干个弱分类器组成</p>
<p>1强 对多个弱分类器 对应多个特征节点</p>
<p>弱分类器作用：计算强分类器特征x1 x2 x3</p>
<p>强分类器的输入特征是多个弱分类器输出特征的综合处理。</p>
<blockquote>
<p>x2 = sum（y1,y2,y3）</p>
</blockquote>
<p>y1 弱分类器特征谁来计算的？</p>
<blockquote>
<p>node节点来计算</p>
</blockquote>
<p>一个弱分类器最多支持三个haar特征，每个haar特征构成一个node节点</p>
<ul>
<li>3个haar -》 3个node</li>
</ul>
<pre><code>node1 haar1 &gt; node的阈值T1  z1 = a1
node1 haar1 &lt; node的阈值T1  z1 = a2
Z = sum(z1,z2,z3) &gt; 弱分类器T y1 = AA
Z = sum(z1,z2,z3) &lt; 弱分类器T y1 = BB
</code></pre>
<h3>从node向强分类器</h3>
<p>haar-&gt;Node z1 z2 z3 弱分类器Z=sum(z1,z2,z3)</p>
<p>Z&gt;T y1 y2 y3: 弱分类器的计算特征</p>
<p>第三层强分类器: x = sum(y1,y2,y3) &gt; T1 obj</p>

          </div>','1531183756'),
('325599','{975714}{1215}{884}{975703}{621}{87910}{7187}{408}{503}{1145}{1246}{727}{1124519}{6090}{424}{2226}{356}{975768}{2233}{29114}{4697}{517}{975878}{975951}{1551}{1580}{975835}{2089}{975721}{946}{975767}{854}{975740}{296}{2240}{785}{975757}{26}{1188}{3971}{164323}{6216}{1021321}{3571}{650}{562}{2241}{6386}{975926}{15575}','(image-ab4eb3-1526290967990)] 我们需要调节2个 alpha 参数，一大一小，使得为0，满足条件后求得 alpha 较大值，然后慢慢调节，再得到一个较大值，这样的按照序列求得最大的优化方法叫 SMO 掉个方向，得到 image 举个例子看的明白些： 给定3个数据点，正例点x1=(3,3)T,x2=(4,3)T,反例点 x3=(1,1)T, 求解线性可分的SVM。 image 解：目标函数是 image [图片上传失败.','通俗易懂的支持向量机SVM','<div class=\"show-content-free\">
            <h2>SVM 的原理和目标</h2>
<h3>几个基本概念</h3>
<p>线性可分SVM——线性 SVM——非线性 SVM<br>
1、线性可分SVM，表示可以用一根线非常清晰的划分两个区域；线到支持向量的距离 d 就是最小的。</p>
<p>2、线性 SVM，表示用一根线划分区域后，可能存在误判点，但还是线性的；线到支持向量的距离不一定是最小的，但忽略其他非规则的支持向量。</p>
<p>3、非线性 SVM，表示使用核函数之后，把低维的非线性转换为高维线性</p>
<hr>
<h3>复习下函数和向量</h3>
<p>假如有个方程</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 658px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 77.96%;\"></div>
<div class=\"image-view\" data-width=\"844\" data-height=\"658\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-86ddb67c4a9161f1.jpg\" data-original-width=\"844\" data-original-height=\"658\" data-original-format=\"image/jpeg\" data-original-filesize=\"43091\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<br><p>y=x/2-1可以变化为 -x+2y+2=0<br>
f(x,y)=-x+2y+2,其中红色的就是他的法向量<br>
写成向量的形式：</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 506px; max-height: 140px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 27.67%;\"></div>
<div class=\"image-view\" data-width=\"506\" data-height=\"140\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-47bd3a334ed909a6.jpg\" data-original-width=\"506\" data-original-height=\"140\" data-original-format=\"image/jpeg\" data-original-filesize=\"5903\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<br>
改写下<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 470px; max-height: 154px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 32.769999999999996%;\"></div>
<div class=\"image-view\" data-width=\"470\" data-height=\"154\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-968e3f90b6d28db8.jpg\" data-original-width=\"470\" data-original-height=\"154\" data-original-format=\"image/jpeg\" data-original-filesize=\"7209\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<br><p></p>
前面的系数项可以令其为<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 24px; max-height: 23px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 95.83%;\"></div>
<div class=\"image-view\" data-width=\"24\" data-height=\"23\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-dd4c476bfc533ef5.jpg\" data-original-width=\"24\" data-original-height=\"23\" data-original-format=\"image/jpeg\" data-original-filesize=\"697\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<br>
x，y 的那一项，其实可以演变为 n行，所以可以简化为<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 20px; max-height: 23px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 114.99999999999999%;\"></div>
<div class=\"image-view\" data-width=\"20\" data-height=\"23\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-75c84fdc23594d0b.jpg\" data-original-width=\"20\" data-original-height=\"23\" data-original-format=\"image/jpeg\" data-original-filesize=\"659\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<br>
后面的2是常数项，用 b 代替<br>
这样处理完，我们的式子可以修改为<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 205px; max-height: 36px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 17.560000000000002%;\"></div>
<div class=\"image-view\" data-width=\"205\" data-height=\"36\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-906fe03b985a46f7.jpg\" data-original-width=\"205\" data-original-height=\"36\" data-original-format=\"image/jpeg\" data-original-filesize=\"3400\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<blockquote>
<p>其中 w 就是我们的法线方向，x 是我们的参数，b 是截距项。</p>
</blockquote>
<p></p>
<p></p>
此时如果有个 <div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 28px; max-height: 19px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 67.86%;\"></div>
<div class=\"image-view\" data-width=\"28\" data-height=\"19\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-4ad7b59a49a0237f.jpg\" data-original-width=\"28\" data-original-height=\"19\" data-original-format=\"image/jpeg\" data-original-filesize=\"776\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>带入式子得到是正值，那么就在法线的同方向；否则在法线的逆方向。如果等于0，那么该点就在线上。</p>

<p>f(x1，x2)代表一个线，f(x1,x2,x3)代表一个面，如果是 n 维的，那么给他一个牛逼的名称“超平面”，由于这个名字太牛逼，所以低维的也叫超平面。</p>
<h3>计算过程和算法步骤</h3>
<p>接入给定一个特征空间上的训练数据集，</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 293px; max-height: 23px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 7.85%;\"></div>
<div class=\"image-view\" data-width=\"293\" data-height=\"23\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-0038027a068293fc.jpg\" data-original-width=\"293\" data-original-height=\"23\" data-original-format=\"image/jpeg\" data-original-filesize=\"4387\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<br><p>其中</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 71px; max-height: 20px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 28.17%;\"></div>
<div class=\"image-view\" data-width=\"71\" data-height=\"20\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-2dc25ae6bfd6594e.jpg\" data-original-width=\"71\" data-original-height=\"20\" data-original-format=\"image/jpeg\" data-original-filesize=\"1152\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 129px; max-height: 22px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 17.05%;\"></div>
<div class=\"image-view\" data-width=\"129\" data-height=\"22\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-777fcea6a45bbfc3.jpg\" data-original-width=\"129\" data-original-height=\"22\" data-original-format=\"image/jpeg\" data-original-filesize=\"2164\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>这里为什么 y 取值+1和-1？<br>
因为这样取值方便后面的推导，如果非要把+1和-1换为+3和-8什么的，其实没有什么不同。后面推导不受影响。<br></p>
<p></p>
写成+1和-1的话，那么我们的<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 58px; max-height: 25px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 43.1%;\"></div>
<div class=\"image-view\" data-width=\"58\" data-height=\"25\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-c97662ec6a8c6195.jpg\" data-original-width=\"58\" data-original-height=\"25\" data-original-format=\"image/jpeg\" data-original-filesize=\"1011\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>,那么<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 64px; max-height: 49px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 76.55999999999999%;\"></div>
<div class=\"image-view\" data-width=\"64\" data-height=\"49\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-fc73c0efb68c2d7d.jpg\" data-original-width=\"64\" data-original-height=\"49\" data-original-format=\"image/jpeg\" data-original-filesize=\"742\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>，两边同时乘以 yj，得到<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 85px; max-height: 43px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 50.59%;\"></div>
<div class=\"image-view\" data-width=\"85\" data-height=\"43\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-d24ab178f783358d.jpg\" data-original-width=\"85\" data-original-height=\"43\" data-original-format=\"image/jpeg\" data-original-filesize=\"952\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>，这样我们就可以很方便的得到两个标记相乘等于两个标记相除。如果选择不同的值，就得不到上面的式子。</p>

<h3>推导目标函数</h3>
<p>分割平面：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 488px; max-height: 122px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 25.0%;\"></div>
<div class=\"image-view\" data-width=\"488\" data-height=\"122\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-2383ccd13270d236.jpg\" data-original-width=\"488\" data-original-height=\"122\" data-original-format=\"image/jpeg\" data-original-filesize=\"4566\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<br>
训练集：x1，x2，x3......<br>
目标集：y1，y2,y3...... y属于1和-1的集合<br>
新数据的分类：sign(y(x))<br>
根据题设<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 488px; max-height: 122px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 25.0%;\"></div>
<div class=\"image-view\" data-width=\"488\" data-height=\"122\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-e3b9d811bbcb5ae2.jpg\" data-original-width=\"488\" data-original-height=\"122\" data-original-format=\"image/jpeg\" data-original-filesize=\"4566\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<br>
其中<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 41px; max-height: 23px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 56.10000000000001%;\"></div>
<div class=\"image-view\" data-width=\"41\" data-height=\"23\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-fd1c7e0088771c1f.jpg\" data-original-width=\"41\" data-original-height=\"23\" data-original-format=\"image/jpeg\" data-original-filesize=\"620\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>就是一个映射，<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 84px; max-height: 23px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 27.38%;\"></div>
<div class=\"image-view\" data-width=\"84\" data-height=\"23\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-d08abcfa38f9a969.jpg\" data-original-width=\"84\" data-original-height=\"23\" data-original-format=\"image/jpeg\" data-original-filesize=\"772\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>就是原封不动一阶映射，二阶映射的例子如下：<br>
{1，x1，x2,x3}==&gt;{1,x1,x2,x3,x1<sup>2,x2</sup>2,x3^2,x1x2,x2x3,x1x3}把原来的四维转换成10维。<br>
接下来就有<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 176px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 20.849999999999998%;\"></div>
<div class=\"image-view\" data-width=\"844\" data-height=\"176\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-69a237074f21cc25.jpg\" data-original-width=\"844\" data-original-height=\"176\" data-original-format=\"image/jpeg\" data-original-filesize=\"9284\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<br>
其中：y(x)为预测值，y 为真实值，他们互为推导。最终他们的乘积大于0.<br>
从而可以得到下面的公式（其实是点到直线的距离）<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 684px; max-height: 200px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 29.24%;\"></div>
<div class=\"image-view\" data-width=\"684\" data-height=\"200\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-82a34cfed942c600.jpg\" data-original-width=\"684\" data-original-height=\"200\" data-original-format=\"image/jpeg\" data-original-filesize=\"8466\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<br>
我们的目标函数：<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 166px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 20.65%;\"></div>
<div class=\"image-view\" data-width=\"804\" data-height=\"166\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-b54511374e9bea0c.jpg\" data-original-width=\"804\" data-original-height=\"166\" data-original-format=\"image/jpeg\" data-original-filesize=\"8465\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<br>
解释：对于所有的样本到平面的距离求最近，也就是最小值，然后得到的这些平面里面根据 w 和 b再求 最大值。<p></p>
<h3>如何优化</h3>
<p>转换等价问题，直接来解决的确棘手。先来看<br>
线性可分的这个图</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 478px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 68.08999999999999%;\"></div>
<div class=\"image-view\" data-width=\"702\" data-height=\"478\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-b1284414e1344684.jpg\" data-original-width=\"702\" data-original-height=\"478\" data-original-format=\"image/jpeg\" data-original-filesize=\"51400\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<br><p>可以看出，有3个支持向量。<br></p>
上虚线五角星的那个点到红线的距离是一个值，而且是最近的一个距离，假如该距离为 d=3，那么也就是<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 193px; max-height: 54px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 27.98%;\"></div>
<div class=\"image-view\" data-width=\"193\" data-height=\"54\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-3f0f925014d43e24.jpg\" data-original-width=\"193\" data-original-height=\"54\" data-original-format=\"image/jpeg\" data-original-filesize=\"1878\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<br>
两边同时除以3<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 192px; max-height: 54px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 28.13%;\"></div>
<div class=\"image-view\" data-width=\"192\" data-height=\"54\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-f10aa57c2a3a9666.jpg\" data-original-width=\"192\" data-original-height=\"54\" data-original-format=\"image/jpeg\" data-original-filesize=\"1919\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<br>
也就是范数 w 乘以3，但是对整个方程来说位置是不发生变化的。（其实 w 是我们要求的，可以随便设置，总能得到右边为1）<br><p></p>
打个比方，假如有个方程 f(x,y)=ax+by+c=0,有个点到该线的距离为d，那么我们可以除以 d，使得到改线的距离为1，可以认为是距离归一处理。那么同理我们总能得到一个 w，使得五角星那个点到红线的距离为1，就得有个约束条件就是<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 420px; max-height: 76px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 18.099999999999998%;\"></div>
<div class=\"image-view\" data-width=\"420\" data-height=\"76\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-fe46fc694f40f4f4.jpg\" data-original-width=\"420\" data-original-height=\"76\" data-original-format=\"image/jpeg\" data-original-filesize=\"3434\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>，好了，现在这个值要求取最小值，而且又是大于等于1，那么就直接取1就是最小值。<br>
那么新的目标函数就变化为<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 278px; max-height: 138px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 49.64%;\"></div>
<div class=\"image-view\" data-width=\"278\" data-height=\"138\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-aa7babb6d2987ac3.jpg\" data-original-width=\"278\" data-original-height=\"138\" data-original-format=\"image/jpeg\" data-original-filesize=\"3020\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<br>
一个数的倒数求最大，也就是这个数求最小，也就是求这个数的平方最小，也就是求这个数平方的一半最小，好了，公式继续演变为：<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 222px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 27.07%;\"></div>
<div class=\"image-view\" data-width=\"820\" data-height=\"222\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-466ee7eb66f5d3e3.jpg\" data-original-width=\"820\" data-original-height=\"222\" data-original-format=\"image/jpeg\" data-original-filesize=\"8631\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>这个就是我们需要求解的目标，假如 n=10000（n 是样本个数），也就是有1万个下面的约束条件下求上式子的最小值，对于这个目标函数如何来求呢？带有约束条件的求极值问题，就想到拉格朗日乘子法。</p>
<h3>拉格朗日乘子法和KKT条件</h3>
<p>其实拉格朗日乘子法的约束条件是要求等于某个条件，而这里是不等于，也可以这么做。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 134px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 13.79%;\"></div>
<div class=\"image-view\" data-width=\"972\" data-height=\"134\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-9dbe49591f840e51.jpg\" data-original-width=\"972\" data-original-height=\"134\" data-original-format=\"image/jpeg\" data-original-filesize=\"8728\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<br><p>这个式子的暂且放一边，我们来梳理下拉格朗日乘子法。</p>

<hr>
<p>假设有个函数要求最小值，min f(x)，其中有两部分要求，第一部分是不等式部分：</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 86px; max-height: 23px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 26.740000000000002%;\"></div>
<div class=\"image-view\" data-width=\"86\" data-height=\"23\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-b030f386e6b1197c.jpg\" data-original-width=\"86\" data-original-height=\"23\" data-original-format=\"image/jpeg\" data-original-filesize=\"906\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 86px; max-height: 23px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 26.740000000000002%;\"></div>
<div class=\"image-view\" data-width=\"86\" data-height=\"23\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-c273345d5c36eee8.jpg\" data-original-width=\"86\" data-original-height=\"23\" data-original-format=\"image/jpeg\" data-original-filesize=\"918\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 88px; max-height: 23px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 26.14%;\"></div>
<div class=\"image-view\" data-width=\"88\" data-height=\"23\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-6d4134da8e25398f.jpg\" data-original-width=\"88\" data-original-height=\"23\" data-original-format=\"image/jpeg\" data-original-filesize=\"864\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<br><p>这里所有条件的方向可以修改为</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 32px; max-height: 18px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 56.25%;\"></div>
<div class=\"image-view\" data-width=\"32\" data-height=\"18\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-ff8507b508340785.jpg\" data-original-width=\"32\" data-original-height=\"18\" data-original-format=\"image/jpeg\" data-original-filesize=\"517\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>，因为可以增加负号来改变方向，不是什么大问题。<br>
最终也就是<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 84px; max-height: 23px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 27.38%;\"></div>
<div class=\"image-view\" data-width=\"84\" data-height=\"23\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-795919d31df181f1.jpg\" data-original-width=\"84\" data-original-height=\"23\" data-original-format=\"image/jpeg\" data-original-filesize=\"894\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<br>
第二部分是等式部分，记为<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 87px; max-height: 24px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 27.589999999999996%;\"></div>
<div class=\"image-view\" data-width=\"87\" data-height=\"24\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-673308dc9e9d4666.jpg\" data-original-width=\"87\" data-original-height=\"24\" data-original-format=\"image/jpeg\" data-original-filesize=\"785\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>本科阶段的拉格朗日乘子法只有等式部分，现在这里增加了不等式部分，针对不等式部分，做如下操作。<br></p>
<p></p>
先设置一系列的<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 55px; max-height: 19px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 34.55%;\"></div>
<div class=\"image-view\" data-width=\"55\" data-height=\"19\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-0ff00cb538881a58.jpg\" data-original-width=\"55\" data-original-height=\"19\" data-original-format=\"image/jpeg\" data-original-filesize=\"622\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>，构造一个新的函数<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 409px; max-height: 63px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 15.4%;\"></div>
<div class=\"image-view\" data-width=\"409\" data-height=\"63\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-fa7925c325b10a50.jpg\" data-original-width=\"409\" data-original-height=\"63\" data-original-format=\"image/jpeg\" data-original-filesize=\"3381\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<br><p></p>
这个就是拉格朗日函数。对于这个函数来说，以<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 17px; max-height: 15px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 88.24%;\"></div>
<div class=\"image-view\" data-width=\"17\" data-height=\"15\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-c7a867b1a66f636f.jpg\" data-original-width=\"17\" data-original-height=\"15\" data-original-format=\"image/jpeg\" data-original-filesize=\"386\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>w为变量，对其求导数，可得系数为<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 43px; max-height: 23px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 53.49%;\"></div>
<div class=\"image-view\" data-width=\"43\" data-height=\"23\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-c3c2e08bae342854.jpg\" data-original-width=\"43\" data-original-height=\"23\" data-original-format=\"image/jpeg\" data-original-filesize=\"637\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>,在二维坐标上就是一条线，同理<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 17px; max-height: 14px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 82.35%;\"></div>
<div class=\"image-view\" data-width=\"17\" data-height=\"14\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-8a9ca5891d000855.jpg\" data-original-width=\"17\" data-original-height=\"14\" data-original-format=\"image/jpeg\" data-original-filesize=\"407\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 17px; max-height: 14px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 82.35%;\"></div>
<div class=\"image-view\" data-width=\"17\" data-height=\"14\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-b65dd36bfcdf9a6e.jpg\" data-original-width=\"17\" data-original-height=\"14\" data-original-format=\"image/jpeg\" data-original-filesize=\"413\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>等。<br>
将他们画在一张坐标系里面，<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 516px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 73.76%;\"></div>
<div class=\"image-view\" data-width=\"1006\" data-height=\"742\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-9fadf82d6dae3cbc.jpg\" data-original-width=\"1006\" data-original-height=\"742\" data-original-format=\"image/jpeg\" data-original-filesize=\"27215\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<br><p>对于这3个线段求取每个 x 点的最小值，得到的是红色的这个区域的线，同理，n 条这样的线也可以得到这样一个凹函数，凹函数必有最大值，并且是全局最大。<br></p>
那么<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 131px; max-height: 28px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 21.37%;\"></div>
<div class=\"image-view\" data-width=\"131\" data-height=\"28\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-de2eeea3ce02cc75.jpg\" data-original-width=\"131\" data-original-height=\"28\" data-original-format=\"image/jpeg\" data-original-filesize=\"1205\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>到底是什么？
<p>继续看下</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 409px; max-height: 63px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 15.4%;\"></div>
<div class=\"image-view\" data-width=\"409\" data-height=\"63\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-33700ee20ff2bb62.jpg\" data-original-width=\"409\" data-original-height=\"63\" data-original-format=\"image/jpeg\" data-original-filesize=\"3381\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 220px; max-height: 23px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 10.45%;\"></div>
<div class=\"image-view\" data-width=\"220\" data-height=\"23\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-f19d3b5ccb1d8f34.jpg\" data-original-width=\"220\" data-original-height=\"23\" data-original-format=\"image/jpeg\" data-original-filesize=\"1675\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>因为</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 55px; max-height: 19px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 34.55%;\"></div>
<div class=\"image-view\" data-width=\"55\" data-height=\"19\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-6579f9ba00ef235a.jpg\" data-original-width=\"55\" data-original-height=\"19\" data-original-format=\"image/jpeg\" data-original-filesize=\"622\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p></p>
而<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 84px; max-height: 23px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 27.38%;\"></div>
<div class=\"image-view\" data-width=\"84\" data-height=\"23\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-5910ec43e0c38dbc.jpg\" data-original-width=\"84\" data-original-height=\"23\" data-original-format=\"image/jpeg\" data-original-filesize=\"894\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>，所以<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 135px; max-height: 61px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 45.190000000000005%;\"></div>
<div class=\"image-view\" data-width=\"135\" data-height=\"61\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-9a10d3ea114a7ce9.jpg\" data-original-width=\"135\" data-original-height=\"61\" data-original-format=\"image/jpeg\" data-original-filesize=\"1435\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<br><p></p>
因为<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 87px; max-height: 24px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 27.589999999999996%;\"></div>
<div class=\"image-view\" data-width=\"87\" data-height=\"24\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-aac75bf43a646cc2.jpg\" data-original-width=\"87\" data-original-height=\"24\" data-original-format=\"image/jpeg\" data-original-filesize=\"785\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>，所以无论 lambda 取什么值，<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 139px; max-height: 63px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 45.32%;\"></div>
<div class=\"image-view\" data-width=\"139\" data-height=\"63\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-bd74ffccea013d04.jpg\" data-original-width=\"139\" data-original-height=\"63\" data-original-format=\"image/jpeg\" data-original-filesize=\"1475\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<br>
,这就是说 G(x)是一个 f(x)加上一个负数，那么也就是说 G(x)的最大也就是 f(x)了。<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 201px; max-height: 28px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 13.930000000000001%;\"></div>
<div class=\"image-view\" data-width=\"201\" data-height=\"28\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-19300d3b1b4e1db1.jpg\" data-original-width=\"201\" data-original-height=\"28\" data-original-format=\"image/jpeg\" data-original-filesize=\"1602\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>，那么要求 minf(x)也就是求 <div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 170px; max-height: 28px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 16.470000000000002%;\"></div>
<div class=\"image-view\" data-width=\"170\" data-height=\"28\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-3d772569161d331d.jpg\" data-original-width=\"170\" data-original-height=\"28\" data-original-format=\"image/jpeg\" data-original-filesize=\"1439\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<h2>可以找到他的对偶函数<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 170px; max-height: 28px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 16.470000000000002%;\"></div>
<div class=\"image-view\" data-width=\"170\" data-height=\"28\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-7011c8a75f109d72.jpg\" data-original-width=\"170\" data-original-height=\"28\" data-original-format=\"image/jpeg\" data-original-filesize=\"1437\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
</h2>
<p>再回到我们原来的式子，</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 134px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 13.79%;\"></div>
<div class=\"image-view\" data-width=\"972\" data-height=\"134\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-cabd6529fda5afa3.jpg\" data-original-width=\"972\" data-original-height=\"134\" data-original-format=\"image/jpeg\" data-original-filesize=\"8728\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<br><p></p>
这个式子比上面的理论要简单点，没有等式约束。那么他的原始极小极大问题<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 550px; max-height: 150px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 27.27%;\"></div>
<div class=\"image-view\" data-width=\"550\" data-height=\"150\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-dab233f34e064bc1.jpg\" data-original-width=\"550\" data-original-height=\"150\" data-original-format=\"image/jpeg\" data-original-filesize=\"5873\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>他的对偶问题为极大极小问题<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 536px; max-height: 112px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 20.9%;\"></div>
<div class=\"image-view\" data-width=\"536\" data-height=\"112\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-317fcc5efaa1da74.jpg\" data-original-width=\"536\" data-original-height=\"112\" data-original-format=\"image/jpeg\" data-original-filesize=\"5412\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>为了求最小值，我们需要对w 和 b 分别求导，并且使之为0，<br>
其中</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 126px; max-height: 37px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 29.37%;\"></div>
<div class=\"image-view\" data-width=\"126\" data-height=\"37\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-04fa4787ae6b91d9.jpg\" data-original-width=\"126\" data-original-height=\"37\" data-original-format=\"image/jpeg\" data-original-filesize=\"6360\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<br><p>最终得到</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 220px; max-height: 51px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 23.18%;\"></div>
<div class=\"image-view\" data-width=\"220\" data-height=\"51\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-61a4d64023eb2107.jpg\" data-original-width=\"220\" data-original-height=\"51\" data-original-format=\"image/jpeg\" data-original-filesize=\"8419\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<br>
也就是<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 139px; max-height: 51px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 36.69%;\"></div>
<div class=\"image-view\" data-width=\"139\" data-height=\"51\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-c6eef3b9736b3f34.jpg\" data-original-width=\"139\" data-original-height=\"51\" data-original-format=\"image/jpeg\" data-original-filesize=\"7084\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<br>
接下来，对 b 求偏导数，得到<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 141px; max-height: 51px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 36.17%;\"></div>
<div class=\"image-view\" data-width=\"141\" data-height=\"51\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-988cf72d399f862f.jpg\" data-original-width=\"141\" data-original-height=\"51\" data-original-format=\"image/jpeg\" data-original-filesize=\"7437\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>w 不就是法向量嘛，其中包含的 alpha=0的点不是支撑向量，而 alpha 不为0 的才是 SV。pha(x）就是核函数，所有的这些在这里可以体现出来了。<br>
归纳下：</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 296px; max-height: 246px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 83.11%;\"></div>
<div class=\"image-view\" data-width=\"296\" data-height=\"246\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-de07e97e5917e666.jpg\" data-original-width=\"296\" data-original-height=\"246\" data-original-format=\"image/jpeg\" data-original-filesize=\"18869\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>继续推导。。。带入 w 和 b，剩下 alpha 的式子。</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 546px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 73.58%;\"></div>
<div class=\"image-view\" data-width=\"742\" data-height=\"546\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-e0cc7674d275beed.jpg\" data-original-width=\"742\" data-original-height=\"546\" data-original-format=\"image/jpeg\" data-original-filesize=\"77939\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>最后得到求 alpha 的式子</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 118px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 13.44%;\"></div>
<div class=\"image-view\" data-width=\"878\" data-height=\"118\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-7d05b3968b52722e.jpg\" data-original-width=\"878\" data-original-height=\"118\" data-original-format=\"image/jpeg\" data-original-filesize=\"24411\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<br><p>使用某种方法（SMO）求得 alpha，带入 w 和 b 就可以求得 w。</p>

<p>SMO 算法，由条件可知，<br>
[图片上传失败...(image-ab4eb3-1526290967990)]<br>
我们需要调节2个 alpha 参数，一大一小，使得为0，满足条件后求得 alpha 较大值，然后慢慢调节，再得到一个较大值，这样的按照序列求得最大的优化方法叫 SMO</p>
<p>掉个方向，得到</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 426px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 40.57%;\"></div>
<div class=\"image-view\" data-width=\"1050\" data-height=\"426\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-41a0d7e76c17f6bf.jpg\" data-original-width=\"1050\" data-original-height=\"426\" data-original-format=\"image/jpeg\" data-original-filesize=\"55225\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>举个例子看的明白些：<br>
给定3个数据点，正例点x1=(3,3)T,x2=(4,3)T,反例点 x3=(1,1)T,  求解线性可分的SVM。</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 356px; max-height: 256px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 71.91%;\"></div>
<div class=\"image-view\" data-width=\"356\" data-height=\"256\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-cadf8586ce7b9fc8.jpg\" data-original-width=\"356\" data-original-height=\"256\" data-original-format=\"image/jpeg\" data-original-filesize=\"16117\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>解：目标函数是</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 694px; max-height: 118px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 17.0%;\"></div>
<div class=\"image-view\" data-width=\"694\" data-height=\"118\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-c114567fa38de2f7.jpg\" data-original-width=\"694\" data-original-height=\"118\" data-original-format=\"image/jpeg\" data-original-filesize=\"19577\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<br><p>[图片上传失败...(image-fbc4d6-1526290967990)]<br>
根据</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 141px; max-height: 51px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 36.17%;\"></div>
<div class=\"image-view\" data-width=\"141\" data-height=\"51\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-bb3f9d1cff10747b.jpg\" data-original-width=\"141\" data-original-height=\"51\" data-original-format=\"image/jpeg\" data-original-filesize=\"7437\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<br>
可得<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 384px; max-height: 144px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 37.5%;\"></div>
<div class=\"image-view\" data-width=\"384\" data-height=\"144\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-406dfe772c28dd1c.jpg\" data-original-width=\"384\" data-original-height=\"144\" data-original-format=\"image/jpeg\" data-original-filesize=\"13503\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<br><p>其中正例就是 x1=(3,3,1) x2=(4,3,1);负例x3=(1,1,-1)<br>
最后是表示 y 的值。x1对应的系数 alpha1，x2对应的系数是 alpha2，x3对应的系数是 alpha3.</p>

<p>将 alpha3=alpha1+alpha2带入式子计算得到关羽 alpha1和 alpha2 的han 函数</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 112px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 13.79%;\"></div>
<div class=\"image-view\" data-width=\"812\" data-height=\"112\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-68b2653aa60fc4fe.jpg\" data-original-width=\"812\" data-original-height=\"112\" data-original-format=\"image/jpeg\" data-original-filesize=\"19355\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>对 alpha1和 alpha2求偏导令他为0，得到在（1.5，-1）这个点取到极值，但是改点不满足 alpha2&gt;0这个条件，所以最小值在边界上没有达到。<br>
令 alpha1=0，最小值 s（0，2/13）=-2/13=-0.1538<br>
令 alpha2=0，最小值 s（1/4，0）=-1/4=-0.25<br>
所以得到s(alpha1,alpha2)在1/4,0处达到最小，此时 alpha3=alpha1+alpha2=0.25<br>
所以 alpha1=alpha3=0.25对应点 x1和 x3构成支持向量。<br>
带入公式：</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 480px; max-height: 222px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 46.25%;\"></div>
<div class=\"image-view\" data-width=\"480\" data-height=\"222\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-017cb0f0034ec62b.jpg\" data-original-width=\"480\" data-original-height=\"222\" data-original-format=\"image/jpeg\" data-original-filesize=\"20340\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<br><p>这里 fai(xi)就是 xi</p>

<p>得到 w1=w2=0.5 b=-2<br>
所以超平面为</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 334px; max-height: 120px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 35.93%;\"></div>
<div class=\"image-view\" data-width=\"334\" data-height=\"120\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-fe3493f20dc8e279.jpg\" data-original-width=\"334\" data-original-height=\"120\" data-original-format=\"image/jpeg\" data-original-filesize=\"9312\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<br><p>图形</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 486px; max-height: 364px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 74.9%;\"></div>
<div class=\"image-view\" data-width=\"486\" data-height=\"364\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-068673b92c033d4c.jpg\" data-original-width=\"486\" data-original-height=\"364\" data-original-format=\"image/jpeg\" data-original-filesize=\"24379\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>x1的系数和x3的系数是0.25，而x2的系数是0，也就是说 x2没有参与支持向量的构建，不是支持向量。</p>
<p>以上是线性可分，那么线性不一定可分的咋办呢？<br>
如图</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 540px; max-height: 510px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 94.44%;\"></div>
<div class=\"image-view\" data-width=\"540\" data-height=\"510\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-5fd12e44f8b19874.jpg\" data-original-width=\"540\" data-original-height=\"510\" data-original-format=\"image/jpeg\" data-original-filesize=\"30969\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<br><p>按照可分那就是实线，貌似虚线应该更好。<br>
若线性不可分，需要加入松弛因子[图片上传失败...(image-8be0f1-1526290967990)]&gt;=0，<br>
此时约束条件变成：</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 159px; max-height: 18px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 11.32%;\"></div>
<div class=\"image-view\" data-width=\"159\" data-height=\"18\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-5d4f82f56d43e843.jpg\" data-original-width=\"159\" data-original-height=\"18\" data-original-format=\"image/jpeg\" data-original-filesize=\"6690\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>当<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 9px; max-height: 16px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 177.78%;\"></div>
<div class=\"image-view\" data-width=\"9\" data-height=\"16\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-ba920647898f9d1a.jpg\" data-original-width=\"9\" data-original-height=\"16\" data-original-format=\"image/jpeg\" data-original-filesize=\"4710\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>=0就是线性可分。<br>
目标函数：<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 480px; max-height: 152px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 31.669999999999998%;\"></div>
<div class=\"image-view\" data-width=\"480\" data-height=\"152\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-fed5e2f6119cdd5c.jpg\" data-original-width=\"480\" data-original-height=\"152\" data-original-format=\"image/jpeg\" data-original-filesize=\"16698\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<br><p></p>
这个式子中，当 C 趋向无穷大的时候，<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 9px; max-height: 16px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 177.78%;\"></div>
<div class=\"image-view\" data-width=\"9\" data-height=\"16\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-a74daef0c3db1523.jpg\" data-original-width=\"9\" data-original-height=\"16\" data-original-format=\"image/jpeg\" data-original-filesize=\"4710\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>只能取0才能保证能取最小值。C 可以认为是允许犯错误的容忍程度，C 太大，表示对错误完全不能容忍，必须要线性可分。从另外角度来说，C 太大，那么不允许犯错误，那么过渡带就会窄。C 小些的话，过渡带会宽些，也就是泛化能力较强。
<p>归纳下：</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 380px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 37.55%;\"></div>
<div class=\"image-view\" data-width=\"1012\" data-height=\"380\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-83fa5fc076bbfca9.jpg\" data-original-width=\"1012\" data-original-height=\"380\" data-original-format=\"image/jpeg\" data-original-filesize=\"50789\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>对应的拉格朗日函数（注意符号，ξ&gt;=0的变化）</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 100px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 10.18%;\"></div>
<div class=\"image-view\" data-width=\"982\" data-height=\"100\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-64ef6cfaf923a759.jpg\" data-original-width=\"982\" data-original-height=\"100\" data-original-format=\"image/jpeg\" data-original-filesize=\"22596\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>然后对其求偏导数</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 518px; max-height: 396px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 76.44999999999999%;\"></div>
<div class=\"image-view\" data-width=\"518\" data-height=\"396\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-e6986926b17b0f49.jpg\" data-original-width=\"518\" data-original-height=\"396\" data-original-format=\"image/jpeg\" data-original-filesize=\"35005\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<br><p>将三个式子带入</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 118px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 13.26%;\"></div>
<div class=\"image-view\" data-width=\"890\" data-height=\"118\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-9434f70f586408e0.jpg\" data-original-width=\"890\" data-original-height=\"118\" data-original-format=\"image/jpeg\" data-original-filesize=\"24139\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>把 μ消掉了<br>
对上式子求关羽 α 的极大，得到<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 422px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 45.379999999999995%;\"></div>
<div class=\"image-view\" data-width=\"930\" data-height=\"422\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-2eb82fb79263a1b9.jpg\" data-original-width=\"930\" data-original-height=\"422\" data-original-format=\"image/jpeg\" data-original-filesize=\"46602\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<br><p>上式右下角的条件限制了α的取值范围是缩小了的。</p>

<p>进一步整理得到对偶问题</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 372px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 51.519999999999996%;\"></div>
<div class=\"image-view\" data-width=\"722\" data-height=\"372\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-09c35a98800b6b1f.jpg\" data-original-width=\"722\" data-original-height=\"372\" data-original-format=\"image/jpeg\" data-original-filesize=\"39448\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<br><p>再进一步构造最优化问题</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 424px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 46.489999999999995%;\"></div>
<div class=\"image-view\" data-width=\"912\" data-height=\"424\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-7fa0169dfc853ff6.jpg\" data-original-width=\"912\" data-original-height=\"424\" data-original-format=\"image/jpeg\" data-original-filesize=\"52867\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<br>
求解得 a^*<br>
然后计算出w 和 b<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 578px; max-height: 286px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 49.480000000000004%;\"></div>
<div class=\"image-view\" data-width=\"578\" data-height=\"286\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-5ee55fa0eae982a2.jpg\" data-original-width=\"578\" data-original-height=\"286\" data-original-format=\"image/jpeg\" data-original-filesize=\"23300\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<br><p>注意：计算 b 的时候需要满足α在（0，C）之间</p>

<p></p>
<p></p>
最终求得超平面：<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 354px; max-height: 86px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 24.29%;\"></div>
<div class=\"image-view\" data-width=\"354\" data-height=\"86\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-7eccce5000166215.jpg\" data-original-width=\"354\" data-original-height=\"86\" data-original-format=\"image/jpeg\" data-original-filesize=\"9505\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<h3>损失函数分析</h3>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 674px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 73.74000000000001%;\"></div>
<div class=\"image-view\" data-width=\"914\" data-height=\"674\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-ee26f09d854282b5.jpg\" data-original-width=\"914\" data-original-height=\"674\" data-original-format=\"image/jpeg\" data-original-filesize=\"89853\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<br><p>圆圈的点：他们的α=C，他们是有损失的，并且哪些红色线段表示损失值；虽然他们分的是对的。但是已经进入敏感地带。<br>
方块的星：他们的0 &lt; α &lt; C,是支持向量<br>
其他点：α = 0</p>

<p>损失值=1-d；d 是点到超平面的距离，那么过渡带之外的损失值为0，于是得到 SVM 损失函数图(Hindge损失）</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 612px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 84.07%;\"></div>
<div class=\"image-view\" data-width=\"728\" data-height=\"612\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-63f4a3addf98a173.jpg\" data-original-width=\"728\" data-original-height=\"612\" data-original-format=\"image/jpeg\" data-original-filesize=\"72044\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<br><p></p>
也就是<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 175px; max-height: 51px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 29.14%;\"></div>
<div class=\"image-view\" data-width=\"175\" data-height=\"51\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-3597d051a8ac7e52.jpg\" data-original-width=\"175\" data-original-height=\"51\" data-original-format=\"image/jpeg\" data-original-filesize=\"7578\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<br>
但得到这个最小的时候，求得新的 w 和 b。
<p>我们再看下<br>
[图片上传失败...(image-faff7c-1526290967990)]<br>
这里出现了损失函数，经过变换得到新的损失函数</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 273px; max-height: 51px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 18.68%;\"></div>
<div class=\"image-view\" data-width=\"273\" data-height=\"51\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/1089494-016b690a09d436f3.jpg\" data-original-width=\"273\" data-original-height=\"51\" data-original-format=\"image/jpeg\" data-original-filesize=\"2283\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<br><p>后面一项是 L2正则</p>


          </div>','1531183757'),
('325600','{975704}{750}{4712}{13957}{19191}{30}{2114}{611}{9694}{975744}{975830}{605}{4617}{19688}{975714}{886}{207}{649}{721}{975978}{5844}{211}{660029}{130458}{1124521}{54830}{136795}{9854}{6013}{6005}{2809}{975818}{6015}{1291}{975757}{975789}{5505}{975751}{975737}{524}{4271}{975712}{975739}{975880}{4203}{1804}{997}{19106}{40}{975796}','配置双系统深度学习环境（双硬盘GPT+UEFI+GTX1070+Linux Mint18.3+CUD 今天有空将电脑配置完成，这里我稍微整理一下，为下一次配置留个记录，顺便分享一下成果。 目录： I. 开始前的电脑配置和环境 II. 双系统安装（win10+linux mint都装在ssd，EFI引导） III. GTX1070驱动 IV. CUDA9.0 V. cuDNN7.0 VI. Anaconda VII. Tensorflow-GPU VIII. Pycharm VIIII. 测试 I. 开始前的电脑配置和环境 美行外星人，本身有出厂安','配置双系统深度学习环境（双硬盘GPT+UEFI+GTX1070+Linux Mint18.3+CUD','<div class=\"show-content-free\">
            <p>今天有空将电脑配置完成，这里我稍微整理一下，为下一次配置留个记录，顺便分享一下成果。</p>
<p>目录：<br>
I.  开始前的电脑配置和环境<br>
II. 双系统安装（win10+linux mint都装在ssd，EFI引导）<br>
III. GTX1070驱动<br>
IV. CUDA9.0<br>
V. cuDNN7.0<br>
VI. Anaconda<br>
VII. Tensorflow-GPU<br>
VIII. Pycharm<br>
VIIII. 测试</p>
<h2>I. 开始前的电脑配置和环境</h2>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 295px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 32.78%;\"></div>
<div class=\"image-view\" data-width=\"900\" data-height=\"295\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11019672-30f88273da794e64.png\" data-original-width=\"900\" data-original-height=\"295\" data-original-format=\"image/png\" data-original-filesize=\"39988\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 335px; max-height: 61px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 18.21%;\"></div>
<div class=\"image-view\" data-width=\"335\" data-height=\"61\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11019672-198ad6ad39dde1f2.png\" data-original-width=\"335\" data-original-height=\"61\" data-original-format=\"image/png\" data-original-filesize=\"4321\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p>美行外星人，本身有出厂安装的win10 64位家庭版正版，win10装在了SSD上，配载了7代i7，GTX1070显卡，有一个128g的SSD和一个1T的机械硬盘，内存是16g的。</p>
<p>买时的目的就是为了在笔记本电脑中配置一个个人的深度学习工作站，同时闲暇时候也能兼顾玩一些大型的游戏，虽然不是最贵的顶配，而且是8代u出了以后，打折促销处理的7代u版，但价格非常美好，且配置已经能很好地完成我的目标了。</p>
<p>另外,电脑的硬盘是GPT格式的，启动也是UEFI模式，这就给安装双系统增加了不少难度，easyBCD什么的已经无法使用了，而且一定要用掉一个U盘作为启动盘，至少我暂时没有找到不需要报废U盘的办法。</p>
<p>另整个流程都是在实际电脑中完成，不是在虚拟机中，请确认你是装机老手，不然还是在虚拟机里练一下吧。</p>
<h2>II. 双系统安装（win10+linux mint都装在ssd）</h2>
<ul>
<li>准备工作：
<ul>
<li>一个空的U盘<br>
(在不得不耗费一个U盘前，我尝试了许多方法，试图从硬盘安装，还研究了下怎么从network安装，但无果，最后只能将一个8g的U盘作为启动盘了。)</li>
<li>linux mint官网下载iso镜像（<a href=\"https://link.jianshu.com?t=https%3A%2F%2Flinuxmint.com%2Fdownload.php\" target=\"_blank\" rel=\"nofollow\">https://linuxmint.com/download.php</a>）<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 660px; max-height: 207px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 31.36%;\"></div>
<div class=\"image-view\" data-width=\"660\" data-height=\"207\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11019672-2f7dc9353a359698.png\" data-original-width=\"660\" data-original-height=\"207\" data-original-format=\"image/png\" data-original-filesize=\"19849\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<br>
选择红框64位版，还有一步是选择合适的镜像，这个选恰当的就行了，我自己在美国，所以随手选了个USA的，速度还是很快的5分钟左右就下完了。</li>
<li>linux mint官网推荐的镜像制作软件Etcher（<a href=\"https://link.jianshu.com?t=https%3A%2F%2Fetcher.io%2F\" target=\"_blank\" rel=\"nofollow\">https://etcher.io/</a>）<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 546px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 76.47%;\"></div>
<div class=\"image-view\" data-width=\"714\" data-height=\"546\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11019672-68f71cab95a91f71.png\" data-original-width=\"714\" data-original-height=\"546\" data-original-format=\"image/png\" data-original-filesize=\"56167\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
</li>
</ul>
</li>
</ul>
<p>接下来：<br>
1.制作Linux Mint启动盘</p>
<ul>
<li>
<p>安装Etcher后打开，和官网给的步骤一样，点击Select Image按钮，选中下载下来的iso镜像，插入空U盘，Etcher会自动检测到U盘，然后点Flash！</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 518px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 55.05%;\"></div>
<div class=\"image-view\" data-width=\"941\" data-height=\"518\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11019672-433c2e01e1226f1a.png\" data-original-width=\"941\" data-original-height=\"518\" data-original-format=\"image/png\" data-original-filesize=\"32202\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
</li>
<li>轻松加愉快，启动盘就制作完成了。Etcher制作完启动盘后会自动弹出U盘，所以直接拔下就可以了。</li>
</ul>
<ol start=\"2\">
<li>调整硬盘分区</li>
</ol>
<ul>
<li><p>右键资源管理器中的This PC/此电脑-管理-磁盘管理，打开windows里的磁盘管理器.</p></li>
<li>
<p>发现原本的电脑SSD盘128G，出厂时划分为了一个主分区装了win10和N块recovery的区域，机械硬盘作为数据盘，没有分区。</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 92px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 9.59%;\"></div>
<div class=\"image-view\" data-width=\"959\" data-height=\"92\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11019672-3f1fc6e97e9f557d.png\" data-original-width=\"959\" data-original-height=\"92\" data-original-format=\"image/png\" data-original-filesize=\"26436\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
</li>
<li><p>我的目标是win10不变将linux mint的系统装到SSD里的空余位置，其它的分区装在机械硬盘上，同时Linux和Win10共用机械硬盘里的数据，所以这里要把SSD里的隐藏恢复分区没有用的给删掉，只保留EFI分区和安装了win10的C盘。</p></li>
<li><p>好吧，128g的SSD还是太小了，家境好的还是选大些的SSD~等我有空也要再换一块。</p></li>
<li><p>打开Terminal：</p></li>
</ul>
<pre><code>1.C:\\Users\\manim&gt;diskpart
2.DISKPART&gt; list disk 
Disk ###  Status         Size     Free     Dyn  Gpt
--------  -------------  -------  -------  ---  ---                                                                     
Disk 0    Online          119 GB  1024 KB        *                                                                      
Disk 1    Online          931 GB      0 B        *   
3.DISKPART&gt; select disk 0（选中SSD盘）
Disk 0 is now the selected disk.
4.DISKPART&gt; list partition（查看分区)
5. DISKPART&gt; select partition N（其中N是查看的分区编号，这一步代表你选中了分区）
Partition N is now the selected partition.
6.DISKPART&gt; delete partition override（做这一步之前请谨慎确认选中的分区是无用的，且里面没有重要数据，否则删除后很难恢复）
DiskPart successfully deleted the selected partition. 
</code></pre>
<p>接下来直接在windows的磁盘管理中，将ssd里分出一个10g大小的freespace即未分配区，机械硬盘里分出一个约400~500g大小的freespace即未分配区，保证主要进行深度学习工作的linux系统里有足够的空间。<br>
将新卷变为未分配区的方法很简单，删除卷标就行了，最后将10G+450G的空间保持在‘未分配空间’的状态，也就是黑色。</p>
<ol start=\"3\">
<li>设置BIOS</li>
</ol>
<ul>
<li>重启电脑，在启动界面猛按或长按某一个神秘按键，进入BIOS。这个按键不同的厂不一样，可能是F2，F10或DEL，一般都是这三个，不确定的话根据电脑自己google一下吧。</li>
<li>这里有三件事要完成：</li>
</ul>
<ol>
<li>将security boot关闭，设置成disabled，False，off之类的。</li>
<li>保持UEFI不变，但将快速boot之类的关闭，切记，千万不要弄巧成拙改成legacy。</li>
<li>将UEFI的USB启动设置为自动启动第一位序，如果没有该选项的话，可以在开机时长按F12之类的按键，自选boot方式。</li>
</ol>
<ul>
<li>重新启动电脑，选择UEFI下的USB启动选项，注意该选项会标注时UEFI模式。</li>
</ul>
<ol start=\"4\">
<li>绕开因显卡而死机的危机</li>
</ol>
<ul>
<li>原本进行完第三步，理所应当从USB启动，click，click，click下去，就能安装好Linux了，然而这里我却遇到了个相当令人烦躁的bug——Ubuntu不支持Nvidia的主流显卡——所以会各种卡在安装界面、logo界面、进度条界面……这也一度让我对外星人灯厂表示失望，当然，最后发现是Ubuntu的问题，和外星人无关。</li>
</ul>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 129px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 17.2%;\"></div>
<div class=\"image-view\" data-width=\"750\" data-height=\"129\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11019672-d5ba5609d48daea0.png\" data-original-width=\"750\" data-original-height=\"129\" data-original-format=\"image/png\" data-original-filesize=\"13798\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<ul>
<li>官方的解释是，有一些显卡和主板不能够很好地在默认的Linux MInt环境的开源驱动下工作。</li>
</ul>
<p>这里参考了linux mint官方文档，完美解决了这个问题。</p>
<p><strong>解决方案一（官方推荐）</strong>：</p>
<ul>
<li>在安装时主动选择boot的选项，长按F12就可以进入主动选择boot选项的页面，选择usb boot，来到USB安装界面</li>
<li>在USB安装界面选择<code>compatibility mode</code>
</li>
<li>
<p>跑完流程后，系统会自动进入到安装界面</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 920px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 131.44%;\"></div>
<div class=\"image-view\" data-width=\"706\" data-height=\"928\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11019672-13b5b0c8fe26ff66.png\" data-original-width=\"706\" data-original-height=\"928\" data-original-format=\"image/png\" data-original-filesize=\"222222\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
</li>
</ul>
<p>解决方案二（如果方案一帮不了你的话）：</p>
<ul>
<li>同样是在USB安装界面</li>
<li>按<code>e</code>修改boot选项</li>
<li>将其中<code>quiet splash --</code>的文本，替换为<code>nomodeset</code>
</li>
<li>最后按<code>F10</code>就可以了<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 555px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 74.9%;\"></div>
<div class=\"image-view\" data-width=\"741\" data-height=\"555\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11019672-83dce5f08a6decd4.png\" data-original-width=\"741\" data-original-height=\"555\" data-original-format=\"image/png\" data-original-filesize=\"41891\"></div>
</div>
<div class=\"image-caption\">image.png</div>
</div>
</li>
</ul>
<p>验证：</p>
<ul>
<li>成功标志1：进入系统后会呈现很辣鸡的显示效果，一点和1080P及GTX高端卡不匹配，估计就是几代前的旧电脑的显示效果。</li>
<li>
<p>成功标志2：在点击开始安装系统后，能够看到linux系统的介绍广告，鼠标可以移动，不会卡住，进度条也继续往下走，还可以开火狐浏览器浏览网页。</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 649px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 85.39%;\"></div>
<div class=\"image-view\" data-width=\"760\" data-height=\"649\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11019672-042b151b92bd17f7.png\" data-original-width=\"760\" data-original-height=\"649\" data-original-format=\"image/png\" data-original-filesize=\"187599\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
</li>
<li>ps:如果该成功的标志没有出现，强制关机，重新来一次吧！</li>
</ul>
<ol start=\"5\">
<li>安装Linux Mint<br>
终于，我们可以开始安装Linux Mint了，这一步需要注意三点。</li>
</ol>
<ul>
<li>
<p>如果你赶时间，记得不要联网，也不要选择<code>Install third-party software</code>那个选项，因为联网后会自动安装更新和驱动等，反之就自由选择，如果不联网，不选这个选项10分钟能跑完系统安装。<br></p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 526px; max-height: 350px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 66.53999999999999%;\"></div>
<div class=\"image-view\" data-width=\"526\" data-height=\"350\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11019672-de17f923783d410a.png\" data-original-width=\"526\" data-original-height=\"350\" data-original-format=\"image/png\" data-original-filesize=\"27228\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p></p>
</li>
<li>
<p>记得不要选择清除windows！也不要选择与什么一起共存的选项！切记要选择<code>Something else</code>。<br></p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 526px; max-height: 350px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 66.53999999999999%;\"></div>
<div class=\"image-view\" data-width=\"526\" data-height=\"350\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11019672-bca3292b7def33a3.png\" data-original-width=\"526\" data-original-height=\"350\" data-original-format=\"image/png\" data-original-filesize=\"49550\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p></p>
</li>
</ul>
<ol start=\"6\">
<li>Linux分区计划</li>
</ol>
<ul>
<li>因为我的目的是主要用linux来运行深度学习的工作，所以一些基本的软件譬如anaconda3，cuda9.0, cuDNN7.1, tensorflow-gpu, pandas, numpy ……等python的包，都要装上。</li>
<li>其中anaconda3在安装时默认装在/home分区，cuda和cuDNN都是装在/usr分区，python包是以anaconda3来管理，所以anaconda3装在哪，包就会在哪。以后写的程序和Project的临时文件一般都会存在/home里，当然，可以用github来管理，但本地还是会占用空间的。</li>
<li>另外虽然linux也可以调用SSD和机械硬盘里的数据，但运行的时候数据放在相同目录下还是会方便些，所以还要留出一定大小的空间直接放数据。</li>
<li>综上，我的分区计划如下：<br><code>/boot</code>不需要，我们把引导项和windows放在一起，因为是先有的win10再装的linux，所以win10不会覆盖掉，注意等会儿boot挂在时挂到SSD中的EFI分区上即可。<br><code>/</code>ext4格式，logical，这个是系统所在的根目录，留下10g给linux系统以及一些杂七杂八的分区足矣。<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 669px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 79.17%;\"></div>
<div class=\"image-view\" data-width=\"845\" data-height=\"669\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11019672-c7c0950dc54f844d.png\" data-original-width=\"845\" data-original-height=\"669\" data-original-format=\"image/png\" data-original-filesize=\"89153\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
</li>
</ul>
<p><code>swap area</code>交换空间，这个空间即你的虚拟内存，在物理内存用尽后，溢出的部分才回到虚拟内存中。该空间太大没用，太小又不行，根据内存大小选择，一般不要小于本身内存的一半，若是本身内存太小，可以设置大些。我的内存是16g，留下8g足够了。<br></p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 642px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 78.68%;\"></div>
<div class=\"image-view\" data-width=\"816\" data-height=\"642\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11019672-7990756bd45f6a79.png\" data-original-width=\"816\" data-original-height=\"642\" data-original-format=\"image/png\" data-original-filesize=\"59089\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p></p>
<p><code>/usr</code>默认软件安装空间，软件默认的安装位置都在这个区，因为要装pycharm和cuda、cuDNN，我留了60g。<br></p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 641px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 78.17%;\"></div>
<div class=\"image-view\" data-width=\"820\" data-height=\"641\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11019672-0a52c64071c5584a.png\" data-original-width=\"820\" data-original-height=\"641\" data-original-format=\"image/png\" data-original-filesize=\"64275\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p></p>
<p><code>/home</code>将余下的freespace都给了Home空间，这个相当于是windows系统中的user区域，anaconda3，文档，日志等默认的位置都在这里，以后数据也都放这里，留多些没坏处。<br></p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 641px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 78.36%;\"></div>
<div class=\"image-view\" data-width=\"818\" data-height=\"641\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11019672-fd18b181c8c8390a.png\" data-original-width=\"818\" data-original-height=\"641\" data-original-format=\"image/png\" data-original-filesize=\"65169\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<p></p>
<ul>
<li>当然，还有更多的譬如<code>/var</code>之类的区域，我没有一一细分，这些都会挂在根目录<code>/</code>下面。</li>
<li>接下来，将<code>boot</code>挂到<code>/dev/sda(x)</code>，这里的x指windows系统放置了引导项的EFI分区编号一般应该不是0就是1，后面会有<code>windows boot manager</code>的后缀，就可以点击开始安装了。</li>
<li>如果不联网的话安装过程在10分钟左右。</li>
</ul>
<h2>III. GTX1070驱动</h2>
<ul>
<li>用linux自带的驱动管理安装最新的驱动即可</li>
<li>
<code>菜单</code> - <code>设置</code> - <code>驱动管理</code>-<code>384版最新驱动</code><br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 639px; max-height: 506px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 79.19%;\"></div>
<div class=\"image-view\" data-width=\"639\" data-height=\"506\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11019672-46b56a3127f94232.png\" data-original-width=\"639\" data-original-height=\"506\" data-original-format=\"image/png\" data-original-filesize=\"46933\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
</li>
</ul>
<h2>IV. CUDA9.0</h2>
<ul>
<li>在官网下载<a href=\"https://link.jianshu.com?t=https%3A%2F%2Fdeveloper.nvidia.com%2Fcuda-90-download-archive\" target=\"_blank\" rel=\"nofollow\">CUDA9.0</a>，选择Ubuntu16.04版，下载</li>
<li>在<code>Download</code>目录打开Terminal，逐一输入以下代码<br>
ps: 所有代码摘抄修改自<a href=\"https://link.jianshu.com?t=http%3A%2F%2Fdeveloper.download.nvidia.com%2Fcompute%2Fcuda%2F9.0%2FProd%2Fdocs%2Fsidebar%2FCUDA_Installation_Guide_Linux.pdf\" target=\"_blank\" rel=\"nofollow\">官方指导</a>：</li>
</ul>
<pre><code>1. sudo dpkg -i cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64.deb
2. sudo apt-key add /var/cuda-repo-9-0-local/7fa2af80.pub
3. sudo apt-get update
4. sudo apt-get install cuda
5. export PATH=/usr/local/cuda-9.0/bin${PATH:+:${PATH}}
6. export LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64\\ ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}
7. export CUDA_HOME=/usr/local/cuda
8. sudo ln -s cuda-9.0 cuda
</code></pre>
<ul>
<li>完成</li>
</ul>
<h2>V. cuDNN7.0</h2>
<ul>
<li>在官网下载cuDNN7.0，这里注意，tensorflow暂时还未支持7.1，所以我下载的还是7.0版本，要下载cuDNN还需要用邮箱注册一下，登录后才能下载，这个就注册下好了。</li>
<li>下载<a href=\"https://link.jianshu.com?t=https%3A%2F%2Fdeveloper.nvidia.com%2Fcompute%2Fmachine-learning%2Fcudnn%2Fsecure%2Fv7.0.5%2Fprod%2F9.0_20171129%2Fcudnn-9.0-linux-x64-v7\" target=\"_blank\" rel=\"nofollow\">cuDNN v7.0.5 Library for Linux</a>就可以了。</li>
<li>下好后同样在<code>Download</code>打开terminal，逐一输入以下代码:<br>
ps: 同样，参考<a href=\"https://link.jianshu.com?t=https%3A%2F%2Fdocs.nvidia.com%2Fdeeplearning%2Fsdk%2Fcudnn-install%2F%23installlinux-tar\" target=\"_blank\" rel=\"nofollow\">官方指导</a>
</li>
</ul>
<pre><code>1. tar -xzvf cudnn-9.0-linux-x64-v7.tgz
2. sudo cp cuda/include/cudnn.h /usr/local/cuda/include
3. sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64
4. sudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn*
5. export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64export
6. CUDA_HOME=/usr/local/cuda
</code></pre>
<ul>
<li>完成了</li>
</ul>
<h2>VI. Anaconda</h2>
<ul>
<li>在官网下载linux版的<a href=\"https://link.jianshu.com?t=https%3A%2F%2Fwww.anaconda.com%2Fdownload%2F%23linux\" target=\"_blank\" rel=\"nofollow\">Anaconda3.6</a>
</li>
<li>在Download目录下打开terminal，逐一输入：</li>
</ul>
<pre><code>1. bash ~/Downloads/Anaconda3-5.1.0-Linux-x86_64.sh
2. 点击N次回车[Enter] 
3. yes
4. 再点击一次回车[Enter]
5. yes
</code></pre>
<ul>
<li>可以尝试下能否打开</li>
</ul>
<pre><code>1. source ~/.bashrc
2. anaconda-navigator
</code></pre>
<ul>
<li>
<p>安装完成</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 363px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 51.99%;\"></div>
<div class=\"image-view\" data-width=\"1914\" data-height=\"995\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11019672-4428146ab44ee864.png\" data-original-width=\"1914\" data-original-height=\"995\" data-original-format=\"image/png\" data-original-filesize=\"137372\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
</li>
</ul>
<h2>VII. Tensorflow-GPU</h2>
<ul>
<li>接下来我们开始安装Tensorflow-gpu版本，这次安装的是最新版tensorflow1.8</li>
<li>打开anaconda的terminal，输入：</li>
</ul>
<pre><code>1. sudo apt-get install python3-numpy python3-dev python3-pip python3-wheel
2.  sudo apt-get install libcupti-dev
3. pip install --upgrade -I setuptools
4. conda create -n tensorflow python=3.6
5. conda activate tensorflow
6. pip install --ignore-installed --upgrade \\
 https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.8.0-cp36-cp36m-linux_x86_64.whl
</code></pre>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 656px; max-height: 410px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 62.5%;\"></div>
<div class=\"image-view\" data-width=\"656\" data-height=\"410\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11019672-5c2dd2ab72c2e3f9.png\" data-original-width=\"656\" data-original-height=\"410\" data-original-format=\"image/png\" data-original-filesize=\"106964\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<ul>
<li>完成<br>
随手在terminal试验下：</li>
</ul>
<pre><code>python
import tensorflow as tf
# Creates a graph.
a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name=\'a\')
b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name=\'b\')
c = tf.matmul(a, b)
# Creates a session with log_device_placement set to True.
sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))
# Runs the op.
print(sess.run(c))
</code></pre>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 325px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 17.02%;\"></div>
<div class=\"image-view\" data-width=\"1910\" data-height=\"325\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11019672-050d48765c37ce8f.png\" data-original-width=\"1910\" data-original-height=\"325\" data-original-format=\"image/png\" data-original-filesize=\"162167\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<h2>VIII. Pycharm</h2>
<ul>
<li>接下来我们装一个python的IDE，先去官网下个<a href=\"https://link.jianshu.com?t=https%3A%2F%2Fwww.jetbrains.com%2Fpycharm%2Fdownload%2F%23section%3Dlinux\" target=\"_blank\" rel=\"nofollow\">Pycharm</a>
</li>
<li>下载好以后，根据官网指导安装一下。</li>
<li>先在<code>/usr</code>目录，或其他你想要安装的目录下解压下载的包，进入pycharm的bin目录，打开terminal，输入<code>sh pycharm.sh</code>就能进入pycharm</li>
<li>如果要用github的话记得下载git：</li>
</ul>
<pre><code>1. sudo apt-get install git
</code></pre>
<h2>VIIII. 测试</h2>
<ul>
<li>最后，测试一下整个环境是否配置完成，我们运行一个minisize的训练计划吧。</li>
<li>minst的20000个epoches跑了86秒，可以说速度贼快了。</li>
</ul>
<pre><code>import tensorflow as tf
import datetime

#Start interactive session
sess = tf.InteractiveSession()
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets(\'MNIST_data\', one_hot=True)

width = 28 # width of the image in pixels
height = 28 # height of the image in pixels
flat = width * height # number of pixels in one image
class_output = 10 # number of possible classifications for the problem

x  = tf.placeholder(tf.float32, shape=[None, flat])
y_ = tf.placeholder(tf.float32, shape=[None, class_output])

x_image = tf.reshape(x, [-1,28,28,1])


W_conv1 = tf.Variable(tf.truncated_normal([5, 5, 1, 32], stddev=0.1))
b_conv1 = tf.Variable(tf.constant(0.1, shape=[32])) # need 32 biases for 32 outputs
convolve1= tf.nn.conv2d(x_image, W_conv1, strides=[1, 1, 1, 1], padding=\'SAME\') + b_conv1
h_conv1 = tf.nn.relu(convolve1)

conv1 = tf.nn.max_pool(h_conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\'SAME\') #max_pool_2x2

W_conv2 = tf.Variable(tf.truncated_normal([5, 5, 32, 64], stddev=0.1))
b_conv2 = tf.Variable(tf.constant(0.1, shape=[64])) #need 64 biases for 64 outputs
convolve2= tf.nn.conv2d(conv1, W_conv2, strides=[1, 1, 1, 1], padding=\'SAME\')+ b_conv2
h_conv2 = tf.nn.relu(convolve2)

conv2 = tf.nn.max_pool(h_conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\'SAME\') #max_pool_2x2

layer2_matrix = tf.reshape(conv2, [-1, 7*7*64])

W_fc1 = tf.Variable(tf.truncated_normal([7 * 7 * 64, 1024], stddev=0.1))
b_fc1 = tf.Variable(tf.constant(0.1, shape=[1024])) # need 1024 biases for 1024 outputs
fcl=tf.matmul(layer2_matrix, W_fc1) + b_fc1
h_fc1 = tf.nn.relu(fcl)

keep_prob = tf.placeholder(tf.float32)
layer_drop = tf.nn.dropout(h_fc1, keep_prob)

W_fc2 = tf.Variable(tf.truncated_normal([1024, 10], stddev=0.1)) #1024 neurons
b_fc2 = tf.Variable(tf.constant(0.1, shape=[10])) # 10 possibilities for digits [0,1,2,3,4,5,6,7,8,9]

fc=tf.matmul(layer_drop, W_fc2) + b_fc2

y_CNN= tf.nn.softmax(fc)


cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_CNN), reduction_indices=[1]))
train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)
correct_prediction = tf.equal(tf.argmax(y_CNN,1), tf.argmax(y_,1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
sess.run(tf.global_variables_initializer())


starttime = datetime.datetime.now()
for i in range(20000):
    batch = mnist.train.next_batch(50)
    if i%100 == 0:
        train_accuracy = accuracy.eval(feed_dict={
            x:batch[0], y_: batch[1], keep_prob: 1.0})
        print(\"step %d, training accuracy %g\"%(i, train_accuracy))
    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})

endtime = datetime.datetime.now()
print(\'Time use:\',(endtime - starttime).seconds, \'s\')
</code></pre>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 528px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 75.48%;\"></div>
<div class=\"image-view\" data-width=\"1044\" data-height=\"788\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11019672-278efa4bbda4501e.png\" data-original-width=\"1044\" data-original-height=\"788\" data-original-format=\"image/png\" data-original-filesize=\"217460\"></div>
</div>
<div class=\"image-caption\"></div>
</div>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 274px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 25.490000000000002%;\"></div>
<div class=\"image-view\" data-width=\"1075\" data-height=\"274\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11019672-9ccaf5d29ffc3b2a.png\" data-original-width=\"1075\" data-original-height=\"274\" data-original-format=\"image/png\" data-original-filesize=\"40448\"></div>
</div>
<div class=\"image-caption\"></div>
</div>

          </div>','1531183759'),
('325601','{5210}{36}{975714}{62}{1277}{1124528}{7671}{1124529}{975837}{1124530}{975951}{975721}{697}{522}{1503}{74}{1318}{89594}{1124531}{975728}{975749}{600}{210}{611}{7489}{3363}{1190}{975915}{3100}{975899}{1751}{2206}{975779}{1354}{11588}{1085143}{2169}{761}{975740}{3090}{9332}{14281}{3461}{1124532}{829}{100207}{190}{127455}{118323}{1290}','分布式TensorFlow入门教程 前 言 深度学习在各个领域实现突破的一部分原因是我们使用了更多的数据（大数据）来训练更复杂的模型（深度神经网络），并且可以利用一些高性能并行计算设备如GPU和FPGA来加速模型训练。但是有时候，模型之大或者训练数据量之多可能超出我们的想象，这个时候就需要分布式训练系统，利用分布式系统我们可以训练更加复杂的模型（单机无法装','分布式TensorFlow入门教程','<div class=\"show-content-free\">
            <h2><strong>前  言</strong></h2>
<p>深度学习在各个领域实现突破的一部分原因是我们使用了更多的数据（大数据）来训练更复杂的模型（深度神经网络），并且可以利用一些高性能并行计算设备如GPU和FPGA来加速模型训练。但是有时候，模型之大或者训练数据量之多可能超出我们的想象，这个时候就需要分布式训练系统，利用分布式系统我们可以训练更加复杂的模型（单机无法装载），还可以加速我们的训练过程，这对于研究者实现模型的超参数优化是非常有意义的。2017年6月，Facebook发布了他们的论文Accurate, Large Minibatch SGD:Training ImageNet in 1 Hour，文中指出他们采用分布在32个服务器上的256块GPUs将Resnet-50模型在ImageNet数据集上的训练时间从两周缩短为1个小时。在软件层面，他们使用了很大的minibatch（8192）来训练模型，并且使学习速率正比于minibatch的大小。这意味着，采用分布式系统可以实现模型在成百个GPUs上的训练，从而大大减少训练时间，你也将有更多的机会去尝试各种各样的超参数组合。作为使用人数最多的深度学习框架，TensorFlow从version 0.8开始支持模型的分布式训练，现在的TensorFlow支持模型的多机多卡（GPUs和 CPUs）训练。在这篇文章里面，我将简单介绍分布式TensorFlow的基础知识，并通过实例来讲解如何使用分布式TensorFlow来训练模型。</p>
<blockquote>
<p>Methods that scale with computation are the future of AI.                                                                                              —Rich Sutton, 强化学习之父</p>
</blockquote>
<p>在开始之前，有必要先简单介绍一下深度学习的分布式训练策略以及分布式架构。这有助于理解分布式TensorFlow系统。</p>
<h2>分布式训练策略</h2>
<p><strong>1.模型并行</strong></p>
<p>所谓模型并行指的是将模型部署到很多设备上（设备可能分布在不同机器上，下同）运行，比如多个机器的GPUs。当神经网络模型很大时，由于显存限制，它是难以在跑在单个GPU上，这个时候就需要模型并行。比如Google的神经机器翻译系统，其可能采用深度LSTM模型，如下图所示，此时模型的不同部分需要分散到许多设备上进行并行训练。深度学习模型一般包含很多层，如果要采用模型并行策略，一般需要将不同的层运行在不同的设备上，但是实际上层与层之间的运行是存在约束的：前向运算时，后面的层需要等待前面层的输出作为输入，而在反向传播时，前面的层又要受限于后面层的计算结果。所以除非模型本身很大，一般不会采用模型并行，因为模型层与层之间存在串行逻辑。但是如果模型本身存在一些可以并行的单元，那么也是可以利用模型并行来提升训练速度，比如GoogLeNet的Inception模块。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 502px; max-height: 420px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 83.67%;\"></div>
<div class=\"image-view\" data-width=\"502\" data-height=\"420\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-2c08c066775d9198\" data-original-width=\"502\" data-original-height=\"420\" data-original-format=\"image/jpeg\" data-original-filesize=\"34666\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>模型并行训练</p>
<p><strong>2.数据并行</strong></p>
<p>深度学习模型最常采用的分布式训练策略是数据并行，因为训练费时的一个重要原因是训练数据量很大。数据并行就是在很多设备上放置相同的模型，并且各个设备采用不同的训练样本对模型训练。训练深度学习模型常采用的是batch SGD方法，采用数据并行，可以每个设备都训练不同的batch，然后收集这些梯度用于模型参数更新。前面所说的Facebook训练Resnet50就是采用数据并行策略，使用256个GPUs，每个GPU读取32个图片进行训练，如下图所示，这样相当于采用非常大的batch（256 × 32 = 8192）来训练模型。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 471px; max-height: 157px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 33.33%;\"></div>
<div class=\"image-view\" data-width=\"471\" data-height=\"157\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-c01ffba95fd95d96\" data-original-width=\"471\" data-original-height=\"157\" data-original-format=\"image/png\" data-original-filesize=\"20640\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>数据并行可以是同步的（synchronous），也可以是异步的（asynchronous）。所谓同步指的是所有的设备都是采用相同的模型参数来训练，等待所有设备的mini-batch训练完成后，收集它们的梯度然后取均值，然后执行模型的一次参数更新。这相当于通过聚合很多设备上的mini-batch形成一个很大的batch来训练模型，Facebook就是这样做的，但是他们发现当batch大小增加时，同时线性增加学习速率会取得不错的效果。同步训练看起来很不错，但是实际上需要各个设备的计算能力要均衡，而且要求集群的通信也要均衡，类似于木桶效应，一个拖油瓶会严重拖慢训练进度，所以同步训练方式相对来说训练速度会慢一些。异步训练中，各个设备完成一个mini-batch训练之后，不需要等待其它节点，直接去更新模型的参数，这样总体会训练速度会快很多。但是异步训练的一个很严重的问题是梯度失效问题（stale gradients），刚开始所有设备采用相同的参数来训练，但是异步情况下，某个设备完成一步训练后，可能发现模型参数其实已经被其它设备更新过了，此时这个梯度就过期了，因为现在的模型参数和训练前采用的参数是不一样的。由于梯度失效问题，异步训练虽然速度快，但是可能陷入次优解（sub-optimal training performance）。异步训练和同步训练在TensorFlow中不同点如下图所示：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 651px; max-height: 545px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 83.72%;\"></div>
<div class=\"image-view\" data-width=\"651\" data-height=\"545\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-57cc815e96a5876f\" data-original-width=\"651\" data-original-height=\"545\" data-original-format=\"image/jpeg\" data-original-filesize=\"55666\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>数据并行中的同步方式和异步方式</p>
<p>为了解决异步训练出现的梯度失效问题，微软提出了一种Asynchronous Stochastic Gradient Descent方法，主要是通过梯度补偿来提升训练效果。应该还有其他类似的研究，感兴趣的可以深入了解一下。</p>
<h2>分布式训练架构</h2>
<p>前面说的是分布式训练策略，这里要谈的是系统架构层，包括两种架构：Parameter server architecture（就是常见的PS架构，参数服务器）和Ring-allreduce architecture。这里主要参考Distributed TensorFlow，完全是拿来主义了。</p>
<p><strong>1.Parameter server架构</strong></p>
<p>在Parameter server架构（PS架构）中，集群中的节点被分为两类：parameter server和worker。其中parameter server存放模型的参数，而worker负责计算参数的梯度。在每个迭代过程，worker从parameter sever中获得参数，然后将计算的梯度返回给parameter server，parameter server聚合从worker传回的梯度，然后更新参数，并将新的参数广播给worker。采用同步SGD方式的PS架构如下图所示：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 580px; max-height: 424px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 73.1%;\"></div>
<div class=\"image-view\" data-width=\"580\" data-height=\"424\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-96735825ca8085b4\" data-original-width=\"580\" data-original-height=\"424\" data-original-format=\"image/png\" data-original-filesize=\"34713\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>PS架构中的同步SGD训练方式</p>
<p>在TensorFlow之前，Google采用的是DistBelief框架，其支持PS架构。TensorFlow从DistBelief借鉴了它的很多分布式训练模式，所以TensorFlow也支持PS架构。Mxnet的主要创建者李沐在前人基础上开发了更加通用的轻量级ps-lite，如果想深入理解PS架构，可以看一下沐神的讲解。PS架构是深度学习最常采用的分布式训练架构。</p>
<p><strong>2.Ring-allreduce架构</strong></p>
<p>在Ring-allreduce架构中，各个设备都是worker，并且形成一个环，如下图所示，没有中心节点来聚合所有worker计算的梯度。在一个迭代过程，每个worker完成自己的mini-batch训练，计算出梯度，并将梯度传递给环中的下一个worker，同时它也接收从上一个worker的梯度。对于一个包含N个worker的环，各个worker需要收到其它N-1个worker的梯度后就可以更新模型参数。其实这个过程需要两个部分：scatter-reduce和allgather，百度的教程对这个过程给出了详细的图文解释。百度开发了自己的allreduce框架，并将其用在了深度学习的分布式训练中。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 580px; max-height: 376px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 64.83%;\"></div>
<div class=\"image-view\" data-width=\"580\" data-height=\"376\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-25943eeb26906319\" data-original-width=\"580\" data-original-height=\"376\" data-original-format=\"image/png\" data-original-filesize=\"33656\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>Ring-allreduce架构示意图</p>
<p>相比PS架构，Ring-allreduce架构是带宽优化的，因为集群中每个节点的带宽都被充分利用。此外，在深度学习训练过程中，计算梯度采用BP算法，其特点是后面层的梯度先被计算，而前面层的梯度慢于前面层，Ring-allreduce架构可以充分利用这个特点，在前面层梯度计算的同时进行后面层梯度的传递，从而进一步减少训练时间。在百度的实验中，他们发现训练速度基本上线性正比于GPUs数目（worker数）。</p>
<h2>分布式TensorFlow简介</h2>
<p>好了，言归正传，现在开始介绍分布式TensorFlow的基础知识。在分布式TensorFlow中，参与分布式系统的所有节点或者设备被总称为一个集群（cluster），一个cluster中包含很多服务器（server），每个server去执行一项任务（task），server和task是一一对应的。所以，cluster可以看成是server的集合，也可以看成是task的集合。TensorFlow为各个task又增加了一个抽象层，将一系列相似的task集合称为一个job，比如在PS架构中，习惯称parameter server的task集合为ps，而称执行梯度计算的task集合为worker。所以cluster又可以看成是job的集合，不过这只是逻辑上的意义，具体还要看这个server真正干什么。在TensorFlow中，job用name（字符串）标识，而task用index（整数索引）标识，那么cluster中的每个task可以用job的name加上task的index来唯一标识。在分布式系统中，一般情况下各个task在不同的节点或者设备上执行。TensorFlow中用tf.train.ClusterSpec创建一个cluster：</p>
<p>&lt;pre data-anchor-id=\"qq27\" style=\"margin: 0px; padding: 10px 15px; max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;\"&gt;</p>
<p><code>cluster = tf.train.ClusterSpec({ \"worker\": [ \"worker0.example.com:2222\", \"worker1.example.com:2222\", \"worker2.example.com:2222\" ], \"ps\": [ \"ps0.example.com:2222\", \"ps1.example.com:2222\" ]})</code></p>
<p>&lt;/pre&gt;</p>
<p>可以看出，cluster接收的其实就是一个字典，字典里面包含了各个task所在host的主机地址，这个cluster共包含两类job：ps和worker，共5个task:</p>
<pre><code>/job:worker/task:0
/job:worker/task:1
/job:worker/task:2
/job:ps/task:0
/job:ps/task:1
</code></pre>
<p>创建好cluster，需要创建各个task的server，使用tf.train.Server函数，比如创建第一个worker的server：</p>
<pre><code>server = tf.train.Server(cluster, job_name=\"worker\", task_index=0)
</code></pre>
<p>在创建sever时必须要传入cluster，这样每个server才可以知道自己所在的cluster包含哪些hosts，然后server与server之间才可以通信。sever的创建需要在自己所在host上，一旦所有的server在各自的host上创建好了，整个集群就搭建好了，cluster之间的各个server可以互相通信。具体来说，每个server包含两个组件：master和worker。其中master提供master service，其主要可以提供对cluster中各个设备的远程访问（RPC协议），同时它的另外一个重要功能是作为创建tf.Session的target。而worker提供worker service，可以用本地设备执行TF中的计算子图。这两个东西并不好理解，这里我们先讲TensorFlow中的另外一个重要概念：client，先抛出官方英文解释：</p>
<pre><code>A client is typically a program that builds a TensorFlow graph and constructs a tensorflow::Session to interact with a cluster. Clients are typically written in Python or C++. A single client process can directly interact with multiple TensorFlow servers (see \"Replicated training\" above), and a single server can serve multiple clients.
</code></pre>
<p>这个client是个很重要的概念，简单来说就是一个程序，它创建了TF的计算图，并通过建立Session与cluster中的设备进行交互。说白了前面创建的cluster与server只是搭建分布式环境，真正要执行计算需要创建client。对于tf.Session这个类，其第一个参数是target，一般情况下大家确实用不到，因为不指定这个参数的话，Session就默认调用本地设备，但是在分布式环境就需要指定了，这就是server里面的master（server.target提供这个参数）。实际上，TensorFlow的完整执行逻辑如下图所示：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 291px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 35.19%;\"></div>
<div class=\"image-view\" data-width=\"827\" data-height=\"291\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-f1f1c89689bc4865\" data-original-width=\"827\" data-original-height=\"291\" data-original-format=\"image/jpeg\" data-original-filesize=\"36924\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>TensorFlow计算图在单机和分布式系统的执行流程图</p>
<p>就是说client要跑计算时，其实要先要把计算图以及要执行的节点（Graph中的Node）发给master，master负责资源调度（就是这个计算该怎么执行，在哪些设备执行），最终的执行需要各个worker进程（使用本地设备执行计算），所以每个server会包含master和worker两个部分。关于master的具体作用，可以参考一下TF教程中的TensorFlow Architecture，不过这里贴一张图，大家意淫一下：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 365px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 44.51%;\"></div>
<div class=\"image-view\" data-width=\"820\" data-height=\"365\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-3ec36c0dae208569\" data-original-width=\"820\" data-original-height=\"365\" data-original-format=\"image/png\" data-original-filesize=\"29876\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>分布式TensorFlow中的master划分子图</p>
<p>上面简单解释了一下server和client相关的一些重要概念，帮助大家理解分布式TensorFlow的执行逻辑。那么，在构建Graph时如何调用cluster中的各个server呢？很简单，使用tf.device，只需要指定task的详细信息即可：</p>
<pre><code>with tf.device(\"/job:ps/task:0\"):
  weights_1 = tf.Variable(...)
  biases_1 = tf.Variable(...)

with tf.device(\"/job:ps/task:1\"):
  weights_2 = tf.Variable(...)
  biases_2 = tf.Variable(...)

with tf.device(\"/job:worker/task:7\"):
  input, labels = ...
  layer_1 = tf.nn.relu(tf.matmul(input, weights_1) + biases_1)
  logits = tf.nn.relu(tf.matmul(layer_1, weights_2) + biases_2)
  # ...
  train_op = ...
</code></pre>
<p>很简单，就如同把cluster里面的设备当成本机设备一样使用，至于怎么真正执行，那就是系统层面的事了。构建了Graph后，我们需要创建Session来执行计算图：</p>
<pre><code>with tf.Session(\"grpc://worker7.example.com:2222\") as sess:
  for _ in range(10000):
    sess.run(train_op)
</code></pre>
<p>注意由于是分布式系统，需要指定Session的target参数，或者采用grpc+主机地址，或者直接利用sever.target，两个是完全一样的。下面我们通过一个简单的实例来理解上面过程，这个例子的cluster共包含3个task：1个ps和2个worker。</p>
<pre><code>import tensorflow as tf

tf.app.flags.DEFINE_string(\"ps_hosts\", \"localhost:2222\", \"ps hosts\")
tf.app.flags.DEFINE_string(\"worker_hosts\", \"localhost:2223,localhost:2224\", \"worker hosts\")
tf.app.flags.DEFINE_string(\"job_name\", \"worker\", \"\'ps\' or\'worker\'\")
tf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")

FLAGS = tf.app.flags.FLAGS

def main(_):
    ps_hosts = FLAGS.ps_hosts.split(\",\")
    worker_hosts = FLAGS.worker_hosts.split(\",\")
    # create cluster
    cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})
    # create the server
    server = tf.train.Server(cluster, job_name=FLAGS.job_name, task_index=FLAGS.task_index)

    server.join()

if __name__ == \"__main__\":
    tf.app.run()
</code></pre>
<p>注意这里用单机环境模拟多机环境，然后分别执行下面三个命令行来创建三个server：</p>
<pre><code>python example.py --job_name=ps --task_index=0
python example.py --job_name=worker --task_index=0
python example.py --job_name=worker --task_index=1
</code></pre>
<p>执行完毕后，三个server都处在等待状态，现在我们在创建一个client来执行一个计算图，并且采用/job:worker/task:0这个server所对应的master，即<a href=\"https://link.jianshu.com?t=grpc%3A%2F%2Flocalhost%3A2223\" target=\"_blank\" rel=\"nofollow\">grpc://localhost:2223</a>来创建Session，如下所示：</p>
<pre><code>import tensorflow as tf

if __name__ == \"__main__\":
    with tf.device(\"/job:ps/task:0\"):
        x = tf.Variable(tf.ones([2, 2]))
        y = tf.Variable(tf.ones([2, 2]))

    with tf.device(\"/job:worker/task:0\"):
        z = tf.matmul(x, y) + x

    with tf.device(\"/job:worker/task:1\"):
        z = tf.matmul(z, x) + x

    with tf.Session(\"grpc://localhost:2223\") as sess:
        sess.run(tf.global_variables_initializer())
        val = sess.run(z)
        print(val)
</code></pre>
<p>其实这个client就是一个进程，但是其在计算时需要依靠cluster中的device来执行部分计算子图。值得注意的是上面的程序我们遵循了PS架构，参数放置在ps，而worker执行计算。但是在TensorFlow中，其实每个task所属的job只是一个概念，并没有什么差别，就是说对于上面的程序，你完全可以把参数放置在worker上。所以说，TensorFlow的分布式架构支持PS模式，并且也往往采用这种方式，但是TensorFlow并不完全与PS架构对等。</p>
<h2>复制训练(Replicated training)</h2>
<p>前面已经说过了，深度学习模型分布式训练最常用的是数据并行策略，在TensorFlow中称这为复制训练（Replicated training），就是说多个worker使用不同的mini-batch训练相同的模型，计算出的梯度用于更新放置在ps的模型参数。由于复制训练是一种最常用的模式，TensorFlow也增加了一些库函数来简化复制训练的实现。在TensorFlow中共有四种不同的方式来实现复制训练：</p>
<ol>
<li><p>In-graph replication：只构建一个client，这个client构建一个Graph，Graph中包含一套模型参数，放置在ps上，同时Graph中包含模型计算部分的多个副本，每个副本都放置在一个worker上，这样多个worker可以同时训练复制的模型。TensorFlow教程中的使用多个GPUs训练cifar10分类模型就属于这个类型，每个GPUs上的计算子图是相同的，但是属于同一个Graph。这种方法很少使用，因为一旦client挂了，整个系统就全崩溃了，容错能力差。</p></li>
<li><p>Between-graph replication：每个worker都创建一个client，这个client一般还与task的主程序在同一进程中。各个client构建相同的Graph，但是参数还是放置在ps上。这种方式就比较好，一个worker的client挂掉了，系统还可以继续跑。</p></li>
<li><p>Asynchronous training：异步方式训练，各个worker自己干自己的，不需要与其它worker来协调，前面也已经详细介绍了异步训练，上面两种方式都可以采用异步训练。</p></li>
<li><p>Synchronous training：同步训练，各个worker要统一步伐，计算出的梯度要先聚合才可以执行一次模型更新，对于In-graph replication方法，由于各个worker的计算子图属于同一个Graph，很容易实现同步训练。但是对于Between-graph replication方式，各个worker都有自己的client，这就需要系统上的设计了，TensorFlow提供了tf.train.SyncReplicasOptimizer来实现Between-graph replication的同步训练。</p></li>
</ol>
<p>由于在TensorFlow中最常用的是Between-graph replication方式，这里着重讲一下如何实现这种方式。在Between-graph replication中，各个worker都包含一个client，它们构建相同的计算图，然后把参数放在ps上，TensorFlow提供了一个专门的函数tf.train.replica_device_setter来方便Graph构建，先看代码：</p>
<pre><code># cluster包含两个ps 和三个 worker
cluster_spec = {
    \"ps\": [\"ps0:2222\", \"ps1:2222\"],
    \"worker\": [\"worker0:2222\", \"worker1:2222\", \"worker2:2222\"]}
cluster = tf.train.ClusterSpec(cluster_spec)
with tf.device(tf.train.replica_device_setter(
                worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,
               cluster=cluster)):
  # Build your graph
  v1 = tf.Variable(...)  # assigned to /job:ps/task:0
  v2 = tf.Variable(...)  # assigned to /job:ps/task:1
  v3 = tf.Variable(...)  # assigned to /job:ps/task:0
  # Run compute
</code></pre>
<p>使用tf.train.replica_device_setter可以自动把Graph中的Variables放到ps上，而同时将Graph的计算部分放置在当前worker上，省去了很多麻烦。由于ps往往不止一个，这个函数在为各个Variable分配ps时默认采用简单的round-robin方式，就是按次序将参数挨个放到各个ps上，但这个方式可能不能使ps负载均衡，如果需要更加合理，可以采用tf.contrib.training.GreedyLoadBalancingStrategy策略。</p>
<p>采用Between-graph replication方式的另外一个问题，由于各个worker都独立拥有自己的client，但是对于一些公共操作比如模型参数初始化与checkpoint文件保存等，如果每个client都独立进行这些操作，显然是对资源的浪费。为了解决这个问题，一般会指定一个worker为chief worker，它将作为各个worker的管家，协调它们之间的训练，并且完成模型初始化和模型保存和恢复等公共操作。在TensorFlow中，可以使用tf.train.MonitoredTrainingSession创建client的Session，并且其可以指定哪个worker是chief worker。关于这些方面，想深入理解可以看一下2017 TensorFlow 开发峰会的官方讲解，其中也对分布式TensorFlow的容错机制做了简单介绍。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 684px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 63.33%;\"></div>
<div class=\"image-view\" data-width=\"1080\" data-height=\"684\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-9fb63d70a0dfb8b3\" data-original-width=\"1080\" data-original-height=\"684\" data-original-format=\"image/jpeg\" data-original-filesize=\"21928\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p>Between-graph replication方式的编程结构图</p>
<h2>MNIST分布式训练实例</h2>
<p>最后，我们给出MNIST的分布式训练实例，采用Between-graph replication方式，并且同步训练和异步训练都支持。在这个例子中，cluster共包含2个ps和2个worker，其中worker1为chief worker。代码如下：</p>
<pre><code>import tensorflow as tf
from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets

tf.app.flags.DEFINE_string(\"ps_hosts\", \"localhost:2222\", \"ps hosts\")
tf.app.flags.DEFINE_string(\"worker_hosts\", \"localhost:2223,localhost:2224\", \"worker hosts\")
tf.app.flags.DEFINE_string(\"job_name\", \"worker\", \"\'ps\' or\'worker\'\")
tf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")
tf.app.flags.DEFINE_integer(\"num_workers\", 2, \"Number of workers\")
tf.app.flags.DEFINE_boolean(\"is_sync\", False, \"using synchronous training or not\")

FLAGS = tf.app.flags.FLAGS

def model(images):
    \"\"\"Define a simple mnist classifier\"\"\"
    net = tf.layers.dense(images, 500, activation=tf.nn.relu)
    net = tf.layers.dense(net, 500, activation=tf.nn.relu)
    net = tf.layers.dense(net, 10, activation=None)
    return net

def main(_):
    ps_hosts = FLAGS.ps_hosts.split(\",\")
    worker_hosts = FLAGS.worker_hosts.split(\",\")

    # create the cluster configured by `ps_hosts\' and \'worker_hosts\'
    cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})

    # create a server for local task
    server = tf.train.Server(cluster, job_name=FLAGS.job_name,
                             task_index=FLAGS.task_index)

    if FLAGS.job_name == \"ps\":
        server.join()  # ps hosts only join
    elif FLAGS.job_name == \"worker\":
        # workers perform the operation
        # ps_strategy = tf.contrib.training.GreedyLoadBalancingStrategy(FLAGS.num_ps)

        # Note: tf.train.replica_device_setter automatically place the paramters (Variables)
        # on the ps hosts (default placement strategy:  round-robin over all ps hosts, and also
        # place multi copies of operations to each worker host
        with tf.device(tf.train.replica_device_setter(worker_device=\"/job:worker/task:%d\" % (FLAGS.task_index),
                                                      cluster=cluster)):
            # load mnist dataset
            mnist = read_data_sets(\"./dataset\", one_hot=True)

            # the model
            images = tf.placeholder(tf.float32, [None, 784])
            labels = tf.placeholder(tf.int32, [None, 10])

            logits = model(images)
            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))

            # The StopAtStepHook handles stopping after running given steps.
            hooks = [tf.train.StopAtStepHook(last_step=2000)]

            global_step = tf.train.get_or_create_global_step()
            optimizer = tf.train.AdamOptimizer(learning_rate=1e-04)

            if FLAGS.is_sync:
                # asynchronous training
                # use tf.train.SyncReplicasOptimizer wrap optimizer
                # ref: https://www.tensorflow.org/api_docs/python/tf/train/SyncReplicasOptimizer
                optimizer = tf.train.SyncReplicasOptimizer(optimizer, replicas_to_aggregate=FLAGS.num_workers,
                                                       total_num_replicas=FLAGS.num_workers)
                # create the hook which handles initialization and queues
                hooks.append(optimizer.make_session_run_hook((FLAGS.task_index==0)))

            train_op = optimizer.minimize(loss, global_step=global_step,
                                          aggregation_method=tf.AggregationMethod.ADD_N)

            # The MonitoredTrainingSession takes care of session initialization,
            # restoring from a checkpoint, saving to a checkpoint, and closing when done
            # or an error occurs.
            with tf.train.MonitoredTrainingSession(master=server.target,
                                                   is_chief=(FLAGS.task_index == 0),
                                                   checkpoint_dir=\"./checkpoint_dir\",
                                                   hooks=hooks) as mon_sess:

                while not mon_sess.should_stop():
                    # mon_sess.run handles AbortedError in case of preempted PS.
                    img_batch, label_batch = mnist.train.next_batch(32)
                    _, ls, step = mon_sess.run([train_op, loss, global_step],
                                               feed_dict={images: img_batch, labels: label_batch})
                    if step % 100 == 0:
                        print(\"Train step %d, loss: %f\" % (step, ls))

if __name__ == \"__main__\":
    tf.app.run()
</code></pre>
<p>异步执行时，分别执行下面四条语句：</p>
<pre><code>  python distributed_mnist.py --ps_hosts=localhost:2222,localhost:2223 --worker_hosts=localhost:2224,localhost:2225 --job_name=ps --task_index=0

   python distributed_mnist.py --ps_hosts=localhost:2222,localhost:2223 --worker_hosts=localhost:2224,localhost:2225 --job_name=ps --task_index=1

   python distributed_mnist.py --ps_hosts=localhost:2222,localhost:2223 --worker_hosts=localhost:2224,localhost:2225 --job_name=worker --task_index=0

   python distributed_mnist.py --ps_hosts=localhost:2222,localhost:2223 --worker_hosts=localhost:2224,localhost:2225 --job_name=worker --task_index=1
</code></pre>
<p>此时你会看到两个worker打印出的step是交叉的，说明此时是异步执行的，每个worker执行一次梯度计算后，立即将梯度发给ps完成参数更新。</p>
<p>对于同步执行，采用tf.train.SyncReplicasOptimizer，分别执行下面四条语句：</p>
<pre><code>   python distributed_mnist.py --ps_hosts=localhost:2222,localhost:2223 --worker_hosts=localhost:2224,localhost:2225 --job_name=ps --task_index=0 --is_sync=True

   python distributed_mnist.py --ps_hosts=localhost:2222,localhost:2223 --worker_hosts=localhost:2224,localhost:2225 --job_name=ps --task_index=1 --is_sync=True

   python distributed_mnist.py --ps_hosts=localhost:2222,localhost:2223 --worker_hosts=localhost:2224,localhost:2225 --job_name=worker --task_index=0 --is_sync=True

   python distributed_mnist.py --ps_hosts=localhost:2222,localhost:2223 --worker_hosts=localhost:2224,localhost:2225 --job_name=worker --task_index=1 --is_sync=True
</code></pre>
<p>此时你可以看到两个worker基本上同时打印相同的step（但是loss是不一样的），说明是同步执行。值得注意的是，TensorFlow中的同步训练可能与你想象中不同，它只是收集足够的梯度（N个step的梯度结果）就聚合这些梯度值然后执行一次参数更新。但是它不管这N个结果是从哪里来的，如果其中某个worker速度很慢，可能这N个结果都是从其他worker计算出的。言外之意就是chief worker聚合的梯度不一定是从全部worker中收集而来的（参考这个issues）。这个机制很怪异，我想是为了容错机制，不至于一个worker死掉了而终止整个训练过程。所以，在同步训练过程中，最好每个worker的能力都差不多，要不然很难得到想要的加速效果（某个worker慢的话，它计算的梯度可能过期，那么只能被丢弃，这种情况下这个worker做的就是无用功）。</p>
<h2>走得更远</h2>
<p>TensorFlow可以与Hadoop和Spark等工具结合，感兴趣的话可以自己深入去学习：</p>
<ul>
<li><p>TensorFlow官方教程：How to run TensorFlow on Hadoop.</p></li>
<li><p>Yahho: Open Sourcing TensorFlowOnSpark: Distributed Deep Learning on Big-Data Clusters.</p></li>
</ul>
<p>总结</p>
<p>最近打算学习一下分布式TensorFlow，所以系统地看了官方文档以及一些国外的博客，然后就把其中一些讲解得比较好的地方以及自己的学习心得总结了一下，所以就有了此文。但是网上的资料并不是很多，所以文中有错误之处在所难免，也恳请各位大佬斧正。很偶然地看到一篇最新的综述文章Demystifying Parallel and Distributed Deep Learning: An In-Depth Concurrency Analysis，60页的paper系统总结了深度学习的并行化策略，想深入学习的可以读读这个paper。</p>
<p>参考文献</p>
<ol>
<li><p>Distributed TensorFlow.</p></li>
<li><p>How to write distributed TensorFlow code—with an example on Clusterone.</p></li>
<li><p>MNIST实例：Distributing TensorFlow.</p></li>
<li><p>Google TF 官网：Distributed TensorFlow.</p></li>
<li><p>Distributed TensorFlow（2017 TensorFlow 开发峰会）.</p></li>
<li><p>Baidu Research: Bringing HPC Techniques to Deep Learning.</p></li>
<li><p>TensorFlow学习笔记（9）：分布式TensorFlow.</p></li>
<li><p>Distributed TensorFlow Example.</p></li>
</ol>
<p><em>（注：文中图片原始来源均可以在文中或者参考文章的链接中找到）</em></p>
<hr>
<blockquote>
<p>个人技术博客：<a href=\"https://link.jianshu.com?t=https%3A%2F%2Fblog.csdn.net%2Fu013709270\" target=\"_blank\" rel=\"nofollow\">https://blog.csdn.net/u013709270</a><br>
微信：TonyJeemy520      加个人微信拉你进机器学习、深度学习交流群，请备注 : 来自简书<br>
QQ交流群：651616387   请备注 : 来自简书<br>
微信公众号：机器学习算法工程师   ----二维码见下图</p>
</blockquote>
<hr>
<p><strong>扫码关注微信号：机器学习算法工程师，更多干货分享， 或加个人微信，拉你进机器学习、深度学习交流群</strong><br></p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 430px; max-height: 430px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 100.0%;\"></div>
<div class=\"image-view\" data-width=\"430\" data-height=\"430\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/11692737-0bdf004ac80c0197\" data-original-width=\"430\" data-original-height=\"430\" data-original-format=\"image/jpeg\" data-original-filesize=\"32906\"></div>
</div>
<div class=\"image-caption\">image</div>
</div>
<p></p>

          </div>','1531183764'),
('325602','{718}{895}{9805}{761}{975714}{34887}{99547}{99545}{975727}{1407}{1735}{365}{1628}{975788}{5127}{667}{33243}{11706}{975757}{975885}{356}{1672}{975773}{1716}{15402}{1124538}{44062}{975741}{2098}{66044}{99498}{117273}{975820}{975926}{975728}{975809}{1583}{778}{975876}{728}{975910}{1841}{553}{1621}{3152}{3601}{975730}{116813}{216990}{975738}','} } 环形队列 为充分利用向量空间，克服\"假溢出\"现象的方法是：将向量空间想象为一个首尾相接的圆环，并称这种向量为循环向量。存储在其中的队列称为循环队列。环形队列也是一种数组，只是它在逻辑上把数组的头和尾相连，形成循环队列，当数组尾满的时候，要判断数组头是否为空，不为空继续存放数据。 class CircularQueue implements QueueInterface { private $queue;','实战PHP数据结构基础之队列','<div class=\"show-content-free\">
            <h3>什么是队列</h3>
<p>队列是另外一种遵循先进先出原则的线性数据结构。队列有两端可供操作，一端出队，一端入队。这个特点和栈不同，栈只有一端可以用来操作。入队总是在后端，出队在前端。</p>
<h3>常见操作</h3>
<ul>
<li>enqueue -&gt; 入队</li>
<li>dequeue -&gt; 出队</li>
<li>peek -&gt; 返回队列前端元素</li>
<li>isEmpty -&gt; 是否为空</li>
</ul>
<h3>PHP实现</h3>
<p>首先我们定义一个QueueInterface。</p>
<pre><code class=\"PHP\">interface QueueInterface
{
    public function enqueue(string $item);
    public function dequeue();
    public function isEmpty();
    public function peek();
}
</code></pre>
<p>来看基于数组的队列实现</p>
<pre><code class=\"PHP\">class ArrQueue implements QueueInterface
{
    private $queue;
    private $limit;

    public function __construct(int $limit = 0)
    {
       $this-&gt;limit = $limit;
       $this-&gt;queue = [];
    }

    public function isEmpty()
    {
        return empty($this-&gt;queue);
    }

    public function dequeue()
    {
        if ($this-&gt;isEmpty()) {
            throw new \\UnderflowException(\'queue is empty\');
        } else {
            array_shift($this-&gt;queue);
        }
    }

    public function enqueue(string $item)
    {
        if (count($this-&gt;queue) &gt;= $this-&gt;limit) {
            throw new \\OverflowException(\'queue is full\');
        } else {
            array_unshift($this-&gt;queue, $item);
        }
    }

    public function peek()
    {
        return current($this-&gt;queue);
    }
}
</code></pre>
<p>得益于PHP强大的array结构，我们轻而易举的写出来了队列的基本操作方法。果然世界上最好的语言名不虚传。</p>
<p>可能机智的同学已经猜到了，我之前已经定义了一个队列接口，那队列的实现肯定不止只有上面一种哈。来看基于链表的实现。</p>
<pre><code class=\"PHP\">class LinkedListQueue implements QueueInterface
{
    private $limit;
    private $queue;

    public function __construct(int $limit = 0)
    {
        $this-&gt;limit = $limit;
        $this-&gt;queue = new LinkedList();
    }

    public function isEmpty()
    {
        return $this-&gt;queue-&gt;getSize() == 0;
    }

    public function peek()
    {
        return $this-&gt;queue-&gt;getNthNode(0)-&gt;data;
    }

    public function enqueue(string $item)
    {
        if ($this-&gt;queue-&gt;getSize() &lt; $this-&gt;limit) {
            $this-&gt;queue-&gt;insert($item);
        } else {
            throw new \\OverflowException(\'queue is full\');
        }
    }

    public function dequeue()
    {
        if ($this-&gt;isEmpty()) {
            throw new \\UnderflowException(\'queue is empty\');
        } else {
            $lastItem = $this-&gt;peek();
            $this-&gt;queue-&gt;deleteFirst();

            return $lastItem;
        }
    }
}
</code></pre>
<p>里面涉及到了之前的链表实现，不了解细节的同学可以去<a href=\"https://github.com/xx19941215/webBlog\" target=\"_blank\" rel=\"nofollow\">这里</a>看看。</p>
<h4>Spl中的队列</h4>
<p>强大的PHP已经内置了队列实现，可以使用的方法和上面我们自己实现的类似。</p>
<pre><code class=\"PHP\">class SqlQueue
{
    private $splQueue;

    public function __construct()
    {
        $this-&gt;splQueue = new \\SplQueue();
    }

    public function enqueue(string $data = null)
    {
        $this-&gt;splQueue-&gt;enqueue($data);
    }

    public function dequeue()
    {
        return $this-&gt;splQueue-&gt;dequeue();
    }
}
</code></pre>
<h4>优先队列</h4>
<p>优先队列是一种特殊的队列，入队或者出队的顺序都是基于每个节点的权重。</p>
<h5>顺序序列</h5>
<p>优先队列可以分为有序优先队列和无序优先队列。有序优先队列又有两种情况，倒序或者顺序。在顺序序列中，我们可以迅速的找出最大或者最小优先级别的节点，复杂度是O(1)。但是插入的话会花费掉更多的时间，因为我们要检查每一个节点的优先级别然后插入到合适的位置。</p>
<h5>无序序列</h5>
<p>在无序序列中，在插入新节点的时候我们不需要根据他们的优先级确定位置。入队的时候像普通队列一样，插入到队列的末尾。但是当我们想移除优先级最高的元素的时候，我们要扫描每一个节点来确定移除优先级最高的那一个。接下来我们还是基于链表实现一个顺序的优先队列。</p>
<pre><code class=\"php\">class LinkedListPriorityQueue
{
    private $limit;
    private $queue;


    public function __construct(int $limit = 0)
    {
        $this-&gt;limit = $limit;
        $this-&gt;queue = new LinkedList();
    }

    public function enqueue(string $data = null, int $priority)
    {
        if ($this-&gt;queue-&gt;getSize() &gt; $this-&gt;limit) {
            throw new \\OverflowException(\'Queue is full\');
        } else {
            $this-&gt;queue-&gt;insert($data, $priority);
        }
    }

    public function dequeue(): string
    {
        if ($this-&gt;isEmpty()) {
            throw new \\UnderflowException(\'Queue is empty\');
        } else {
            $item = $this-&gt;peek();
            $this-&gt;queue-&gt;deleteFirst();

            return $item-&gt;data;
        }
    }

    public function peek()
    {
        return $this-&gt;queue-&gt;getNthNode(0);
    }

    public function isEmpty()
    {
        return $this-&gt;queue-&gt;getSize() === 0;
    }
}
</code></pre>
<h4>环形队列</h4>
<p>为充分利用向量空间，克服\"假溢出\"现象的方法是：将向量空间想象为一个首尾相接的圆环，并称这种向量为循环向量。存储在其中的队列称为循环队列。环形队列也是一种数组，只是它在逻辑上把数组的头和尾相连，形成循环队列，当数组尾满的时候，要判断数组头是否为空，不为空继续存放数据。</p>
<pre><code class=\"php\">class CircularQueue implements QueueInterface
{
    private $queue;
    private $limit;
    private $front = 0;
    private $rear = 0;

    public function __construct(int $limit = 0)
    {
        $this-&gt;limit = $limit;
        $this-&gt;queue = [];
    }

    public function isEmpty()
    {
        return $this-&gt;front === $this-&gt;rear;
    }

    public function isFull()
    {
        $diff = $this-&gt;rear - $this-&gt;front;

        if ($diff == -1 || $diff == ($this-&gt;limit - 1)) {
            return true;
        }

        return false;
    }

    public function peek()
    {
        return $this-&gt;queue[$this-&gt;front];
    }

    public function dequeue()
    {
        if ($this-&gt;isEmpty()) {
            throw new \\UnderflowException(\'Queue is empty\');
        }

        $item = $this-&gt;queue[$this-&gt;front];
        $this-&gt;queue[$this-&gt;front] = null;
        $this-&gt;front = ($this-&gt;front + 1) % $this-&gt;limit;

        return $item;
    }

    public function enqueue(string $item)
    {
        if ($this-&gt;isFull()) {
            throw new \\OverflowException(\'Queue is full\');
        }

        $this-&gt;queue[$this-&gt;rear] = $item;
        $this-&gt;rear = ($this-&gt;rear + 1) % $this-&gt;limit;

    }
}
</code></pre>
<h4>双端队列</h4>
<p>截止目前我们所实现的队列都是在队尾(rear)入队，队首(front) 出队的结构。在特殊的情况下，我们希望不论是队首还是队尾都可以入队或者出队，这种结构就叫做双端队列。基于我们之前实现的链表结构，我们可以轻而易举的实现这样的结构。</p>
<pre><code class=\"php\">class LinkedListDeQueue
{
    private $limit = 0;
    private $queue;

    public function __construct(int $limit = 0)
    {
        $this-&gt;limit = $limit;
        $this-&gt;queue = new \\DataStructure\\LinkedList\\LinkedList();
    }

    public function dequeueFromFront(): string 
    {
        if ($this-&gt;isEmpty()) {
            throw new \\UnderflowException(\'Queue is empty\');
        }

        $item = $this-&gt;queue-&gt;getNthNode(0);
        $this-&gt;queue-&gt;deleteFirst();

        return $item-&gt;data;
    }

    public function dequeueFromBack(): string 
    {
        if ($this-&gt;isEmpty()) {
            throw new \\UnderflowException(\'Queue is empty\');
        }

        $item = $this-&gt;queue-&gt;getNthNode($this-&gt;queue-&gt;getSize() - 1);
        $this-&gt;queue-&gt;deleteLast();

        return $item-&gt;data;
    }

    public function isFull()
    {
        return $this-&gt;queue-&gt;getSize() &gt;= $this-&gt;limit;
    }

    public function enqueueAtBack(string $data = null)
    {
        if ($this-&gt;isFull()) {
            throw new \\OverflowException(\'queue is full\');
        }

        $this-&gt;queue-&gt;insert($data);
    }

    public function enqueueAtFront(string $data = null)
    {
        if ($this-&gt;isFull()) {
            throw new \\OverflowException(\'queue is full\');
        }

        $this-&gt;queue-&gt;insertAtFirst($data);
    }

    public function isEmpty()
    {
        return $this-&gt;queue-&gt;getSize() === 0;
    }

    public function peekFront()
    {
        return $this-&gt;queue-&gt;getNthNode(0)-&gt;data;
    }

    public function peekBack()
    {
        return $this-&gt;queue-&gt;getNthNode($this-&gt;queue-&gt;getSize() - 1)-&gt;data;
    }
}
</code></pre>
<p>里面涉及到了之前的链表实现，不了解细节的同学可以去<a href=\"https://github.com/xx19941215/webBlog\" target=\"_blank\" rel=\"nofollow\">这里</a>看看。</p>
<h3>总结</h3>
<p>栈和队列是我们最常用的数据结构，我们会在其他的复杂数据结构中看到这两种抽象数据类型的应用。在下一章，我们继续学习PHP数据结构之递归，这是一种将复杂问题简单化的常用思路。</p>
<h3>专题系列</h3>
<p>PHP基础数据结构专题系列目录地址：<a href=\"https://github.com/xx19941215/webBlog\" target=\"_blank\" rel=\"nofollow\">地址</a> 主要使用PHP语法总结基础的数据结构和算法。还有我们日常PHP开发中容易忽略的基础知识和现代PHP开发中关于规范、部署、优化的一些实战性建议，同时还有对Javascript语言特点的深入研究。</p>

          </div>','1531183765'),
('325603','{895}{761}{1382}{975788}{1628}{975727}{667}{975714}{9007}{1716}{975773}{1906}{975901}{975866}{4986}{1124539}{44062}{1621}{975728}{975837}{1583}{975876}{728}{975910}{1841}{2329}{757}{116813}{975738}{3601}{975757}{975881}{832}{9114}{718}{5278}{7829}{42500}{6209}{423}{405}{975745}{922}{1124540}{5886}{780}{1598}{410}{975766}{975730}','实战PHP数据结构基础之栈 栈和队列 栈和队列和之前讲到的实战PHP数据结构基础之双链表 一样都是线性结构。 栈有什么特点 栈遵循后进先出的原则(LIFO)。这意味着栈只有一个出口用来压入元素和弹出元素，当我们执行压入或者弹出操作的时候要注意栈是否已满或者栈是否是空的。 常见操作 还是废话不多说，直接来看我们对栈执行的常用操作。 push pop top isEmpty . $this- stack = [];','实战PHP数据结构基础之栈','<div class=\"show-content-free\">
            <h3>栈和队列</h3>
<p>栈和队列和之前讲到的<a href=\"https://www.jianshu.com/p/8c0a62fe3e5c\" target=\"_blank\">实战PHP数据结构基础之双链表<br></a> 一样都是线性结构。</p>
<h4>栈有什么特点</h4>
<p>栈遵循后进先出的原则(LIFO)。这意味着栈只有一个出口用来压入元素和弹出元素，当我们执行压入或者弹出操作的时候要注意栈是否已满或者栈是否是空的。</p>
<h4>常见操作</h4>
<p>还是废话不多说，直接来看我们对栈执行的常用操作。</p>
<ul>
<li>push</li>
<li>pop</li>
<li>top</li>
<li>isEmpty</li>
<li>...</li>
</ul>
<h4>PHP实现</h4>
<p>首先我们定义一个StackInterface。</p>
<pre><code class=\"PHP\">interface StackInterface
{
    public function push(string $item);
    public function pop();
    public function top();
    public function isEmpty();
}
</code></pre>
<p>来看基于数组的栈实现</p>
<pre><code class=\"PHP\">class ArrStack implements StackInterface
{
    private $stack;
    private $limit;

    public function __construct(int $limit = 20)
    {
        $this-&gt;limit = $limit;
        $this-&gt;stack = [];
    }

    public function __get($val)
    {
        return $this-&gt;$val;
    }

    public function push(string $data = null)
    {
        if (count($this-&gt;stack) &lt; $this-&gt;limit) {
            array_push($this-&gt;stack, $data);
        } else {
            throw new \\OverflowException(\'stack is overflow\');
        }
    }

    public function pop()
    {
        if ($this-&gt;isEmpty()) {
            throw new \\UnderflowException(\'stack is empty\');
        } else {
            return array_pop($this-&gt;stack);
        }
    }

    public function isEmpty()
    {
        return empty($this-&gt;stack);
    }

    public function top()
    {
        return end($this-&gt;stack);
    }
</code></pre>
<p>得益于PHP强大的array结构，我们轻而易举的写出来了栈的基本操作方法。果然世界上最好的语言名不虚传。</p>
<p>那有同学说了，你说栈和之前的链表都是线性结构，那可不可以直接使用链表实现栈呢？这个问题非常犀利啊，答案是可以的。</p>
<p>可能机智的同学已经猜到了，我之前已经定义了一个栈接口，那栈的实现肯定不止只有上面一种哈。来看基于链表的实现。</p>
<pre><code class=\"PHP\">class LinkedListStack implements StackInterface
{
    private $stack;
    private $limit;

    public function __construct(int $limit)
    {
        $this-&gt;limit = $limit;
        $this-&gt;stack = new LinkedList();
    }

    public function top()
    {
        return $this-&gt;stack-&gt;getNthNode($this-&gt;stack-&gt;getSize() - 1)-&gt;data;
    }

    public function isEmpty()
    {
        return $this-&gt;stack-&gt;getSize() === 0;
    }

    public function pop()
    {
        if ($this-&gt;isEmpty()) {
            throw new \\UnderflowException(\'stack is empty\');
        } else {
            $lastItem = $this-&gt;top();
            $this-&gt;stack-&gt;deleteLast();

            return $lastItem;
        }
    }

    public function push(string $item)
    {
        if ($this-&gt;stack-&gt;getSize() &lt; $this-&gt;limit) {
            $this-&gt;stack-&gt;insert($item);
        } else {
            throw new \\OverflowException(\'stack is overflow\');
        }
    }
</code></pre>
<p>里面涉及到了之前的链表实现，不了解细节的同学可以去<a href=\"https://github.com/xx19941215/webBlog\" target=\"_blank\" rel=\"nofollow\">这里</a>看看。有同学又问，那栈到底有什么用处？这个问题非常好，来看一个需求。</p>
<p>请实现一个数学表达式检查类，输入一个下面表达式，预期结果为true。</p>
<pre><code>\"8 * (9 -2) + { (4 * 5) / ( 2 * 2) }
</code></pre>
<p>下面的为false。</p>
<pre><code>\"5 * 8 * 9 / ( 3 * 2 ) )\"
</code></pre>
<p>下面的也为false。</p>
<pre><code>\"[{ (2 * 7) + ( 15 - 3) ]\"
</code></pre>
<p>自己想一下，再往下看实现。</p>
<pre><code class=\"PHP\">class ExpressionChecker
{
    //$expressions[] = \"8 * (9 -2) + { (4 * 5) / ( 2 * 2) }\";
    //$expressions[] = \"5 * 8 * 9 / ( 3 * 2 ) )\";
    //$expressions[] = \"[{ (2 * 7) + ( 15 - 3) ]\";

    public function check(string $expression): bool
    {
        $stack = new \\SplStack();

        foreach (str_split($expression) as $item) {
            switch ($item) {
                case \'{\':
                case \'[\':
                case \'(\':
                    $stack-&gt;push($item);
                    break;

                case \'}\':
                case \']\':
                case \')\':
                    if ($stack-&gt;isEmpty()) return false;

                    $last = $stack-&gt;pop();

                    if (
                        $item == \'{\' &amp;&amp; $last != \'}\' ||
                        $item == \'(\' &amp;&amp; $last != \')\' ||
                        $item == \'[\' &amp;&amp; $last != \']\'
                    )
                        return false;

                    break;
            }
        }

        if ($stack-&gt;isEmpty()) {
            return true;
        }

        return false;
    }
}
</code></pre>
<h4>专题系列</h4>
<p>PHP基础数据结构专题系列目录地址：<a href=\"https://github.com/xx19941215/light-tips\" target=\"_blank\" rel=\"nofollow\">地址</a> 主要使用PHP语法总结基础的数据结构和算法。还有我们日常PHP开发中容易忽略的基础知识和现代PHP开发中关于规范、部署、优化的一些实战性建议，同时还有对Javascript语言特点的深入研究。</p>

          </div>','1531183766'),
('325604','{73230}{1672}{761}{1628}{18640}{975788}{993}{667}{2104}{976073}{3226}{975773}{975727}{975757}{975745}{975730}{1621}{975714}{975871}{975876}{1158}{975885}{5437}{611114}{189976}{137642}{34640}{117}{421}{21170}{975945}{393}{1124541}{14795}{3792}{975738}{975881}{9114}{40145}{40153}{1123889}{1124542}{1123890}{214467}{1123891}{922}{423}{405}','PHP语言实现 首先我们根据定义实现一个双链表的ListNode类。 class ListNode { public $data = null; private $length = 0; } 接下来还是老规矩，直接看如何实现第一个即常用的插入，这是是一个平均时间复杂度为O(n)的操作。 /** * 插入一个节点 * @param string|null $data * @return bool * complexity O(n) */ public function insert(string $data = null) { $newNode = new ListNode($data); $nextNode- prev = $prevNode; $currentNode = $this- head;','实战PHP数据结构基础之双链表','<div class=\"show-content-free\">
            <h3>什么是双链表？</h3>
<p>上一篇<a href=\"https://www.jianshu.com/p/d4e679f1fcc2\" target=\"_blank\">实战PHP数据结构基础之单链表</a>说到</p>
<blockquote>
<p>单链表由一个一个的作为节点的对象构成的，每一个节点都有指向下一个节点的指针，最后一个节点的指针域指向空。每个节点可以存储任何数据类型。</p>
</blockquote>
<p>而双链表每个节点有两个指针域，分别指向前驱和后继节点。单链表是单向的，而双链表是双向的。</p>
<h3>常见操作</h3>
<p>对双链表我们常见的操作有如下：</p>
<ul>
<li>insert</li>
<li>insertBefore</li>
<li>insertAfter</li>
<li>insertAtFirst</li>
<li>insertAtLast</li>
<li>deleteFirst</li>
<li>deleteLast</li>
<li>delete</li>
<li>reverse</li>
<li>getNthNode</li>
<li>...</li>
</ul>
<h3>PHP语言实现</h3>
<p>首先我们根据定义实现一个双链表的ListNode类。</p>
<pre><code class=\"PHP\">class ListNode
{
    public $data = null;
    public $next = null;
    public $prev = null;

    public function __construct(string $data)
    {
        $this-&gt;data = $data;
    }
}
</code></pre>
<p>再来看链表类，首先需要3个私有属性，分别是头节点、尾巴节点和长度。</p>
<pre><code class=\"PHP\">class DoubleLinkedList
{
    private $head = null;
    private $last = null;
    private $length = 0;
}
</code></pre>
<p>接下来还是老规矩，直接看如何实现第一个即常用的插入，这是是一个平均时间复杂度为O(n)的操作。</p>
<pre><code class=\"PHP\">/**
 * 插入一个节点
 * @param string|null $data
 * @return bool
 * complexity O(n)
 */
public function insert(string $data = null)
{
    $newNode = new ListNode($data);
    if ($this-&gt;head) {
        $currentNode = $this-&gt;head;
        while ($currentNode) {
            if ($currentNode-&gt;next === null) {
                $currentNode-&gt;next = $newNode;
                $newNode-&gt;prev = $currentNode;
                $this-&gt;last = $newNode;
                $this-&gt;length++;
                return true;
            }

            $currentNode = $currentNode-&gt;next;
        }
    } else {
        $this-&gt;head = &amp;$newNode;
        $this-&gt;last = $newNode;
        $this-&gt;length++;
        return true;
    }

}
</code></pre>
<p>再来看如何删除节点。</p>
<pre><code class=\"PHP\">/**
 * 删除一个节点
 * @param string $data
 * @return bool|ListNode
 * complexity O(n)
 */
public function delete(string $query = null)
{
    if ($this-&gt;head) {
        $currentNode = $this-&gt;head;
        $prevNode = null;
    
        while ($currentNode) {
            if ($currentNode-&gt;data === $query) {
                if ($nextNode = $currentNode-&gt;next) {
                    if ($prevNode) {
                        $prevNode-&gt;next = $nextNode;
                        $nextNode-&gt;prev = $prevNode;
                    } else {
                        $this-&gt;head = $nextNode;
                        $nextNode-&gt;prev = null;
                    }
    
                    unset($currentNode);
                } else {
                    if ($prevNode) {
                        $this-&gt;last = $prevNode;
                        $prevNode-&gt;next = null;
                        unset($currentNode);
                    } else {
                        $this-&gt;head = null;
                        $this-&gt;last = null;
                    }
                }
    
                $this-&gt;length--;
                return true;
            }
    
            $prevNode = $currentNode;
            $currentNode = $currentNode-&gt;next;
        }
    }

    return false;
}
</code></pre>
<p>反转双链表也不是很复杂。</p>
<pre><code class=\"PHP\">public function reverse()
{
    if ($this-&gt;head !== null) {

        if ($this-&gt;head-&gt;next !== null) {
            $reversedList = null;
            $currentNode = $this-&gt;head;

            while ($currentNode !== null) {
                $next = $currentNode-&gt;next;
                $currentNode-&gt;next = $reversedList;
                $currentNode-&gt;prev = $next;

                $reversedList = $currentNode;
                $currentNode = $next;

            }

            $this-&gt;last = $this-&gt;head;
            $this-&gt;head = $reversedList;
        }
    }
}
</code></pre>
<p>双链表其他操作的详细实现可以参考 <a href=\"https://github.com/xx19941215/webBlog\" target=\"_blank\" rel=\"nofollow\">这里</a>。</p>
<p>双链表是链表这种链式存取数据结构中相对于单链表比较特殊的部分，同样属于链表结构的还有单链表，环形链表和多链表。</p>
<h4>专题系列</h4>
<p>PHP基础数据结构专题系列目录地址：<a href=\"https://github.com/xx19941215/webBlog\" target=\"_blank\" rel=\"nofollow\">https://github.com/...</a> 主要使用PHP语法总结基础的数据结构和算法。还有我们日常PHP开发中容易忽略的基础知识和现代PHP开发中关于规范、部署、优化的一些实战性建议，同时还有对Javascript语言特点的深入研究。</p>

          </div>','1531183766'),
('325605','{1999}{976033}{3509}{975790}{952}{3519}{74}{975786}{975707}{975784}{118}{2447}{2799}{6609}{975773}{1038}{1033}{7}{1095}{1358}{976051}{176614}{115573}{9371}{74745}{12666}{1145}{975890}{975904}{975736}{1330}{2240}{975757}{1484}{5060}{526}{8770}{9060}{85741}{133}{1651}{207}{1068}{1573}','PHP 小程序获取绑定手机号码 小程序端调用wx.login获取到code，把code发送给服务端，服务端去请求该接口 $appid=\'\';appid=$appid secret=$secret js_code=$code grant_type=authorization_code\"; $timeout = 5; //获取openid和session_key curl_setopt($ch, CURLOPT_URL, $url); $contents = curl_exec($ch); // $errno = curl_errno( $ch ); $pc = new WXBizDataCrypt($appid, $sessionKey); $errCode = $pc- decryptData($encryptedData, $iv, $data);','PHP 小程序获取绑定手机号码','<div class=\"show-content-free\">
            <ul>
<li>小程序端调用wx.login获取到code，把code发送给服务端，服务端去请求该接口</li>
</ul>
<pre><code class=\"php\">$appid=\'\';
$secret=\'\';
$code=\'\';
$url = \"https://api.weixin.qq.com/sns/jscode2session?appid=$appid&amp;secret=$secret&amp;js_code=$code&amp;grant_type=authorization_code\";
        $ch = curl_init();
        $timeout = 5;
        //获取openid和session_key
        curl_setopt($ch, CURLOPT_URL, $url);
        curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);
        curl_setopt($ch, CURLOPT_CONNECTTIMEOUT, $timeout);
        $contents = curl_exec($ch);
//      $errno = curl_errno( $ch );
        curl_close($ch);
        $info = json_decode($contents);
</code></pre>
<p>如果满足unionID下发的条件的时候，也会返回unionID，</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 517px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 73.87%;\"></div>
<div class=\"image-view\" data-width=\"1022\" data-height=\"755\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6095563-446d12177e226138.png\" data-original-width=\"1022\" data-original-height=\"755\" data-original-format=\"image/png\" data-original-filesize=\"23571\"></div>
</div>
<div class=\"image-caption\">返回说明</div>
</div>
<ul>
<li><p>服务端应该把返回的数据存在数据库中。</p></li>
<li>
<p>然后小程序调用wx.getPhoneNumber的API</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 214px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 22.06%;\"></div>
<div class=\"image-view\" data-width=\"970\" data-height=\"214\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6095563-a83275932f9a6757.png\" data-original-width=\"970\" data-original-height=\"214\" data-original-format=\"image/png\" data-original-filesize=\"13108\"></div>
</div>
<div class=\"image-caption\">返回参数</div>
</div>
</li>
<li><p>然后把这两个参数和openid发送给服务端，服务端根据openid在数据库中读取session_key,拿session_key换取</p></li>
</ul>
<p>点击<a href=\"https://developers.weixin.qq.com/miniprogram/dev/demo/aes-sample.zip\" target=\"_blank\" rel=\"nofollow\">该链接下载示例</a>，里面有wxBizDataCrypt.php和errorCode.php这两个文件，引入该文件</p>
<pre><code class=\"php\">include_once \"wxBizDataCrypt.php\";
$data = \'\';
$pc = new WXBizDataCrypt($appid, $sessionKey);
$errCode = $pc-&gt;decryptData($encryptedData, $iv, $data);
//解密后的
if ($errCode == 0) {
        $data = json_decode($data);
}
</code></pre>
<p>返回结果</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 222px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 22.720000000000002%;\"></div>
<div class=\"image-view\" data-width=\"977\" data-height=\"222\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/6095563-bc2db592f7b01546.png\" data-original-width=\"977\" data-original-height=\"222\" data-original-format=\"image/png\" data-original-filesize=\"8017\"></div>
</div>
<div class=\"image-caption\">请求结果参数</div>
</div>

          </div>','1531183767'),
('325606','{1399}{75793}{1124543}{662}{975837}{976136}{975728}{694}{1420}{975713}{46163}{26425}{1906}{4644}{341}{975776}{753}{533}{2111}{6446}{660}{761}{975757}{975704}{976103}{247}{649}{448}{718}{1258}{1124544}{1124545}{1124546}{1124547}{5310}{907}{4736}{20688}{792}{975865}{1033}{952}{133}{2232}{922}{975720}{775}{975818}','$redis- select(0); } /** * 获取服务 * @return TaskExecutor */ public function getTaskService() { return create_object( [ // 类路径 \'class\' = \'mix\\task\\TaskExecutor\', // 服务名称 \'name\' = \"mix-daemon: {$this- programName}\", // 执行类型 \'type\' = \\mix\\task\\TaskExecutor::TYPE_DAEMON, // 执行模式 \'mode\' = \\mix\\task\\TaskExecutor::MODE_PUSH, // 左进程数 \'leftProcess\' = 1, // 中进程数 \'centerProcess\' = 5, // 任务超时时间 (秒) \'timeout\' = 5, ] );','使用 mixphp 打造多进程异步邮件发送','<div class=\"show-content-free\">
            <p>邮件发送是很常见的需求，由于发送邮件的操作一般是比较耗时的，所以我们一般采用异步处理来提升用户体验，而异步通常我们使用消息队列来实现。</p>
<p>传统 MVC 框架由于缺少多进程开发能力，通常是采用同一个脚本执行多次，产生多个进程的方式，mixphp 封装了 TaskExecutor 专用于多进程开发，用户能非常简单的开发出功能完善的高可用多进程应用。</p>
<p>下面演示一个异步邮件发送系统的开发过程，涉及知识点：</p>
<ul>
<li>异步</li>
<li>消息队列</li>
<li>多进程</li>
<li>守护进程</li>
</ul>
<h2>如何使用消息队列实现异步</h2>
<p>PHP 使用消息队列通常是使用中间件来实现，常用的消息中间件有：</p>
<ul>
<li>redis</li>
<li>rabbitmq</li>
<li>kafka</li>
</ul>
<p>本次我们选用 redis 来实现异步邮件发送，redis 的数据类型中有一个 list 类型，可实现消息队列，使用以下命令：</p>
<pre><code>// 入列
$redis-&gt;lpush($key, $data);
// 出列
$data = $redis-&gt;rpop($key);
// 阻塞出列
$data = $redis-&gt;brpop($key, 10);
</code></pre>
<h2>架构设计</h2>
<p>本实例由传统 MVC 框架投递邮件发送需求，MixPHP 多进程执行发送任务。</p>
<h2>邮件发送库选型</h2>
<p>以往我们通常使用框架提供的邮件发送库，或者网上下载别的用户分享的库，<a href=\"https://www.phpcomposer.com/\" target=\"_blank\" rel=\"nofollow\">composer</a> 出现后，<a href=\"https://packagist.org/\" target=\"_blank\" rel=\"nofollow\">https://packagist.org/</a> 上有大量优质的库，我们只需选择一个最好的即可，本例选择 swiftmailer。</p>
<p>由于发送任务是由 MixPHP 执行，所以 swiftmailer 是安装在 MixPHP 项目中，在项目根目录中执行以下命令安装：</p>
<pre><code>composer require swiftmailer/swiftmailer
</code></pre>
<h2>生产者开发</h2>
<p>在邮件发送这个需求中生产者是指投递发送任务的一方，这一方通常是一个接口或网页，这个部分并不一定需 mixphp 开发，TP、CI、YII 这些都可以，只需在接口或网页中把任务信息投递到消息队列中即可。</p>
<p>在传统 MVC 框架的控制器中增加如下代码：</p>
<blockquote>
<p>通常框架中使用 redis 会安装一个类库来使用，本例使用原生代码，便于理解。</p>
</blockquote>
<pre><code>// 连接
$redis = new \\Redis();
if (!$redis-&gt;connect(\'127.0.0.1\', 6379)) {
    throw new \\Exception(\'Redis Connect Failure\');
}
$redis-&gt;auth(\'\');
$redis-&gt;select(0);
// 投递任务
$data = [
    \'to\'      =&gt; [\'***@qq.com\' =&gt; \'A name\'],
    \'body\'    =&gt; \'Here is the message itself\',
    \'subject\' =&gt; \'The title content\',
];
$redis-&gt;lpush(\'queue:email\', serialize($data));
</code></pre>
<p>通常异步开发中，投递完成后就会立即响应一个消息给用户，当然此时该任务并没有执行。</p>
<h2>消费者开发</h2>
<p>本例我们使用 MixPHP 的多进程开发工具 TaskExecutor 来完成这个需求，通常使用常驻进程来处理队列的消费，所以我们使用 TaskExecutor 的 TYPE_DAEMON 类型，MODE_PUSH 模式。</p>
<p>TaskExecutor  的 MODE_PUSH 模式有二种进程：</p>
<ul>
<li>左进程：负责从消息队列取出任务数据，投放给中进程。</li>
<li>中进程：负责执行邮件发送任务。</li>
</ul>
<p>PushCommand.php 代码如下：</p>
<pre><code>&lt;?php

namespace apps\\daemon\\commands;

use mix\\console\\ExitCode;
use mix\\facades\\Input;
use mix\\facades\\Redis;
use mix\\task\\CenterProcess;
use mix\\task\\LeftProcess;
use mix\\task\\TaskExecutor;

/**
 * 推送模式范例
 * @author 刘健 &lt;coder.liu@qq.com&gt;
 */
class PushCommand extends BaseCommand
{

    // 配置信息
    const HOST = \'smtpdm.aliyun.com\';
    const PORT = 465;
    const SECURITY = \'ssl\';
    const USERNAME = \'****@email.***.com\';
    const PASSWORD = \'****\';

    // 初始化事件
    public function onInitialize()
    {
        parent::onInitialize(); // TODO: Change the autogenerated stub
        // 获取程序名称
        $this-&gt;programName = Input::getCommandName();
        // 设置pidfile
        $this-&gt;pidFile = \"/var/run/{$this-&gt;programName}.pid\";
    }

    /**
     * 获取服务
     * @return TaskExecutor
     */
    public function getTaskService()
    {
        return create_object(
            [
                // 类路径
                \'class\'         =&gt; \'mix\\task\\TaskExecutor\',
                // 服务名称
                \'name\'          =&gt; \"mix-daemon: {$this-&gt;programName}\",
                // 执行类型
                \'type\'          =&gt; \\mix\\task\\TaskExecutor::TYPE_DAEMON,
                // 执行模式
                \'mode\'          =&gt; \\mix\\task\\TaskExecutor::MODE_PUSH,
                // 左进程数
                \'leftProcess\'   =&gt; 1,
                // 中进程数
                \'centerProcess\' =&gt; 5,
                // 任务超时时间 (秒)
                \'timeout\'       =&gt; 5,
            ]
        );
    }

    // 启动
    public function actionStart()
    {
        // 预处理
        if (!parent::actionStart()) {
            return ExitCode::UNSPECIFIED_ERROR;
        }
        // 启动服务
        $service = $this-&gt;getTaskService();
        $service-&gt;on(\'LeftStart\', [$this, \'onLeftStart\']);
        $service-&gt;on(\'CenterStart\', [$this, \'onCenterStart\']);
        $service-&gt;start();
        // 返回退出码
        return ExitCode::OK;
    }

    // 左进程启动事件回调函数
    public function onLeftStart(LeftProcess $worker)
    {
        try {
            // 模型内使用长连接版本的数据库组件，这样组件会自动帮你维护连接不断线
            $queueModel = Redis::getInstance();
            // 保持任务执行状态，循环结束后当前进程会退出，主进程会重启一个新进程继续执行任务，这样做是为了避免长时间执行内存溢出
            for ($j = 0; $j &lt; 16000; $j++) {
                // 从消息队列中间件阻塞获取一条消息
                $data = $queueModel-&gt;brpop(\'queue:email\', 10);
                if (empty($data)) {
                    continue;
                }
                list(, $data) = $data;
                // 将消息推送给中进程去处理，push有长度限制 (https://wiki.swoole.com/wiki/page/290.html)
                $worker-&gt;push($data, false);
            }
        } catch (\\Exception $e) {
            // 休息一会，避免 CPU 出现 100%
            sleep(1);
            // 抛出错误
            throw $e;
        }
    }

    // 中进程启动事件回调函数
    public function onCenterStart(CenterProcess $worker)
    {
        // 保持任务执行状态，循环结束后当前进程会退出，主进程会重启一个新进程继续执行任务，这样做是为了避免长时间执行内存溢出
        for ($j = 0; $j &lt; 16000; $j++) {
            // 从进程消息队列中抢占一条消息
            $data = $worker-&gt;pop();
            if (empty($data)) {
                continue;
            }
            // 处理消息
            try {
                // 处理消息，比如：发送短信、发送邮件、微信推送
                var_dump($data);
                $ret = self::sendEmail($data);
                var_dump($ret);
            } catch (\\Exception $e) {
                // 回退数据到消息队列
                $worker-&gt;rollback($data);
                // 休息一会，避免 CPU 出现 100%
                sleep(1);
                // 抛出错误
                throw $e;
            }
        }
    }

    // 发送邮件
    public static function sendEmail($data)
    {
        // Create the Transport
        $transport = (new \\Swift_SmtpTransport(self::HOST, self::PORT, self::SECURITY))
            -&gt;setUsername(self::USERNAME)
            -&gt;setPassword(self::PASSWORD);
        // Create the Mailer using your created Transport
        $mailer = new \\Swift_Mailer($transport);
        // Create a message
        $message = (new \\Swift_Message($data[\'subject\']))
            -&gt;setFrom([self::USERNAME =&gt; \'**网\'])
            -&gt;setTo($data[\'to\'])
            -&gt;setBody($data[\'body\']);
        // Send the message
        $result = $mailer-&gt;send($message);
        return $result;
    }

}
</code></pre>
<h2>测试</h2>
<ol>
<li>在 shell 中启动 push 常驻程序。</li>
</ol>
<pre><code>[root@localhost bin]# ./mix-daemon push start
mix-daemon \'push\' start successed.
</code></pre>
<ol start=\"2\">
<li>调用接口往消息队列投放任务。</li>
</ol>
<p>此时 shell 终端将打印：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 344px; max-height: 230px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 66.86%;\"></div>
<div class=\"image-view\" data-width=\"344\" data-height=\"230\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/5710806-053c97487e5344ac.png\" data-original-width=\"344\" data-original-height=\"230\" data-original-format=\"image/png\" data-original-filesize=\"7120\"></div>
</div>
<div class=\"image-caption\">图1</div>
</div>
<p>成功收到测试邮件：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 385px; max-height: 214px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 55.58%;\"></div>
<div class=\"image-view\" data-width=\"385\" data-height=\"214\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/5710806-6f6ce15dc9d77933.png\" data-original-width=\"385\" data-original-height=\"214\" data-original-format=\"image/png\" data-original-filesize=\"14543\"></div>
</div>
<div class=\"image-caption\">图2</div>
</div>

          </div>','1531183768'),
('325607','{1124548}{9060}{255}{975714}{148645}{110591}{975721}{975786}{975872}{11426}{200851}{215219}{4816}{990}{19542}{17477}{33342}{27605}{120184}{792}{7453}{7280}{25435}{15965}{1188}{6747}{976906}{74847}{45246}{5972}{2019}{1410}{975745}{423}{3429}{405}{2510}{975766}{975866}{2516}{5555}{975895}{975901}{29257}{4133}','} $cardType = $len==16? $card = strrev(trim($card)); for ($i = 0; }else { $even += $card[$i] * 2; } }else { // 奇数位号码 $odd += $card[$i];_input_charset=utf-8 cardBinCheck=true cardNo=银行卡号 返回格式如下： { \"bank\": \"COMM\", \"validated\": true, \"cardType\": \"DC\", \"key\": \"cardNo***\", \"message\":[], \"stat\": \"ok\" } 从卡号分析获得银行名称，这个也可以自己去实现，不过就需要银行卡号开头数字的相关数据，可以在上网搜的到。','luhn算法','<div class=\"show-content-free\">
            <p>Luhn 算法或是Luhn 公式，也被称作“模10算法”。它是一种简单的校验公式，一般会被用于银行卡，身份证号码，IMEI号码，美国供应商识别号码，或是加拿大的社会保险号码的验证。该算法是由IBM的科学家Hans Peter Luhn所创造，于1954年1月6日提出该专利的申请。</p>
<p>算法特点：<br>
1、从卡号最后一位数字开始,偶数位乘以2,如果乘以2的结果是两位数，将结果减去9。<br>
2、把所有数字相加,得到总和。<br>
3、如果号码是合法的，总和可以被10整除。（模10算法）</p>
<pre><code>    /**
     * [checkBankCard luhn算法验证银行卡号]
     * @param  [type] $card      [description]
     * @param  [string] &amp;$cardType [description]
     * @return [type]            [description]
     */
    public static function checkBankCard($card, &amp;$cardType = \'\') {
        $len = strlen($card);
        if (!is_numeric($card) || ($len != 16 &amp;&amp; $len != 19)) {
            return false;
        }
        $cardType = $len==16?\"CC\":\"DC\";// 区分借记卡和贷记卡，找不到如何区分准贷记卡和预付费卡，阿里的接口可以区分，但这两种卡应该是少数

        $odd = $even = 0;
        $card = strrev(trim($card));
        for ($i = 0; $i &lt; $len; ++$i) {
            if ($i % 2) {
                // 偶数位号码
                if (($tmp = $card[$i] * 2) &gt;= 10) {
                    $even += $tmp - 9;
                }else {
                    $even += $card[$i] * 2;
                }
            }else {
                // 奇数位号码
                $odd += $card[$i];
            }
        }
        return !(($odd + $even) % 10);
    }
</code></pre>
<p>算法比较简单，就不多做解释了。<br>
如果有看官需要验证银行卡号码合法性，且需要结果比较详细的话，可以使用阿里的免费API：</p>
<pre><code>https://ccdcapi.alipay.com/validateAndCacheCardInfo.json?_input_charset=utf-8&amp;cardBinCheck=true&amp;cardNo=银行卡号
</code></pre>
<p>返回格式如下：</p>
<pre><code>{
    \"bank\": \"COMM\",
    \"validated\": true,
    \"cardType\": \"DC\",
    \"key\": \"cardNo***\",
    \"message\":[],
    \"stat\": \"ok\"
}
</code></pre>
<p>从卡号分析获得银行名称，这个也可以自己去实现，不过就需要银行卡号开头数字的相关数据，可以在上网搜的到。</p>

          </div>','1531183769'),
('325608','{7252}{2457}{975768}{327}{1124549}{2446}{975703}{5574}{59260}{2414}{93892}{744}{569}{975749}{2782}{975856}{832}{975812}{3850}{975811}{975728}{975890}{1099}{975951}{1858}{247}{4987}{977416}{3050}{1025183}{5014}{1152}{975790}{2597}{133}{1879}{2389}{189}{975714}{4467}{975739}{975784}{74}{4708}{975786}{43698}{975736}','// 结果为1989，而非1990 这就导致了订单创建时给微信的支付数据是1990，而再次支付时却是1980，所以接口返回了“订单号重复”的错误。 为什么会少了1分钱呢？PHP的官方文档中是这么说： image.png 随后我又实验了很多数字，结果如下： echo (int)(19.1 * 100);4转化后返回了正确的结果。 更有趣的是： echo (int)((19.8+0.1) * 100); // 1990 注意此时结果是正确的 var_dump(19.9 == (19.8 + 0.1));','复盘微信支付金额不正确问题解决过程——PHP浮点型计算','<div class=\"show-content-free\">
            <h1>问题</h1>
<p>2017年9月份，商城项目在运行过程中，购买某商品时如果在下单时没有完成付款，而是稍后再从“个人中心-我的订单”发起付款，则无法调起微信支付界面</p>
<h1>思路</h1>
<ul>
<li>其他商品正常，说明导致问题的原因大概率是商品本身</li>
<li>只有从会员中心发起的付款存在此问题，说明大概率是会员中心的代码存在问题</li>
<li>需要先观察问题出现时“统一下单”是否能够成功，检查是否是参数问题导致订单无法在微信端创建</li>
</ul>
<h1>观察统一下单返回值</h1>
<pre><code>result_code=FAIL
err_code=OUT_TRADE_NO_USED
err_code_des=商户订单号重复
</code></pre>
<p>微信官方对于此问题的描述如下：</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 58px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 6.1899999999999995%;\"></div>
<div class=\"image-view\" data-width=\"937\" data-height=\"58\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/30492-be2c0e357bbac595.png\" data-original-width=\"937\" data-original-height=\"58\" data-original-format=\"image/png\" data-original-filesize=\"17355\"></div>
</div>
<div class=\"image-caption\">image.png</div>
</div>
<p>出现这个问题的时候建议核查订单号是否重复提交，但实际上在这个使用场景下，我们是“故意”重复提交订单号的。因为从会员中心发起支付的时候订单已经创建了，系统会再次请求微信统一下单接口，即便如此，我们也没有必要每一次请求支付都创建一个新的订单号。</p>
<h1>那为什么返回了这个错误</h1>
<p>我先给出结论再描述排错过程：</p>
<p><strong>所谓的同一笔交易不能多次提交，实际上指的是在商品描述、标价金额不相同的情况下，用同一个订单号访问了统一下单接口。</strong></p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 481px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 51.17%;\"></div>
<div class=\"image-view\" data-width=\"940\" data-height=\"481\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/30492-f1cc6b243b45e09c.png\" data-original-width=\"940\" data-original-height=\"481\" data-original-format=\"image/png\" data-original-filesize=\"135047\"></div>
</div>
<div class=\"image-caption\">image.png</div>
</div>
<p>这里的错误实际上是因为：从会员中心发起支付时“标价金额”与提交订单时的不相同。</p>
<h1>PHP浮点型运算</h1>
<p>以下是某位程序员写的微信支付代码：</p>
<pre><code>$total_fee = (int)($order_total * 100);
</code></pre>
<p>微信要求金额的单位必须为分，而数据库中订单金额单位是元，所以使用订单金额*100是正确的做法。<br>
订单支付金额的计算非常复杂，所以单位转化为分之后再转化为整型，可以保证微信支付参数不出错，也是正确的做法。</p>
<p>但这里面隐藏了一个问题，还记得我们问题发生的条件必须是“购买某商品时”吗？如果单独购买这个商品的话，订单的金额是19.9。我们可以尝试：</p>
<pre><code>echo (int)(19.9 * 100);
// 结果为1989，而非1990
</code></pre>
<p>这就导致了订单创建时给微信的支付数据是1990，而再次支付时却是1980，所以接口返回了“订单号重复”的错误。</p>
<p>为什么会少了1分钱呢？PHP的官方文档中是这么说：</p>
<br><div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 388px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 37.63%;\"></div>
<div class=\"image-view\" data-width=\"1031\" data-height=\"388\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/30492-ee25db1a057d4be5.png\" data-original-width=\"1031\" data-original-height=\"388\" data-original-format=\"image/png\" data-original-filesize=\"157353\"></div>
</div>
<div class=\"image-caption\">image.png</div>
</div>
<p>随后我又实验了很多数字，结果如下：</p>
<pre><code>echo (int)(19.1 * 100);// 1910
echo (int)(19.2 * 100);// 1920
echo (int)(19.3 * 100);// 1930
echo (int)(19.4 * 100);// 1939 注意这里出现了问题
echo (int)(19.5 * 100);// 1950
echo (int)(19.6 * 100);// 1960
echo (int)(19.7 * 100);// 1970
echo (int)(19.8 * 100);// 1980
</code></pre>
<p>这个问题的产生，似乎存在规律，例如19.4、18.4和17.4转化后是错误的，而8.4转化后返回了正确的结果。<br>
更有趣的是：</p>
<pre><code>echo (int)((19.8+0.1) * 100);
// 1990 注意此时结果是正确的
var_dump(19.9 == (19.8 + 0.1));
// false
</code></pre>
<h1>调试过程</h1>
<p>实际上这种奇怪的问题排查起来没有什么捷径，无非就是打日志追踪变量，最多也就是细心点罢了。</p>
<p>最终使用了一个比较讨巧的方式解决了这个问题，将代码改为了：</p>
<pre><code>$total_fee = (int)(($order_total + 0.00001) * 100);
</code></pre>
<p>至于更加严谨的浮点数计算方法，今后遇到的时候，再研究吧。</p>

          </div>','1531183769'),
('325609','{2917}{15167}{73930}{210310}{1124550}{3304}{13441}{120533}{975728}{23605}{1124551}{3034}{975749}{61}{448}{975714}{831}{975757}{975761}{2041}{3483}{21914}{562}{4904}{2980}{1299}{4453}{975820}{761}{975721}{2337}{3411}{763}{2040}{1802}{4547}{975830}{975788}{1687}{1192}{7}{415}{23054}{12}{1323}{2118}{650}{3297}{976526}{14901}','PHP面试之数据库—创建高性能索引 真题 简单描述MySQL中，索引、主键、唯一索引、联合索引的区别，对数据库的性能有什么影响？ MySQL索引的基础和类型 索引：类似于书籍的目录，想找到一本书的某个特定的主题，需要先找到书的目录，定位对应的页码。 MySQL中存储引擎使用类似的方式进行查询，先去索引中查找对应的值，然后根据匹配的索引找到对应的数据行。 索引对性','PHP面试之数据库—创建高性能索引','<div class=\"show-content-free\">
            <h1>真题</h1>
<p>简单描述MySQL中，索引、主键、唯一索引、联合索引的区别，对数据库的性能有什么影响？</p>
<h1>MySQL索引的基础和类型</h1>
<p>索引：类似于书籍的目录，想找到一本书的某个特定的主题，需要先找到书的目录，定位对应的页码。</p>
<p>MySQL中存储引擎使用类似的方式进行查询，先去索引中查找对应的值，然后根据匹配的索引找到对应的数据行。</p>
<p>索引对性能的影响：</p>
<ul>
<li>大大减少服务器需要扫描的数据量。</li>
<li>帮助服务器避免排序和临时表。</li>
<li>将随机I/O变顺序I/O。</li>
<li>大大提高查询速度，降低写的速度、占用磁盘。</li>
</ul>
<p>索引的使用场景：</p>
<ul>
<li>对非常小的表，大部分情况下全表扫描效率更高。</li>
<li>对中大型表，索引非常有效</li>
<li>特大型的表，建立和使用索引的代价随着增长，可以使用分区技术来解决。</li>
</ul>
<h2>索引的类型</h2>
<p>索引都是实现在存储引擎层的。</p>
<ul>
<li>普通索引：最基本的索引，没有任何约束</li>
<li>唯一索引：与普通索引类似，但具有唯一性约束</li>
<li>主键索引：特殊的唯一索引，不允许有空值</li>
<li>组合索引：将多个列组合在一起创建索引，可以覆盖多个列</li>
<li>外键索引：只有InnoDB类型的表才可以使用外键索引，保证数据的一致性、完整性和实现级联操作</li>
<li>全文索引：MySQL自带的全文索引只能用于MyISAM，并且只能对英文进行全文检索，一般使用全文索引引擎</li>
</ul>
<p>主键索引和唯一索引的去别：</p>
<ul>
<li>一个表只能有一个主键索引，可以有多个唯一索引</li>
<li>主键索引一定是唯一索引，唯一索引不是主键索引</li>
<li>主键和外键构成参照完整性约束，防止数据不一致</li>
</ul>
<h1>延伸：MySQL索引的创建原则</h1>
<ul>
<li>最适合索引的列是出现在<code>where</code>子句中的列，或连接子句中的列，而不是出现在<code>select</code>关键字后的列；</li>
<li>索引列的基数越大，索引效果越好；</li>
<li>对字符串进行索引，应该定制一个前缀长度，可以节省大量的索引空间；</li>
<li>根据情况创建复合索引，复合索引可以提高查询效率；</li>
<li>避免创建过多的索引，索引会额外占用磁盘空间，降低写操作效率；</li>
<li>主键尽可能选择较短的数据类型，可以有效减少索引的磁盘占用提高查询效率</li>
</ul>
<h1>延伸：MySQL索引的注意事项</h1>
<ul>
<li>复合索引遵循前缀原则；</li>
<li>like查询，%不能在前，可以使用全文索引；</li>
<li>
<code>column is null</code>可以使用索引；</li>
<li>如果MySQL估计使用索引比全表扫描更慢，会放弃使用索引；</li>
<li>列类型是字符串类型，查询时一定要给值加引号，否则索引失效。</li>
</ul>
</div>','1531183770'),
('325610','{394}{4421}{975707}{2376}{92}{721109}{975879}{1494}{76362}{975721}{975761}{975830}{2534}{4088}{3576}{1325}{976553}{1627}{4065}{842}{1537}{667}{975714}{975786}{4324}{4904}{975857}{1314}{975911}{14301}{1076}{408}{2865}{1048}{569}{6079}{1847}{1762}{157847}{2919}{422}{16351}{1124552}{409415}{2803}{3872}','PHP实现Huffman编码/解码 Huffman 编码是一种数据压缩算法。我们常用的 zip 压缩，其核心就是 Huffman 编码，还有在 HTTP/2 中，Huffman 编码被用于 HTTP 头部的压缩。 本文就来用 PHP 来实践一下 Huffman 编码和解码。 1. 编码 字数统计 Huffman编码的第一步就是要统计文档中每个字符出现的次数，PHP的内置函数 count_chars() 就可以做到： $input = file_get_contents(\'input.txt\'); } $root = current($huffmanTree);','PHP实现Huffman编码/解码','<div class=\"show-content-free\">
            <p>Huffman 编码是一种数据压缩算法。我们常用的 zip 压缩，其核心就是 Huffman 编码，还有在 HTTP/2 中，Huffman 编码被用于 HTTP 头部的压缩。</p>
<p>本文就来用 PHP 来实践一下 Huffman 编码和解码。</p>
<h2>1. 编码</h2>
<h3>字数统计</h3>
<p>Huffman编码的第一步就是要统计文档中每个字符出现的次数，PHP的内置函数 <code>count_chars()</code> 就可以做到：</p>
<pre><code class=\"php\">$input = file_get_contents(\'input.txt\');
$stat = count_chars($input, 1);
</code></pre>
<h3>构造Huffman树</h3>
<p>接下来根据统计结果构造Huffman树，构造方法在 <a href=\"https://link.jianshu.com?t=https%3A%2F%2Fzh.wikipedia.org%2Fwiki%2F%25E9%259C%258D%25E5%25A4%25AB%25E6%259B%25BC%25E7%25BC%2596%25E7%25A0%2581\" target=\"_blank\" rel=\"nofollow\">Wikipedia</a> 有详细的描述。这里用PHP写了一个简易版的：</p>
<pre><code class=\"php\">$huffmanTree = [];
foreach ($stat as $char =&gt; $count) {
    $huffmanTree[] = [
        \'k\' =&gt; chr($char),
        \'v\' =&gt; $count,
        \'left\' =&gt; null,
        \'right\' =&gt; null,
    ];
}

// 构造树的层级关系，思想见wiki：https://zh.wikipedia.org/wiki/%E9%9C%8D%E5%A4%AB%E6%9B%BC%E7%BC%96%E7%A0%81
$size = count($huffmanTree);
for ($i = 0; $i !== $size - 1; $i++) {
    uasort($huffmanTree, function ($a, $b) {
        if ($a[\'v\'] === $b[\'v\']) {
            return 0;
        }
        return $a[\'v\'] &lt; $b[\'v\'] ? -1 : 1;
    });
    $a = array_shift($huffmanTree);
    $b = array_shift($huffmanTree);
    $huffmanTree[] = [
        \'v\' =&gt; $a[\'v\'] + $b[\'v\'],
        \'left\' =&gt; $b,
        \'right\' =&gt; $a,
    ];
}
$root = current($huffmanTree);
</code></pre>
<p>经过计算之后，<code>$root</code> 就会指向 Huffman 树的根节点</p>
<h3>根据Huffman树生成编码字典</h3>
<p>有了 Huffman 树，就可以生成用于编码的字典：</p>
<pre><code class=\"php\">function buildDict($elem, $code = \'\', &amp;$dict) {
    if (isset($elem[\'k\'])) {
        $dict[$elem[\'k\']] = $code;
    } else {
        buildDict($elem[\'left\'], $code.\'0\', $dict);
        buildDict($elem[\'right\'], $code.\'1\', $dict);
    }
}
$dict = [];
buildDict($root, \'\', $dict);
</code></pre>
<h3>写文件</h3>
<p>运用字典将文件内容进行编码，并写入文件。将Huffman编码写入文件的有几个注意的地方：</p>
<ol>
<li><p>将编码字典和编码内容一起写入文件后，就没法区分他们的边界了，因此需要在文件开始写入他们各自占用的字节数</p></li>
<li><p>PHP提供的 <code>fwrite()</code> 函数一次能写入 8-bit（一个字节）或者是 8的整数倍个bit。但Huffman编码中，一个字符可能只使用 1-bit 表示，PHP不支持只往文件中写入 1-bit 这种操作。所以需要我们自行对编码进行拼接，每凑齐 8-bit 才写入文件。</p></li>
</ol>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 265px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 35.52%;\"></div>
<div class=\"image-view\" data-width=\"746\" data-height=\"265\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/8470638-ba24d5e1aee7ce30.png\" data-original-width=\"746\" data-original-height=\"265\" data-original-format=\"image/png\" data-original-filesize=\"14178\"></div>
</div>
<div class=\"image-caption\">每凑齐8-bit才写入</div>
</div>
<ol start=\"3\">
<li>与第二条类似，最终形成的文件大小一定是 8-bit 的整数倍。所以如果整个编码的大小是 8001-bit的话，还要在末尾补上 7个 0</li>
</ol>
<pre><code class=\"php\">$dictString = serialize($dict);
// 写入字典和编码各自占用的字节数
$header = pack(\'VV\', strlen($dictString), strlen($input));
fwrite($outFile, $header);
// 写入字典本身
fwrite($outFile, $dictString);

// 写入编码的内容
$buffer = \'\';
$i = 0;
while (isset($input[$i])) {
    $buffer .= $dict[$input[$i]];
    while (isset($buffer[7])) {
        $char = bindec(substr($buffer, 0, 8));
        fwrite($outFile, chr($char));
        $buffer = substr($buffer, 8);
    }
    $i++;
}
// 末尾的内容如果没有凑齐 8-bit，需要自行补齐
if (!empty($buffer)) {
    $char = bindec(str_pad($buffer, 8, \'0\'));
    fwrite($outFile, chr($char));
}
fclose($outFile);
</code></pre>
<h2>解码</h2>
<p>Huffman编码的解码相对简单：先读取编码字典，然后根据字典解码出原始字符。</p>
<p>解码过程有个问题需要注意：由于我们在编码过程中，在文件末尾补齐了几个0-bit，如果这些 0-bit 在字典中恰巧是某个字符的编码时，就会造成错误的解码。</p>
<p>所以解码过程中，当已解码的字符数达到文档长度时，就要停止解码。</p>
<pre><code class=\"php\">&lt;?php
$content = file_get_contents(\'a.out\');

// 读出字典长度和编码内容长度
$header = unpack(\'VdictLen/VcontentLen\', $content);
$dict = unserialize(substr($content, 8, $header[\'dictLen\']));
$dict = array_flip($dict);

$bin = substr($content, 8 + $header[\'dictLen\']);
$output = \'\';
$key = \'\';
$decodedLen = 0;
$i = 0;
while (isset($bin[$i]) &amp;&amp; $decodedLen !== $header[\'contentLen\']) {
    $bits = decbin(ord($bin[$i]));
    $bits = str_pad($bits, 8, \'0\', STR_PAD_LEFT);
    for ($j = 0; $j !== 8; $j++) {
        // 每拼接上 1-bit，就去与字典比对是否能解码出字符
        $key .= $bits[$j];
        if (isset($dict[$key])) {
            $output .= $dict[$key];
            $key = \'\';
            $decodedLen++;
            if ($decodedLen === $header[\'contentLen\']) {
                break;
            }
        }
    }
    $i++;
}

echo $output;
</code></pre>
<h2>试验</h2>
<p>我们将<a href=\"https://link.jianshu.com?t=https%3A%2F%2Fzh.wikipedia.org%2Fwiki%2F%25E9%259C%258D%25E5%25A4%25AB%25E6%259B%25BC%25E7%25BC%2596%25E7%25A0%2581\" target=\"_blank\" rel=\"nofollow\">Huffman编码Wiki页</a> 的HTML代码保存到本地，进行Huffman编码测试，试验结果：</p>
<blockquote>
<p>编码前: 418,504 字节</p>
<p>编码后: 280,127 字节</p>
</blockquote>
<p>空间节省了 33%，如果原文的重复内容较多，Huffman编码节省的空间可以达到 50% 以上.</p>
<p>除了文本内容，我们再尝试将一个二进制文件进行Huffman编码，比如 <a href=\"https://link.jianshu.com?t=https%3A%2F%2Fjustgetflux.com%2F\" target=\"_blank\" rel=\"nofollow\">f.lux的安装程序</a>，试验结果如下：</p>
<blockquote>
<p>编码前: 770,384 字节</p>
<p>编码后: 773,076 字节</p>
</blockquote>
<p>编码后反而占用了更大的空间，一方面是由于我们存储字典时，并没有做额外的处理，占用了不少空间。另一方面，二进制文件中，各个字符出现的概率相对比较平均，无法发挥Huffman编码的优势。</p>

          </div>','1531183771'),
('325611','{647}{949}{975789}{975704}{243}{1889}{65}{1891}{679}{3533}{3370}{4096}{975761}{975820}{975728}{975741}{975857}{886}{975794}{975751}{542}{3926}{6870}','PHPStorm 使用git bash 为terminal终端 修改方法如下 File - Settings - Tools - Terminal 设置shell_path为\"D:\\yzm\\Git\\bin\\sh.exe\" --login -i这里注意路径两侧的双引号，如果去掉打开终端会报错。 具体每台电脑安装的git路径不一样，请自行根据git安装情况进行路径设置。','PHPStorm 使用git bash 为terminal终端','<div class=\"show-content-free\">
            <p>修改方法如下</p>
<pre><code class=\"sh\">File -&gt; Settings -&gt; Tools -&gt; Terminal
</code></pre>
<p>设置<code>shell_path</code>为<code>\"D:\\yzm\\Git\\bin\\sh.exe\" --login -i</code>这里注意路径两侧的双引号，如果去掉打开终端会报错。</p>
<p>具体每台电脑安装的git路径不一样，请自行根据git安装情况进行路径设置。</p>
<div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 700px; max-height: 606px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 86.59%;\"></div>
<div class=\"image-view\" data-width=\"1148\" data-height=\"994\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/3827973-cbd4a67acdf1ba7a.png\" data-original-width=\"1148\" data-original-height=\"994\" data-original-format=\"image/png\" data-original-filesize=\"124975\"></div>
</div>
<div class=\"image-caption\"></div>
</div>

          </div>','1531183771'),
('325612','{4312}{3345}{43075}{118}{975728}{721}{408}{761}{365}{975714}{975766}{975786}{975790}{1975}{2003}{13970}{975738}{975721}{5136}{975779}{975776}{975837}{975841}{2098}{667}{907}{1716}{28920}{2861}{4517}{2916}{42080}{553}{975788}{1124}{405}{975895}{5279}{165543}{1560}{978153}{5099}{788}{12726}{3794}{975815}{8792}','foreach(squares(1,5) as $k = $v){ printf(\"%d squared is n\",$k,$v); $i += $step) { yield $i; foreach (range(1, 9, 2) as $number) { echo \"$number \"; } 注意： yield 的应用场景，一般多用于循环体，比如数据库的 fetch 操作，这样可以减少内存的消耗，ZanPHP 框架就大量的这样使用。 但切莫滥用 yield 操作，当数据量大的时候，yield可能会是一个高耗时的操作，会使程序性能大大降低。 参考： 官方文档','PHP关键字之01 - yield','<div class=\"show-content-free\">
            <div class=\"image-package\">
<div class=\"image-container\" style=\"max-width: 444px; max-height: 335px;\">
<div class=\"image-container-fill\" style=\"padding-bottom: 75.44999999999999%;\"></div>
<div class=\"image-view\" data-width=\"444\" data-height=\"335\"><img data-original-src=\"//upload-images.jianshu.io/upload_images/2021264-8b2a0e256569381a.png\" data-original-width=\"444\" data-original-height=\"335\" data-original-format=\"image/png\" data-original-filesize=\"30235\"></div>
</div>
<div class=\"image-caption\">Generator类</div>
</div>
<p>从PHP5.5开始，可以使用生成器来处理一个序列。生成器是一个函数，它不会调用return来返回一个值，而会调用yield（可能在一个循环中调用）。有了这样一个生成器，可以在原先使用数组的地方调用这个生成器函数，然后处理传递到yield关键字的值序列。</p>
<p>生成器函数的核心是yield关键字。它最简单的调用形式看起来像一个return申明，不同之处在于普通return会返回值并终止函数的执行，而yield会返回一个值给循环调用此生成器的代码并且只是暂停执行生成器函数。</p>
<p><strong>例一：使用生成器来生成一个平方数列表</strong></p>
<pre><code>function squares($start, $stop) {
    if ($start &lt; $stop) {
        for($i = $start; $i &lt;= $stop; $i++){
            yield $i =&gt; $i * $i;
        }
    } else {
        for($i = $stop; $i &lt;= $start; $i++){
            yield $i =&gt; $i * $i;
        }
    }

}

var_dump(squares(1,5));

foreach(squares(1,5) as $k =&gt; $v){
    printf(\"%d squared is %d\\n\",$k,$v);
}
</code></pre>
<p>可以在foreach中使用传入yield的健和值，就像常规的数组元素一样。<br>
运行结果如下：</p>
<pre><code>object(Generator)#1 (0) {
}
1 squared is 1
2 squared is 4
3 squared is 9
4 squared is 16
5 squared is 25
</code></pre>
<p><strong>例二：生成器来重新实现 <a href=\"http://php.net/manual/zh/function.range.php\" target=\"_blank\" rel=\"nofollow\">range()</a> 函数</strong></p>
<p>标准的 range() 函数需要在内存中生成一个数组包含每一个在它范围内的值，然后返回该数组, 结果就是会产生多个很大的数组。 比如，调用 range(0, 1000000) 将导致内存占用超过 100 MB。</p>
<p>做为一种替代方法, 我们可以实现一个 xrange() 生成器, 只需要足够的内存来创建 Iterator 对象并在内部跟踪生成器的当前状态，这样只需要不到1K字节的内存。</p>
<pre><code>function xrange($start, $limit, $step = 1) {
    if ($start &lt; $limit) {
        if ($step &lt;= 0) {
            throw new LogicException(\'Step must be +ve\');
        }

        for ($i = $start; $i &lt;= $limit; $i += $step) {
            yield $i;
        }
    } else {
        if ($step &gt;= 0) {
            throw new LogicException(\'Step must be -ve\');
        }

        for ($i = $start; $i &gt;= $limit; $i += $step) {
            yield $i;
        }
    }
}
</code></pre>
<p>所以，下面range()和xrange()输出的结果是一样的。</p>
<pre><code>echo \'Single digit odd numbers from range():  \';
foreach (range(1, 9, 2) as $number) {
    echo \"$number \";
}
echo \"\\n\";

echo \'Single digit odd numbers from xrange(): \';
foreach (xrange(1, 9, 2) as $number) {
    echo \"$number \";
}
</code></pre>
<p><strong>注意：</strong></p>
<p>yield 的应用场景，一般多用于循环体，比如数据库的 fetch 操作，这样可以减少内存的消耗，ZanPHP 框架就大量的这样使用。</p>
<p>但切莫滥用 yield 操作，当数据量大的时候，yield可能会是一个高耗时的操作，会使程序性能大大降低。</p>
<p><strong>参考：</strong></p>
<p>官方文档：</p>
<ul>
<li><a href=\"http://php.net/manual/zh/language.generators.php\" target=\"_blank\" rel=\"nofollow\">http://php.net/manual/zh/language.generators.php</a></li>
<li><a href=\"http://php.net/manual/zh/class.generator.php\" target=\"_blank\" rel=\"nofollow\">http://php.net/manual/zh/class.generator.php</a></li>
</ul>
<p>相关书籍：</p>
<ul>
<li>《PHP经典实例》 David Sklar &amp; Adam Trachtenberg</li>
</ul>
<p><strong>扩展阅读</strong></p>
<p>在PHP中使用协程实现多任务调度：</p>
<ul>
<li>译文 <a href=\"http://www.laruence.com/2015/05/28/3038.html\" target=\"_blank\" rel=\"nofollow\">http://www.laruence.com/2015/05/28/3038.html</a>
</li>
<li>原文 <a href=\"http://nikic.github.io/2012/12/22/Cooperative-multitasking-using-coroutines-in-PHP.html\" target=\"_blank\" rel=\"nofollow\">http://nikic.github.io/2012/12/22/Cooperative-multitasking-using-coroutines-in-PHP.html</a>
</li>
</ul>
<p>听雨阁中听雨歌 - <a href=\"https://www.cnblogs.com/tingyugetc/p/6347286.html\" target=\"_blank\" rel=\"nofollow\">PHP的生成器、yield和协程</a><br>
猫猫大侠 - <a href=\"https://www.cnblogs.com/lynxcat/p/7954456.html\" target=\"_blank\" rel=\"nofollow\">PHP yield 分析，以及协程的实现，超详细版(上)</a><br>
取个好名字真难的博客 - <a href=\"https://blog.csdn.net/qq_20329253/article/details/52202811\" target=\"_blank\" rel=\"nofollow\">php利用yield写一个简单中间件</a></p>

          </div>','1531183771'),
('325613','{975872}{975728}{1124555}{25760}{209359}{975714}{408}{146}{26}{181396}{142}{2916}{11617}{11469}{58}{975790}{975741}{74}{12}{716}{975715}{2970}{7422}{751}{448}{2720}{15296}{10}{975891}{975725}{975786}{7976}{975835}{22235}{2728}{7606}{6484}{2861}{76275}{6767}{2423}{975901}{5438}{667}{1075}{975892}','PHP强化之02 - 数字 Math 一、语法 1、简介 在PHP中，数字被分为两种类型：整数、浮点数。 2、类型的转换 1）转换为整形 方法一：使用(int)或(integer)转换成整形 方法二：使用intval($var)把 $var 转换成整形 方法三：使用settype($var, \"integer\")，第二个参数也可以设成int。 $num = 3.14; $num1 = (float) $num; //十六进制 $b = 0b11; //true $num = 0xA3; // 5 echo ceil(9.999); // 4 echo floor(9.999);, $number);','PHP强化之02 - 数字 Math','<div class=\"show-content-free\">
            <h2>一、语法</h2>
<h4>1、简介</h4>
<p>在PHP中，数字被分为两种类型：整数、浮点数。</p>
<h4>2、类型的转换</h4>
<p><strong>1）转换为整形</strong><br>
方法一：使用<code>(int)</code>或<code>(integer)</code>转换成整形<br>
方法二：使用<code>intval($var)</code>把 $var 转换成整形<br>
方法三：使用<code>settype($var, \"integer\")</code>，第二个参数也可以设成<code>int</code>。</p>
<pre><code>$num = 3.14;   
$num1 = (int) $num; 
var_dump($num1); //输出int(3)   
var_dum($num); //输出int(3)  
</code></pre>
<p><strong>2）转换为浮点形</strong><br>
方法一：使用<code>(float)</code>、<code>(double)</code>或<code>(real)</code>转换成浮点形<br>
方法二：使用<code>floatval($var)</code>把 $var 转换成浮点形<br>
方法三：使用<code>settype($var, \"float\")</code>，对于旧版本中使用的<code>double</code>现已停用。</p>
<pre><code>$num = \'3.14ab\';   
$num1 = (float) $num;  //使用（integer）也一样
var_dump($num1); //输出float(3.14)     
var_dum(floatval($num)); //输出float(3.14)     
</code></pre>
<p>3<strong>）转换进制</strong><br>
base_convert — 在任意进制之间转换数字<br><code>string base_convert ( string $number , int $frombase , int $tobase )</code><br>
返回一字符串，包含 number 以 tobase 进制的表示。number 本身的进制由 frombase 指定。frombase 和 tobase 都只能在 2 和 36 之间（包括 2 和 36）。高于十进制的数字用字母 a-z 表示，例如 a 表示 10，b 表示 11 以及 z 表示 35。<br>
相关函数：<br><code>bindec</code> — 二进制转换为十进制<br><code>decbin</code> — 十进制转换为二进制<br><code>octdec</code> — 八进制转换为十进制<br><code>decoct</code> — 十进制转换为八进制<br><code>hexdec</code> — 十六进制转换为十进制<br><code>dechex</code> — 十进制转换为十六进制<br><code>hex2bin</code> — 转换十六进制字符串为二进制字符串<br><code>bin2hex</code> — 二进制字符串转换为十六进制值</p>
<h2>二、Integer 整型</h2>
<p>整型值可以使用十进制，十六进制，八进制或二进制(PHP 5.4.0 起可用)表示，前面可以加上可选的符号（- 或者 +）。</p>
<p>要使用八进制表达，数字前必须加上 <em>0</em>（零）。要使用十六进制表达，数字前必须加上 <em>0x</em>。要使用二进制表达，数字前必须加上 <em>0b</em>。</p>
<pre><code>$a = 0x11; //十六进制 
$b = 0b11; //二进制
$c = 011; //八进制

var_dump($a);
var_dump($b);
var_dump($c);
</code></pre>
<p>输出结果为(十进制)：</p>
<pre><code>int(17) int(3) int(9)
</code></pre>
<h2>三、Float 浮点型</h2>
<p>浮点型（也叫浮点数 float，双精度数 double 或实数 real）可以用以下任一语法定义：</p>
<pre><code>$a = 1.234;
$b = 1.2e3;
$c = 7E-10;

var_dump($a);
var_dump($b);
var_dump($c);
</code></pre>
<p>输出结果为：</p>
<pre><code>float(1.234) float(1200) float(7.0E-10)
</code></pre>
<h2>四、常用方法</h2>
<h4>1、数字类型的判断</h4>
<p><strong>1）is_numeric—检测变量是否为数字或数字字符串</strong></p>
<pre><code>bool is_numeric ( mixed $var)
</code></pre>
<p>检测变量是否为数字或数字字符串，如果var是数字和数字字符串则返回<strong>TRUE</strong>，否则返回<strong>FALSE</strong>。<br>
注意还要考虑科学记数法和十六进制数。</p>
<pre><code>$num = \'22e33\'; //true
$num = 0xA3;  //true
$num = \'0xA3\';  //false
$num = \'5,112\';  //false
</code></pre>
<p><strong>2）用正则式判断是否是整数</strong></p>
<pre><code>preg_match(\"/^[0-9]*$/\", $var);
</code></pre>
<p>该规则为纯数字判断。</p>
<p><strong>3）is_int - 检测变量是否是整数</strong></p>
<pre><code>bool is_int ( mixed $var )
</code></pre>
<p><code>is_integer</code> — is_int() 的别名<br><code>is_long</code> — is_int() 的别名</p>
<pre><code>$num = \'5\';  //false
$num = 5;  //true
$num = 5.0;  //false
</code></pre>
<p><strong>4）is_float—检测变量是否是浮点型</strong></p>
<pre><code>bool is_float ( mixed $var )
</code></pre>
<p><code>is_double</code> — is_float() 的别名<br><code>is_real</code> — is_float() 的别名</p>
<pre><code>$num = 5.0;  //true
$num = \'5.0\';  //false
</code></pre>
<h4>2、小数与整数的舍取</h4>
<p><strong>1）round—对浮点数进行四舍五入</strong></p>
<pre><code>float round ( float $val [, int $precision = 0 [, int $mode = PHP_ROUND_HALF_UP ]] )
</code></pre>
<p>返回将val根据指定精度precision（十进制小数点后数字的数目）进行四舍五入的结果。precision也可以是负数或零（默认值）。</p>
<pre><code>echo round(3.4);         // 3
echo round(3.5);         // 4
echo round(1.95583, 2);  // 1.96
echo round(1241757, -3); // 1242000
</code></pre>
<p>如传入第三个参数mode值为以下之一（刚好需要舍取的下一位数刚好为5的情况）：  <code>PHP_ROUND_HALF_UP</code>（向上舍取）、<code>PHP_ROUND_HALF_DOWN</code>（向下舍取）、<code>PHP_ROUND_HALF_EVEN</code>(取最近的偶数）或<code>PHP_ROUND_HALF_ODD</code>（取最近的奇数）</p>
<p><strong>2）ceil—进一法取整</strong></p>
<pre><code>float ceil ( float $value )
</code></pre>
<p>返回不小于 value 的下一个整数。 ceil() 返回的类型仍然是 float，因为 float 值的范围通常比 integer 要大</p>
<pre><code>echo ceil(4.3);    // 5
echo ceil(9.999);  // 10
echo ceil(-3.14);  // -3
</code></pre>
<p><strong>3）floor—舍去法取整</strong></p>
<pre><code>float floor ( float $value )
</code></pre>
<p>返回不大于 value 的最接近的整数，将 value 的小数部分舍去取整。floor() 返回的类型仍然是 float，因为 float 值的范围通常比 integer 要大。</p>
<pre><code>echo floor(4.3);   // 4
echo floor(9.999); // 9
echo floor(-3.14); // -4
</code></pre>
<p><strong>4）abs—绝对值</strong></p>
<pre><code>number abs ( mixed $number )
</code></pre>
<p>number 的绝对值。 如果参数 number 是 float，则返回的类型也是 float，否则返回 integer（因为 float 通常比 integer 有更大的取值范围）。</p>
<pre><code>$abs = abs(-4.2); // $abs = 4.2; (double/float)
$abs2 = abs(5);   // $abs2 = 5; (integer)
$abs3 = abs(-5);  // $abs3 = 5; (integer)
</code></pre>
<h4>3、数字的自动生成</h4>
<p><strong>1）range — 根据范围创建数组，包含指定的元素</strong></p>
<pre><code>array range ( mixed $start , mixed $end [, number $step = 1 ] )
</code></pre>
<p>建立一个包含指定范围单元的数组。</p>
<pre><code>range(3,7,2);

//返回结果如下：
array(3) {
  [0]=&gt;
  int(3)
  [1]=&gt;
  int(5)
  [2]=&gt;
  int(7)
}
</code></pre>
<p><strong>2）rand — 产生一个随机整数</strong></p>
<pre><code>int rand ( void )
</code></pre>
<pre><code>int rand ( int $min , int $max )
</code></pre>
<p>如果没有提供可选参数 min 和 max，rand() 返回 0 到 getrandmax() 之间的伪随机整数。</p>
<p><strong>3）mt_rand — 生成更好的随机数</strong></p>
<pre><code>int mt_rand ( void )
</code></pre>
<pre><code>int mt_rand ( int $min , int $max )
</code></pre>
<p>很多老的 libc 的随机数发生器具有一些不确定和未知的特性而且很慢。PHP 的<code>rand()</code>函数默认使用 libc 随机数发生器。<code>mt_rand()</code> 函数是非正式用来替换它的。该函数用了 <a href=\"http://www.math.sci.hiroshima-u.ac.jp/~m-mat/MT/emt.html\" target=\"_blank\" rel=\"nofollow\"> Mersenne Twister</a>中已知的特性作为随机数发生器，它可以产生随机数值的平均速度比 libc 提供的 <code>rand()</code> 快四倍。</p>
<p>如果没有提供可选参数 min 和 max，mt_rand() 返回 0 到 mt_getrandmax() 之间的伪随机数。</p>
<h4>4、常用数学方法</h4>
<p><strong>1）log — 自然对数</strong><br><code>float log ( float $arg [, float $base = M_E ] )</code><br>
如果指定了可选的参数 base，log() 返回 logbase arg，否则 log() 返回参数 arg 的自然对数。<br>
相关方法：<br><code>log10()</code> — 以 10 为底的对数<br><strong>2）exp — 计算 e 的指数</strong><br><code>float exp ( float $arg )</code><br>
返回 e 的 arg 次方值。</p>
<p><strong>3）pow — 指数表达式</strong><br><code>number pow ( number $base , number $exp )</code><br>
返回 base 的 exp 次方的幂。如果可能，本函数会返回 integer。</p>
<h4>5、格式化一个数字</h4>
<p><strong>1）number_format — 以千位分隔符方式格式化一个数字</strong><br><code>string number_format ( float $number [, int $decimals = 0 ] )</code><br><code>string number_format ( float $number , int $decimals = 0 , string $dec_point = \".\" , string $thousands_sep = \",\" )</code><br>
本函数可以接受1个、2个或者4个参数（注意：不能是3个）:</p>
<ul>
<li>如果只提供第一个参数，number的小数部分会被去掉 并且每个千位分隔符都是英文小写逗号\",\"</li>
<li>如果提供两个参数，number将保留小数点后的位数到你设定的值，其余同楼上</li>
<li>如果提供了四个参数，number 将保留decimals个长度的小数部分, 小数点被替换为dec_point，千位分隔符替换为thousands_sep</li>
</ul>
<p>场景：默认地，number_format函数会把这个数舍入到最接近的整数。如果想你保留整个数，但又无法提前知道小数点后有多少位，这时你该怎么办？可以使用以下解决方法：</p>
<pre><code>$number = 31415.93421;  //你的数
list($int,$dec) = explode(\'.\', $number);
$formatted = number_format($number，strlen($dec)); //$formatted为：31,415.93421
</code></pre>
<h2>参考：</h2>
<p>官方文档：</p>
<ul>
<li>Integer 整型 - <a href=\"http://php.net/manual/zh/language.types.integer.php\" target=\"_blank\" rel=\"nofollow\">http://php.net/manual/zh/language.types.integer.php</a>
</li>
<li>Float 浮点型 - <a href=\"http://php.net/manual/zh/language.types.float.php\" target=\"_blank\" rel=\"nofollow\">http://php.net/manual/zh/language.types.float.php</a>
</li>
<li>Math 函数 - <a href=\"http://php.net/manual/zh/ref.math.php\" target=\"_blank\" rel=\"nofollow\">http://php.net/manual/zh/ref.math.php</a>
</li>
</ul>
<p>相关书籍：</p>
<ul>
<li>《PHP经典实例》 David Sklar &amp; Adam Trachtenberg</li>
</ul>
<p>--《最后更新时间：2018/6/1》--</p>

          </div>','1531183772')	2006: MySQL server has gone away